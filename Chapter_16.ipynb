{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Rahmanda Afebrio Yuris Soesatyo - Chapter 16:Natural Language Processing with RNNs and Attention"
      ],
      "metadata": {
        "id": "spjH1AmlnQCI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Character RNN and Shakespeare Text Generation\n",
        "\n",
        "Bab ini diawali dengan pembahasan Character-level RNN (Char-RNN), yaitu model rekuren yang dilatih untuk memprediksi karakter berikutnya dalam teks karya Shakespeare. Dengan mempelajari distribusi karakter, model dapat menghasilkan teks baru secara autoregresif, satu karakter demi satu, menyerupai gaya penulisan aslinya.\n",
        "\n",
        "Karena ukuran korpus teks sangat besar‚Äîmencapai lebih dari satu juta karakter‚Äîdata tidak diproses secara utuh. Sebagai gantinya, teks dipecah menjadi banyak window pendek menggunakan tf.data.Dataset.window() dan pendekatan truncated Backpropagation Through Time (BPTT). Contohnya, urutan sepanjang 100 karakter digunakan sebagai input, dengan karakter ke-101 sebagai target prediksi.\n",
        "\n",
        "Penyusunan Dataset\n",
        "\n",
        "Setelah window dibentuk, pipeline dataset umumnya melibatkan langkah-langkah berikut:\n",
        "\n",
        "Flattening window menggunakan flat_map,\n",
        "\n",
        "pengacakan data (shuffle),\n",
        "\n",
        "pengelompokan ke dalam batch (batch),\n",
        "\n",
        "pemisahan antara:\n",
        "\n",
        "input (semua karakter kecuali yang terakhir),\n",
        "\n",
        "target (semua karakter kecuali yang pertama).\n",
        "\n",
        "Setiap karakter direpresentasikan dalam bentuk one-hot encoding menggunakan tf.one_hot.\n",
        "\n",
        "Arsitektur Model\n",
        "\n",
        "Model yang digunakan biasanya berupa:\n",
        "\n",
        "GRU dua lapis,\n",
        "\n",
        "diikuti oleh TimeDistributed(Dense) dengan fungsi aktivasi softmax.\n",
        "\n",
        "Model dilatih menggunakan fungsi loss sparse_categorical_crossentropy untuk memprediksi distribusi probabilitas seluruh karakter (misalnya 39 karakter unik) pada setiap time step.\n",
        "\n",
        "Stateless vs Stateful RNN\n",
        "\n",
        "Pada stateless RNN, hidden state selalu di-reset ke nol pada setiap batch. Akibatnya, model hanya mampu menangkap ketergantungan temporal sepanjang window yang digunakan dalam BPTT.\n",
        "\n",
        "Untuk mempelajari dependensi yang lebih panjang tanpa melakukan backpropagation pada seluruh teks, digunakan pendekatan stateful RNN. Pada metode ini, hidden state terakhir dari satu batch digunakan sebagai initial state untuk batch berikutnya.\n",
        "\n",
        "Agar stateful RNN bekerja dengan benar, beberapa syarat harus dipenuhi:\n",
        "\n",
        "batch data harus benar-benar berurutan,\n",
        "\n",
        "window dibuat tanpa overlap (shift = n_steps),\n",
        "\n",
        "batch size biasanya kecil (bahkan 1 untuk konfigurasi paling sederhana),\n",
        "\n",
        "layer RNN didefinisikan dengan stateful=True dan\n",
        "batch_input_shape = [batch_size, None, vocab_size],\n",
        "\n",
        "hidden state di-reset secara eksplisit di awal setiap epoch menggunakan callback khusus."
      ],
      "metadata": {
        "id": "8YWG9twqnd0X"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Tj03LIc6nOtm"
      },
      "outputs": [],
      "source": [
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import numpy as np\n",
        "\n",
        "# encoded: array 1D berisi ID karakter (0..max_id-1)\n",
        "dataset_size = len(encoded)\n",
        "train_size = dataset_size * 90 // 100\n",
        "dataset = tf.data.Dataset.from_tensor_slices(encoded[:train_size])\n",
        "\n",
        "n_steps = 100\n",
        "window_length = n_steps + 1\n",
        "\n",
        "dataset = dataset.window(window_length, shift=1, drop_remainder=True)\n",
        "dataset = dataset.flat_map(lambda window: window.batch(window_length))\n",
        "\n",
        "batch_size = 32\n",
        "dataset = dataset.shuffle(10000).batch(batch_size)\n",
        "dataset = dataset.map(lambda windows: (windows[:, :-1], windows[:, 1:]))\n",
        "\n",
        "max_id = 39  # jumlah karakter unik\n",
        "dataset = dataset.map(\n",
        "    lambda X_batch, Y_batch: (tf.one_hot(X_batch, depth=max_id), Y_batch)\n",
        ")\n",
        "dataset = dataset.prefetch(1)\n",
        "\n",
        "model = keras.models.Sequential([\n",
        "    keras.layers.GRU(128, return_sequences=True,\n",
        "                     input_shape=[None, max_id],\n",
        "                     dropout=0.2, recurrent_dropout=0.2),\n",
        "    keras.layers.GRU(128, return_sequences=True,\n",
        "                     dropout=0.2, recurrent_dropout=0.2),\n",
        "    keras.layers.TimeDistributed(\n",
        "        keras.layers.Dense(max_id, activation=\"softmax\")\n",
        "    )\n",
        "])\n",
        "\n",
        "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"adam\")\n",
        "history = model.fit(dataset, epochs=20)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Stateful RNN for Longer Dependencies\n",
        "\n",
        "Pada stateless RNN, hidden state selalu di-reset ke nol pada setiap batch. Konsekuensinya, model hanya mampu mempelajari ketergantungan yang terbatas pada panjang window yang di-unroll selama Backpropagation Through Time (BPTT).\n",
        "\n",
        "Untuk menangkap ketergantungan jangka lebih panjang tanpa harus melakukan backpropagation pada seluruh urutan teks, diperkenalkan konsep stateful RNN. Pada pendekatan ini, hidden state akhir dari suatu batch dipertahankan dan digunakan kembali sebagai state awal untuk batch berikutnya, sehingga informasi dapat mengalir lintas batch.\n",
        "\n",
        "Agar stateful RNN bekerja secara benar dan konsisten, beberapa syarat penting harus dipenuhi:\n",
        "\n",
        "Batch harus berisi urutan yang benar-benar saling berlanjut, bukan hasil shuffle acak.\n",
        "Dataset dibentuk menggunakan window non-overlap, dengan shift = n_steps.\n",
        "Batch size kecil sangat dianjurkan (bahkan 1 untuk konfigurasi paling sederhana).\n",
        "Layer RNN dikonfigurasi dengan stateful=True dan\n",
        "batch_input_shape = [batch_size, None, vocab_size].\n",
        "Hidden state harus di-reset secara eksplisit pada awal setiap epoch, biasanya menggunakan callback khusus.\n",
        "Dengan pengaturan ini, stateful RNN mampu memodelkan struktur dan dependensi jangka panjang secara lebih efektif dibanding stateless RNN, tanpa biaya komputasi backpropagation yang berlebihan."
      ],
      "metadata": {
        "id": "gOokE3xOniqi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "batch_size = 1  # sederhana: satu sequence per batch\n",
        "n_steps = 100\n",
        "window_length = n_steps + 1\n",
        "\n",
        "dataset = tf.data.Dataset.from_tensor_slices(encoded[:train_size])\n",
        "dataset = dataset.window(window_length, shift=n_steps, drop_remainder=True)\n",
        "dataset = dataset.flat_map(lambda window: window.batch(window_length))\n",
        "dataset = dataset.batch(batch_size)\n",
        "dataset = dataset.map(lambda windows: (windows[:, :-1], windows[:, 1:]))\n",
        "dataset = dataset.map(\n",
        "    lambda X_batch, Y_batch: (tf.one_hot(X_batch, depth=max_id), Y_batch)\n",
        ")\n",
        "dataset = dataset.prefetch(1)\n",
        "\n",
        "model = keras.models.Sequential([\n",
        "    keras.layers.GRU(\n",
        "        128, return_sequences=True, stateful=True,\n",
        "        dropout=0.2, recurrent_dropout=0.2,\n",
        "        batch_input_shape=[batch_size, None, max_id]\n",
        "    ),\n",
        "    keras.layers.GRU(\n",
        "        128, return_sequences=True, stateful=True,\n",
        "        dropout=0.2, recurrent_dropout=0.2\n",
        "    ),\n",
        "    keras.layers.TimeDistributed(\n",
        "        keras.layers.Dense(max_id, activation=\"softmax\")\n",
        "    )\n",
        "])\n",
        "\n",
        "class ResetStatesCallback(keras.callbacks.Callback):\n",
        "    def on_epoch_begin(self, epoch, logs=None):\n",
        "        self.model.reset_states()\n",
        "\n",
        "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"adam\")\n",
        "history = model.fit(dataset, epochs=50, callbacks=[ResetStatesCallback()])\n"
      ],
      "metadata": {
        "id": "5X9u3w__nmOc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Sentiment Analysis with Word-Level RNNs\n",
        "\n",
        "Setelah membahas pemodelan pada tingkat karakter, bab ini beralih ke word-level RNN untuk tugas sentiment analysis, menggunakan dataset IMDb yang berisi sekitar 50.000 ulasan film.\n",
        "\n",
        "Pada pendekatan ini, setiap kata direpresentasikan sebagai ID numerik, baik dengan memanfaatkan dataset IMDb bawaan Keras maupun dengan membangun vocabulary kustom menggunakan Tokenizer atau tf.lookup.StaticVocabularyTable. ID kata tersebut kemudian dipetakan ke vektor kontinu berdimensi rendah melalui Embedding layer, sehingga model dapat mempelajari representasi semantik kata secara end-to-end.\n",
        "\n",
        "Untuk membangun vocabulary kustom, seluruh teks diproses satu kali untuk menghitung frekuensi kata (misalnya dengan Counter). Selanjutnya dipilih sejumlah kata paling sering muncul (misalnya 10.000 kata), sementara kata lain dipetakan ke bucket out-of-vocabulary (OOV).\n",
        "\n",
        "Pipeline pemrosesan data secara keseluruhan adalah sebagai berikut:\n",
        "\n",
        "dataset di-batch,\n",
        "token teks di-preprocess,\n",
        "token di-lookup menjadi ID,\n",
        "urutan ID dimasukkan ke model RNN dua lapis berbasis GRU,\n",
        "satu neuron output dengan aktivasi sigmoid digunakan untuk memprediksi sentimen positif atau negatif.\n",
        "Pendekatan word-level ini memungkinkan model menangkap konteks semantik yang lebih kaya dibanding character-level modeling, sehingga umumnya memberikan performa yang lebih baik pada tugas analisis sentimen."
      ],
      "metadata": {
        "id": "aWiIywbVnmuv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow_datasets as tfds\n",
        "from collections import Counter\n",
        "from tensorflow import keras\n",
        "import tensorflow as tf\n",
        "\n",
        "datasets, info = tfds.load(\"imdb_reviews\", as_supervised=True, with_info=True)\n",
        "\n",
        "def preprocess(X_batch, y_batch):\n",
        "    # lowercase & split jadi tokens bytes\n",
        "    X_batch = tf.strings.regex_replace(X_batch, rb\"\", b\" \")\n",
        "    X_batch = tf.strings.regex_replace(X_batch, rb\"[^a-zA-Z']\", b\" \")\n",
        "    X_batch = tf.strings.lower(X_batch)\n",
        "    return tf.strings.split(X_batch), y_batch\n",
        "\n",
        "vocabulary = Counter()\n",
        "for X_batch, y_batch in datasets[\"train\"].batch(32).map(preprocess):\n",
        "    for review in X_batch:\n",
        "        vocabulary.update(list(review.numpy()))\n",
        "\n",
        "vocab_size = 10000\n",
        "truncated_vocabulary = [\n",
        "    word for word, count in vocabulary.most_common()[:vocab_size]\n",
        "]\n",
        "\n",
        "words = tf.constant(truncated_vocabulary)\n",
        "word_ids = tf.range(len(truncated_vocabulary), dtype=tf.int64)\n",
        "vocab_init = tf.lookup.KeyValueTensorInitializer(words, word_ids)\n",
        "num_oov_buckets = 1000\n",
        "table = tf.lookup.StaticVocabularyTable(vocab_init, num_oov_buckets)\n",
        "\n",
        "def encode_words(X_batch, y_batch):\n",
        "    return table.lookup(X_batch), y_batch\n",
        "\n",
        "train_set = datasets[\"train\"].batch(32).map(preprocess)\n",
        "train_set = train_set.map(encode_words).prefetch(1)\n",
        "\n",
        "embed_size = 128\n",
        "model = keras.models.Sequential([\n",
        "    keras.layers.Embedding(vocab_size + num_oov_buckets, embed_size,\n",
        "                           input_shape=[None]),\n",
        "    keras.layers.GRU(128, return_sequences=True),\n",
        "    keras.layers.GRU(128),\n",
        "    keras.layers.Dense(1, activation=\"sigmoid\")\n",
        "])\n",
        "model.compile(loss=\"binary_crossentropy\", optimizer=\"adam\",\n",
        "              metrics=[\"accuracy\"])\n",
        "history = model.fit(train_set, epochs=5)"
      ],
      "metadata": {
        "id": "9OYvCvc6nqxe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. Masking, Pretrained Embeddings, and Variable-Length Sequences\n",
        "\n",
        "Ulasan teks memiliki panjang yang bervariasi, sehingga perlu dilakukan padding agar dapat diproses dalam batch. Untuk mencegah model belajar dari token padding (biasanya direpresentasikan dengan ID 0), digunakan mekanisme masking, sehingga token tersebut diabaikan saat perhitungan hidden state dan loss.\n",
        "\n",
        "Pendekatan paling sederhana adalah dengan mengaktifkan opsi\n",
        "mask_zero=True pada Embedding layer. Dengan cara ini, Keras secara otomatis membuat mask untuk token bernilai nol dan meneruskannya ke layer berikutnya, seperti RNN atau TimeDistributed, yang mendukung masking.\n",
        "\n",
        "Pada arsitektur yang lebih kompleks‚Äîmisalnya kombinasi Conv1D dan GRU‚Äîmasking sering perlu ditangani secara manual. Mask dapat dihitung menggunakan ekspresi seperti K.not_equal(inputs, 0) di dalam Lambda layer, kemudian diteruskan secara eksplisit ke layer GRU melalui Functional API.\n",
        "\n",
        "Selain pendekatan berbasis RNN, bab ini juga memperkenalkan penggunaan pretrained sentence embeddings dari TensorFlow Hub sebagai alternatif yang lebih sederhana. Contohnya, model nnlm-en-dim50 dapat digunakan melalui hub.KerasLayer untuk:\n",
        "\n",
        "menerima input berupa string mentah,\n",
        "langsung menghasilkan embedding kalimat berdimensi tetap,\n",
        "kemudian hanya memerlukan satu atau beberapa Dense layer untuk tugas sentiment analysis.\n",
        "Pendekatan berbasis pretrained embeddings ini sering memberikan hasil yang kompetitif, terutama ketika data berlabel terbatas, serta secara signifikan menyederhanakan arsitektur model."
      ],
      "metadata": {
        "id": "OEX0pZMHnrUY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from tensorflow import keras\n",
        "import tensorflow.keras.backend as K\n",
        "import tensorflow_hub as hub\n",
        "\n",
        "# Masking manual di Functional API\n",
        "vocab_size = 10000; num_oov_buckets = 1000; embed_size = 128\n",
        "inputs = keras.layers.Input(shape=[None])\n",
        "mask = keras.layers.Lambda(lambda x: K.not_equal(x, 0))(inputs)\n",
        "z = keras.layers.Embedding(vocab_size + num_oov_buckets, embed_size)(inputs)\n",
        "z = keras.layers.GRU(128, return_sequences=True)(z, mask=mask)\n",
        "z = keras.layers.GRU(128)(z, mask=mask)\n",
        "outputs = keras.layers.Dense(1, activation=\"sigmoid\")(z)\n",
        "model_mask = keras.Model(inputs=inputs, outputs=outputs)\n",
        "\n",
        "# Model dengan sentence embedding dari TF Hub\n",
        "hub_layer = hub.KerasLayer(\n",
        "    \"https://tfhub.dev/google/tf2-preview/nnlm-en-dim50/1\",\n",
        "    dtype=tf.string, input_shape=[], output_shape=[50]\n",
        ")\n",
        "model_hub = keras.Sequential([\n",
        "    hub_layer,\n",
        "    keras.layers.Dense(128, activation=\"relu\"),\n",
        "    keras.layers.Dense(1, activation=\"sigmoid\")\n",
        "])\n",
        "model_hub.compile(loss=\"binary_crossentropy\", optimizer=\"adam\",\n",
        "                  metrics=[\"accuracy\"])\n",
        "train_set = datasets[\"train\"].batch(32).prefetch(1)\n",
        "history = model_hub.fit(train_set, epochs=5)"
      ],
      "metadata": {
        "id": "sAThKxBPnvNZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. Encoder‚ÄìDecoder for Machine Translation and Beam Search\n",
        "\n",
        "Bab ini membahas penerapan arsitektur encoder‚Äìdecoder pada tugas Neural Machine Translation (NMT). Dalam arsitektur ini, encoder bertugas memproses kalimat sumber (misalnya bahasa Inggris) dan merangkum informasinya ke dalam representasi state. Selanjutnya, decoder menggunakan representasi tersebut untuk menghasilkan kalimat target (misalnya bahasa Prancis) secara autoregresif, yaitu satu kata pada setiap langkah waktu.\n",
        "\n",
        "Proses Training dan Teacher Forcing\n",
        "\n",
        "Selama proses pelatihan, digunakan teknik teacher forcing, di mana input ke decoder bukan hasil prediksi sebelumnya, melainkan kalimat target asli yang digeser satu langkah ke kanan. Proses decoding biasanya diawali dengan token khusus SOS (start-of-sequence) sebagai penanda awal kalimat.\n",
        "\n",
        "Model dilatih menggunakan fungsi loss sparse_categorical_crossentropy, dengan tujuan memaksimalkan probabilitas distribusi kata target yang benar pada setiap time step.\n",
        "\n",
        "Implementasi Encoder‚ÄìDecoder\n",
        "\n",
        "Untuk mempermudah implementasi, TensorFlow Addons menyediakan API seq2seq yang mendukung pembangunan arsitektur encoder‚Äìdecoder secara modular. Beberapa komponen utama yang digunakan antara lain:\n",
        "\n",
        "LSTMCell sebagai unit rekuren pada encoder dan decoder,\n",
        "\n",
        "TrainingSampler untuk mengatur input decoder selama tahap training,\n",
        "\n",
        "BasicDecoder sebagai pengendali alur proses decoding.\n",
        "\n",
        "Pendekatan ini memungkinkan pemisahan yang jelas antara tahap encoding dan decoding, sehingga arsitektur model menjadi lebih fleksibel dan mudah dikembangkan.\n",
        "\n",
        "Greedy Decoding vs Beam Search\n",
        "\n",
        "Pada tahap inference, strategi paling sederhana adalah greedy decoding, yaitu memilih kata dengan probabilitas tertinggi pada setiap langkah. Namun, pendekatan ini sering menghasilkan terjemahan yang kurang optimal karena tidak mempertimbangkan konteks global kalimat.\n",
        "\n",
        "Sebagai solusi, digunakan beam search, yaitu teknik decoding yang:\n",
        "\n",
        "mempertahankan beberapa kandidat terjemahan terbaik secara bersamaan,\n",
        "\n",
        "memperluas seluruh kandidat tersebut pada setiap langkah waktu,\n",
        "\n",
        "kemudian hanya menyimpan beam_width urutan dengan probabilitas gabungan tertinggi.\n",
        "\n",
        "Dengan mempertimbangkan beberapa kemungkinan urutan sekaligus, beam search mampu menghasilkan terjemahan yang lebih akurat, koheren, dan natural, terutama untuk kalimat yang panjang dan kompleks."
      ],
      "metadata": {
        "id": "13RTdrC1nv53"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow_addons as tfa\n",
        "from tensorflow import keras\n",
        "import tensorflow as tf\n",
        "\n",
        "vocab_size = 20000\n",
        "embed_size = 256\n",
        "units = 512\n",
        "\n",
        "encoder_inputs = keras.layers.Input(shape=[None], dtype=np.int32)\n",
        "decoder_inputs = keras.layers.Input(shape=[None], dtype=np.int32)\n",
        "sequence_lengths = keras.layers.Input(shape=[], dtype=np.int32)\n",
        "\n",
        "embeddings = keras.layers.Embedding(vocab_size, embed_size)\n",
        "encoder_emb = embeddings(encoder_inputs)\n",
        "decoder_emb = embeddings(decoder_inputs)\n",
        "\n",
        "encoder = keras.layers.LSTM(units, return_state=True)\n",
        "encoder_outputs, state_h, state_c = encoder(encoder_emb)\n",
        "encoder_state = [state_h, state_c]\n",
        "\n",
        "sampler = tfa.seq2seq.sampler.TrainingSampler()\n",
        "decoder_cell = keras.layers.LSTMCell(units)\n",
        "output_layer = keras.layers.Dense(vocab_size)\n",
        "\n",
        "decoder = tfa.seq2seq.basic_decoder.BasicDecoder(\n",
        "    decoder_cell, sampler, output_layer=output_layer\n",
        ")\n",
        "final_outputs, final_state, final_seq_lengths = decoder(\n",
        "    decoder_emb, initial_state=encoder_state,\n",
        "    sequence_length=sequence_lengths\n",
        ")\n",
        "Y_proba = tf.nn.softmax(final_outputs.rnn_output)\n",
        "\n",
        "model = keras.Model(\n",
        "    inputs=[encoder_inputs, decoder_inputs, sequence_lengths],\n",
        "    outputs=[Y_proba]\n",
        ")\n",
        "\n",
        "# Beam search decoding (inference)\n",
        "beam_width = 10\n",
        "beam_decoder = tfa.seq2seq.beam_search_decoder.BeamSearchDecoder(\n",
        "    cell=decoder_cell, beam_width=beam_width, output_layer=output_layer\n",
        ")\n",
        "encoder_state_beam = tfa.seq2seq.beam_search_decoder.tile_batch(\n",
        "    encoder_state, multiplier=beam_width\n",
        ")\n",
        "start_tokens = tf.fill([batch_size], sos_id)\n",
        "end_token = eos_id\n",
        "outputs, _, _ = beam_decoder(\n",
        "    decoder_emb, start_tokens=start_tokens, end_token=end_token,\n",
        "    initial_state=encoder_state_beam\n",
        ")\n"
      ],
      "metadata": {
        "id": "PLAqpqAmn87H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "6. Attention Mechanisms and Transformer Architecture\n",
        "\n",
        "Mekanisme attention memungkinkan decoder untuk secara dinamis memfokuskan perhatian pada bagian input yang paling relevan di setiap langkah waktu. Dengan cara ini, jalur informasi dari kata sumber ke kata target jadi jauh lebih pendek dibanding arsitektur encoder‚Äìdecoder klasik. Dampaknya cukup besar: kualitas terjemahan meningkat signifikan, terutama saat menangani kalimat yang panjang dan kompleks.\n",
        "\n",
        "Pada Bahdanau attention, yang juga dikenal sebagai additive atau concatenative attention, setiap output encoder dikombinasikan dengan hidden state decoder untuk menghitung skor kesesuaian\n",
        "ùëí\n",
        "(\n",
        "ùë°\n",
        ",\n",
        "ùëñ\n",
        ")\n",
        "e(t,i) menggunakan sebuah jaringan saraf kecil. Skor ini kemudian dinormalisasi dengan softmax untuk menghasilkan bobot perhatian\n",
        "ùõº\n",
        "(\n",
        "ùë°\n",
        ",\n",
        "ùëñ\n",
        ")\n",
        "Œ±(t,i). Selanjutnya, context vector pada waktu\n",
        "ùë°\n",
        "t dihitung sebagai kombinasi linear dari seluruh output encoder yang ditimbang oleh bobot perhatian tersebut, lalu digabungkan dengan hidden state decoder untuk menghasilkan prediksi kata berikutnya.\n",
        "\n",
        "Sebagai alternatif yang lebih efisien, Luong attention menggunakan pendekatan multiplicative dengan menghitung skor perhatian melalui dot product antara hidden state decoder dan output encoder, atau versi general yang menambahkan transformasi linear. Metode ini lebih ringan secara komputasi dan dalam banyak kasus justru memberikan performa yang lebih baik. TensorFlow Addons sendiri sudah menyediakan implementasi siap pakai seperti LuongAttention dan AttentionWrapper yang memudahkan integrasi attention ke dalam decoder.\n",
        "\n",
        "Bab ini kemudian beralih ke Transformer, yaitu arsitektur yang sepenuhnya meninggalkan RNN dan CNN, dan hanya mengandalkan attention untuk memodelkan dependensi dalam data sekuens. Transformer mengombinasikan multi-head attention, feed-forward network yang diaplikasikan per posisi, layer normalization, residual connection, serta positional encoding berbasis fungsi sinus dan kosinus agar informasi urutan tetap terjaga.\n",
        "\n",
        "Inti dari Transformer adalah scaled dot-product attention, yang menghitung perhatian dengan mengalikan query dan key, menskalakannya dengan akar dimensi key, lalu menerapkan softmax sebelum dikalikan dengan value. Pada multi-head attention, query, key, dan value diproyeksikan ke beberapa ruang representasi berbeda sehingga setiap head dapat mempelajari pola perhatian yang beragam, seperti hubungan posisi, struktur sintaks, maupun makna semantik. Hasil dari seluruh head kemudian digabungkan kembali. Berkat kemampuannya memodelkan dependensi jangka panjang secara paralel dan efisien, Transformer menjadi fondasi utama bagi hampir semua model NLP modern, termasuk BERT, GPT, dan sistem penerjemahan mutakhir."
      ],
      "metadata": {
        "id": "zxNgNIl4oC-b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow_addons as tfa\n",
        "from tensorflow import keras\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "\n",
        "# Luong Attention + AttentionWrapper\n",
        "units = 512\n",
        "encoder_outputs = ...              # [batch, time_src, units]\n",
        "encoder_seq_len = ...              # [batch]\n",
        "decoder_cell = keras.layers.LSTMCell(units)\n",
        "\n",
        "attention_mech = tfa.seq2seq.attention_wrapper.LuongAttention(\n",
        "    units, encoder_outputs,\n",
        "    memory_sequence_length=encoder_seq_len\n",
        ")\n",
        "attn_cell = tfa.seq2seq.attention_wrapper.AttentionWrapper(\n",
        "    decoder_cell, attention_mech, attention_layer_size=units\n",
        ")\n",
        "\n",
        "# Positional Encoding layer\n",
        "class PositionalEncoding(keras.layers.Layer):\n",
        "    def __init__(self, max_steps, max_dims, dtype=tf.float32, **kwargs):\n",
        "        super().__init__(dtype=dtype, **kwargs)\n",
        "        if max_dims % 2 == 1:\n",
        "            max_dims += 1\n",
        "        p, i = np.meshgrid(np.arange(max_steps), np.arange(max_dims // 2))\n",
        "        pos_emb = np.empty((1, max_steps, max_dims))\n",
        "        pos_emb[:, :, 0::2] = np.sin(p / 10000 ** (2 * i / max_dims)).T\n",
        "        pos_emb[:, :, 1::2] = np.cos(p / 10000 ** (2 * i / max_dims)).T\n",
        "        self.positional_embedding = tf.constant(pos_emb.astype(self.dtype))\n",
        "\n",
        "    def call(self, inputs):\n",
        "        shape = tf.shape(inputs)\n",
        "        return inputs + self.positional_embedding[:, :shape[-2], :shape[-1]]\n",
        "\n",
        "# Transformer-like skeleton using keras.layers.Attention\n",
        "embed_size = 512; max_steps = 500; vocab_size = 10000\n",
        "encoder_inputs = keras.layers.Input(shape=[None], dtype=np.int32)\n",
        "decoder_inputs = keras.layers.Input(shape=[None], dtype=np.int32)\n",
        "\n",
        "embeddings = keras.layers.Embedding(vocab_size, embed_size)\n",
        "enc_emb = embeddings(encoder_inputs)\n",
        "dec_emb = embeddings(decoder_inputs)\n",
        "\n",
        "pos_enc = PositionalEncoding(max_steps, embed_size)\n",
        "encoder_in = pos_enc(enc_emb)\n",
        "decoder_in = pos_enc(dec_emb)\n",
        "\n",
        "Z = encoder_in\n",
        "for _ in range(6):\n",
        "    Z = keras.layers.Attention(use_scale=True)([Z, Z])\n",
        "encoder_outputs = Z\n",
        "\n",
        "Z = decoder_in\n",
        "for _ in range(6):\n",
        "    Z = keras.layers.Attention(use_scale=True, causal=True)([Z, Z])\n",
        "    Z = keras.layers.Attention(use_scale=True)([Z, encoder_outputs])\n",
        "\n",
        "outputs = keras.layers.TimeDistributed(\n",
        "    keras.layers.Dense(vocab_size, activation=\"softmax\")\n",
        ")(Z)\n",
        "\n",
        "transformer = keras.Model(\n",
        "    inputs=[encoder_inputs, decoder_inputs],\n",
        "    outputs=outputs\n",
        ")\n"
      ],
      "metadata": {
        "id": "gX4A1EUcoF2m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "7. Recent Language Models: ELMo, ULMFiT, GPT, and BERT\n",
        "\n",
        "Bagian penutup bab ini membahas lompatan besar di dunia Natural Language Processing pada rentang 2018‚Äì2019, periode ketika cara merepresentasikan dan memodelkan bahasa berubah drastis. Fokus utama pergeseran ini adalah pemanfaatan pretraining skala besar dan representasi bahasa yang jauh lebih kontekstual, sehingga model tidak lagi memahami kata secara terisolasi, melainkan sebagai bagian dari konteks kalimat yang utuh.\n",
        "\n",
        "ELMo memperkenalkan konsep contextualized word embeddings, di mana representasi sebuah kata diambil dari state internal language model bidirectional yang dalam. Dengan pendekatan ini, satu kata yang sama bisa memiliki embedding berbeda tergantung konteks penggunaannya, sehingga makna ganda atau polisemik dapat ditangkap dengan jauh lebih baik dibanding embedding statis seperti Word2Vec atau GloVe.\n",
        "\n",
        "ULMFiT menunjukkan bahwa language model berbasis LSTM yang dipretrain menggunakan self-supervised learning dapat di-fine-tune secara bertahap untuk berbagai tugas downstream. Strategi fine-tuning ini terbukti sangat efektif, bahkan ketika data berlabel terbatas, dan menjadi salah satu pijakan penting lahirnya konsep transfer learning modern di NLP.\n",
        "\n",
        "GPT dan GPT-2 mengadopsi arsitektur Transformer decoder dengan masked multi-head attention dan dilatih sebagai language model autoregresif. Melalui pretraining skala besar, model ini mampu menunjukkan kemampuan zero-shot dan few-shot learning pada berbagai tugas NLP, hanya dengan mengandalkan pengetahuan bahasa yang telah dipelajari tanpa perlu pelatihan ulang yang berat.\n",
        "\n",
        "BERT kemudian membawa pendekatan berbeda dengan menggunakan Transformer encoder bidirectional. Model ini dipretrain menggunakan dua objektif utama, yaitu Masked Language Model yang memprediksi token tersembunyi dari konteks dua arah, serta Next Sentence Prediction untuk mempelajari hubungan antar-kalimat. Hasilnya adalah representasi bahasa yang sangat kuat dan fleksibel, yang dapat di-fine-tune dengan ringan untuk beragam tugas seperti klasifikasi teks, question answering, dan natural language inference.\n",
        "\n",
        "Secara keseluruhan, terobosan utama dari model-model ini terletak pada penggunaan tokenisasi berbasis subword, peralihan arsitektur dari LSTM ke Transformer, serta pemanfaatan self-supervised pretraining skala besar yang kemudian dapat diadaptasi secara efisien ke banyak tugas NLP downstream."
      ],
      "metadata": {
        "id": "pM5Dxbk2oIMD"
      }
    }
  ]
}