{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Rahmanda Afebrio Yuris Soesatyo - Chapter 13:,Loading and Preprocessing Data with TensorFlow"
      ],
      "metadata": {
        "id": "jnlS1nqni41Z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Dasar-dasar Data API\n",
        "\n",
        "tf.data API menyediakan abstraksi Dataset, yaitu representasi urutan data (sequence of elements) yang bisa berasal dari memori maupun file. Dataset ini dirancang supaya proses loading dan preprocessing data bisa dilakukan cepat, efisien, dan scalable.\n",
        "\n",
        "Salah satu konsep penting di tf.data adalah immutability. Artinya, setiap operasi seperti map, batch, atau repeat tidak mengubah Dataset yang ada, melainkan menghasilkan Dataset baru. Karena itu, transformasi data bisa dirangkai dengan mudah menggunakan method chaining, membentuk pipeline yang fleksibel dan mudah dibaca.\n",
        "\n",
        "Operasi Inti pada Dataset\n",
        "\n",
        "Beberapa operasi utama yang sering digunakan dalam tf.data.Dataset antara lain:\n",
        "\n",
        "map\n",
        "Menerapkan fungsi preprocessing ke setiap elemen dataset.\n",
        "\n",
        "filter\n",
        "Menyaring data berdasarkan kondisi tertentu.\n",
        "\n",
        "batch / unbatch\n",
        "Mengelompokkan data ke dalam batch atau memecah batch kembali menjadi elemen tunggal.\n",
        "\n",
        "repeat\n",
        "Mengulang dataset untuk beberapa epoch.\n",
        "\n",
        "shuffle\n",
        "Mengacak urutan data agar distribusi lebih acak.\n",
        "\n",
        "take\n",
        "Mengambil sejumlah elemen pertama dari dataset.\n",
        "\n",
        "interleave\n",
        "Membaca data dari beberapa sumber secara paralel untuk meningkatkan throughput.\n",
        "\n",
        "prefetch\n",
        "Melakukan overlap antara preprocessing di CPU dan training di GPU, sehingga pipeline tidak menjadi bottleneck.\n",
        "\n",
        "Operasi-operasi ini umumnya dikombinasikan untuk membangun pipeline training yang i.i.d., teracak"
      ],
      "metadata": {
        "id": "-brFy8ePivwf"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VZAS081VivSF",
        "outputId": "cd110d46-4128-4550-a721-e10364152205"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:From /tmp/ipython-input-2072617712.py:19: unbatch (from tensorflow.python.data.experimental.ops.batching) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.data.Dataset.unbatch()`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor(0, shape=(), dtype=int32)\n",
            "tf.Tensor(1, shape=(), dtype=int32)\n",
            "tf.Tensor(2, shape=(), dtype=int32)\n",
            "tf.Tensor(3, shape=(), dtype=int32)\n",
            "tf.Tensor(4, shape=(), dtype=int32)\n",
            "tf.Tensor(5, shape=(), dtype=int32)\n",
            "tf.Tensor(6, shape=(), dtype=int32)\n",
            "tf.Tensor(7, shape=(), dtype=int32)\n",
            "tf.Tensor(8, shape=(), dtype=int32)\n",
            "tf.Tensor(9, shape=(), dtype=int32)\n",
            "tf.Tensor([0 1 2 3 4 5 6], shape=(7,), dtype=int32)\n",
            "tf.Tensor([7 8 9 0 1 2 3], shape=(7,), dtype=int32)\n",
            "tf.Tensor([4 5 6 7 8 9 0], shape=(7,), dtype=int32)\n",
            "tf.Tensor([1 2 3 4 5 6 7], shape=(7,), dtype=int32)\n",
            "tf.Tensor([8 9], shape=(2,), dtype=int32)\n",
            "tf.Tensor(0, shape=(), dtype=int32)\n",
            "tf.Tensor(2, shape=(), dtype=int32)\n",
            "tf.Tensor(4, shape=(), dtype=int32)\n",
            "tf.Tensor([0 2 3 6 7 9 4], shape=(7,), dtype=int64)\n",
            "tf.Tensor([5 0 1 1 8 6 5], shape=(7,), dtype=int64)\n",
            "tf.Tensor([4 8 7 1 2 3 0], shape=(7,), dtype=int64)\n",
            "tf.Tensor([5 4 2 7 8 9 9], shape=(7,), dtype=int64)\n",
            "tf.Tensor([3 6], shape=(2,), dtype=int64)\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "# 1) Dataset sederhana dari tensor\n",
        "X = tf.range(10)\n",
        "dataset = tf.data.Dataset.from_tensor_slices(X)\n",
        "\n",
        "for item in dataset:\n",
        "    print(item)  # 0..9\n",
        "\n",
        "# 2) Chaining: repeat, batch\n",
        "dataset = dataset.repeat(3).batch(7)\n",
        "for item in dataset:\n",
        "    print(item)\n",
        "\n",
        "# 3) Map: preprocessing per item\n",
        "dataset = dataset.map(lambda x: x * 2)  # 0,2,4,...\n",
        "\n",
        "# 4) Unbatch (experimental) + filter + take\n",
        "dataset = dataset.apply(tf.data.experimental.unbatch())\n",
        "dataset = dataset.filter(lambda x: x < 10)\n",
        "for item in dataset.take(3):\n",
        "    print(item)\n",
        "\n",
        "# 5) Shuffle + batch\n",
        "dataset = tf.data.Dataset.range(10).repeat(3)\n",
        "dataset = dataset.shuffle(buffer_size=5, seed=42).batch(7)\n",
        "for batch in dataset:\n",
        "    print(batch)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. CSV Input Pipelines dengan tf.data\n",
        "\n",
        "Untuk dataset tabular berukuran besar—seperti California Housing—dibutuhkan pipeline input yang efisien agar proses training tidak terhambat oleh I/O. Pola pipeline yang umum digunakan biasanya sebagai berikut:\n",
        "\n",
        "list_files\n",
        "Mengumpulkan daftar file CSV yang akan dibaca.\n",
        "\n",
        "interleave dengan beberapa TextLineDataset\n",
        "Membaca banyak file secara paralel, sambil melewati baris header.\n",
        "\n",
        "map(preprocess)\n",
        "Menerapkan fungsi preprocessing ke setiap baris data.\n",
        "\n",
        "shuffle\n",
        "Mengacak data untuk menghindari bias urutan.\n",
        "\n",
        "repeat (opsional)\n",
        "Mengulang dataset untuk beberapa epoch.\n",
        "\n",
        "batch\n",
        "Mengelompokkan data ke dalam batch.\n",
        "\n",
        "prefetch\n",
        "Menjalankan preprocessing di CPU secara paralel dengan training di GPU.\n",
        "\n",
        "Pipeline seperti ini memungkinkan:\n",
        "\n",
        "pembacaan data dari banyak file secara paralel,\n",
        "\n",
        "proses shuffling yang lebih efektif,\n",
        "\n",
        "overlap antara loading/preprocessing dan proses training,\n",
        "\n",
        "sehingga bottleneck I/O bisa diminimalkan secara signifikan.\n",
        "\n",
        "Preprocessing Data CSV\n",
        "\n",
        "Tahap preprocessing CSV biasanya dilakukan langsung di dalam pipeline tf.data, dengan memanfaatkan:\n",
        "\n",
        "tf.io.decode_csv\n",
        "Untuk mem-parsing setiap baris CSV menjadi tensor fitur dan label.\n",
        "\n",
        "Operasi scaling\n",
        "Seperti normalisasi atau standardisasi, yang diterapkan langsung setelah parsing.\n",
        "\n",
        "Dengan pendekatan ini, seluruh proses input data—mulai dari membaca file hingga preprocessing—tetap berada di dalam TensorFlow computation graph, sehingga bisa dioptimalkan secara end-to-end dan berjalan lebih efisien."
      ],
      "metadata": {
        "id": "Ul_5N3KBjfyM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import numpy as np\n",
        "\n",
        "# Precomputed mean & std per feature (misal dari NumPy)\n",
        "X_mean = tf.constant([...], dtype=tf.float32)\n",
        "X_std  = tf.constant([...], dtype=tf.float32)\n",
        "n_inputs = 8\n",
        "\n",
        "def preprocess(line):\n",
        "    # record_defaults: 8 feature float (default 0.), 1 target float (no default)\n",
        "    defs = [0.] * n_inputs + [tf.constant((), dtype=tf.float32)]\n",
        "    fields = tf.io.decode_csv(line, record_defaults=defs)\n",
        "    x = tf.stack(fields[:-1])\n",
        "    y = tf.stack(fields[-1:])\n",
        "    x = (x - X_mean) / X_std\n",
        "    return x, y\n",
        "\n",
        "def csv_reader_dataset(filepaths, repeat=1, n_readers=5,\n",
        "                       n_read_threads=None, shuffle_buffer_size=10000,\n",
        "                       n_parse_threads=5, batch_size=32):\n",
        "    dataset = tf.data.Dataset.list_files(filepaths)\n",
        "    dataset = dataset.interleave(\n",
        "        lambda fp: tf.data.TextLineDataset(fp).skip(1),\n",
        "        cycle_length=n_readers,\n",
        "        num_parallel_calls=n_read_threads\n",
        "    )\n",
        "    dataset = dataset.map(preprocess,\n",
        "                          num_parallel_calls=n_parse_threads)\n",
        "    dataset = dataset.shuffle(shuffle_buffer_size).repeat(repeat)\n",
        "    dataset = dataset.batch(batch_size).prefetch(1)\n",
        "    return dataset\n",
        "\n",
        "train_set = csv_reader_dataset(train_filepaths)\n",
        "valid_set = csv_reader_dataset(valid_filepaths)\n",
        "test_set  = csv_reader_dataset(test_filepaths)\n",
        "\n",
        "model = keras.models.Sequential([\n",
        "    keras.layers.Dense(30, activation=\"elu\",\n",
        "                       kernel_initializer=\"he_normal\"),\n",
        "    keras.layers.Dense(1)\n",
        "])\n",
        "model.compile(loss=\"mse\", optimizer=\"nadam\")\n",
        "model.fit(train_set, epochs=10, validation_data=valid_set)\n",
        "model.evaluate(test_set)\n"
      ],
      "metadata": {
        "id": "_JG_xlSujjNO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. TFRecord Format & Protocol Buffers\n",
        "\n",
        "TFRecord merupakan format file biner sederhana yang menyimpan data sebagai rangkaian record dengan panjang variabel. Format ini sangat cocok digunakan untuk dataset berskala besar dan bertipe kompleks—seperti gambar, audio, maupun tensor arbitrer—karena:\n",
        "\n",
        "proses baca jauh lebih cepat dibandingkan format teks (misalnya CSV),\n",
        "\n",
        "ukuran file lebih ringkas dan efisien,\n",
        "\n",
        "mudah diparalelkan dalam pipeline input TensorFlow.\n",
        "\n",
        "Dalam praktiknya, setiap record TFRecord biasanya berisi objek Protocol Buffers khusus TensorFlow, yaitu:\n",
        "\n",
        "Example\n",
        "Digunakan untuk satu data contoh dengan struktur tetap (fixed-length).\n",
        "\n",
        "SequenceExample\n",
        "Digunakan untuk data berurutan, seperti time series, teks, atau urutan frame.\n",
        "\n",
        "Proses parsing dilakukan sepenuhnya di dalam TensorFlow menggunakan operasi bawaan, seperti:\n",
        "\n",
        "tf.io.parse_single_example\n",
        "\n",
        "tf.io.parse_single_sequence_example\n",
        "\n",
        "Typical TFRecord Pipeline\n",
        "\n",
        "Penggunaan TFRecord umumnya dibagi menjadi dua tahap utama:\n",
        "\n",
        "Konversi Offline\n",
        "Data mentah (misalnya CSV atau folder gambar) diubah terlebih dahulu menjadi file TFRecord yang berisi objek Example atau SequenceExample.\n",
        "\n",
        "Pipeline Saat Training\n",
        "Pada tahap pelatihan, pipeline biasanya meliputi:\n",
        "\n",
        "TFRecordDataset untuk membaca file,\n",
        "\n",
        "parsing protobuf menjadi tensor,\n",
        "\n",
        "preprocessing standar seperti map, batch, dan prefetch.\n",
        "\n",
        "Pendekatan ini memisahkan proses preprocessing berat dari training loop, sehingga kinerja I/O meningkat secara signifikan dan proses pelatihan menjadi lebih efisien serta stabil."
      ],
      "metadata": {
        "id": "ZZvKj5fiji52"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "# Write raw strings to TFRecord\n",
        "with tf.io.TFRecordWriter(\"my_data.tfrecord\") as f:\n",
        "    f.write(b\"This is the first record\")\n",
        "    f.write(b\"And this is the second record\")\n",
        "\n",
        "# Read TFRecord\n",
        "dataset = tf.data.TFRecordDataset([\"my_data.tfrecord\"])\n",
        "for item in dataset:\n",
        "    print(item)  # scalar string tensors\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xvOJ8nMpkbbJ",
        "outputId": "3ccd5062-2c80-4e6d-fc1f-a2ab1eade3bc"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor(b'This is the first record', shape=(), dtype=string)\n",
            "tf.Tensor(b'And this is the second record', shape=(), dtype=string)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from tensorflow.train import BytesList, FloatList, Int64List\n",
        "from tensorflow.train import Feature, Features, Example\n",
        "\n",
        "# Build Example\n",
        "person_example = Example(\n",
        "    features=Features(\n",
        "        feature={\n",
        "            \"name\":   Feature(bytes_list=BytesList(value=[b\"Alice\"])),\n",
        "            \"id\":     Feature(int64_list=Int64List(value=[123])),\n",
        "            \"emails\": Feature(bytes_list=BytesList(\n",
        "                value=[b\"a@b.com\", b\"c@d.com\"]\n",
        "            )),\n",
        "        }\n",
        "    )\n",
        ")\n",
        "\n",
        "with tf.io.TFRecordWriter(\"my_contacts.tfrecord\") as f:\n",
        "    f.write(person_example.SerializeToString())\n",
        "\n",
        "# Parse with tf.io.parse_single_example\n",
        "feature_description = {\n",
        "    \"name\":   tf.io.FixedLenFeature([], tf.string, default_value=\"\"),\n",
        "    \"id\":     tf.io.FixedLenFeature([], tf.int64, default_value=0),\n",
        "    \"emails\": tf.io.VarLenFeature(tf.string),\n",
        "}\n",
        "\n",
        "for serialized in tf.data.TFRecordDataset([\"my_contacts.tfrecord\"]):\n",
        "    parsed = tf.io.parse_single_example(serialized, feature_description)\n",
        "    name   = parsed[\"name\"]\n",
        "    pid    = parsed[\"id\"]\n",
        "    emails = parsed[\"emails\"].values  # sparse → dense values\n"
      ],
      "metadata": {
        "id": "UEi5tdxfkdfj"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. Preprocessing Numerical & Categorical Features\n",
        "\n",
        "Preprocessing fitur numerik dan kategorikal dapat dilakukan pada beberapa level, tergantung kebutuhan eksperimen dan target deployment model.\n",
        "\n",
        "Level Preprocessing\n",
        "\n",
        "Di luar TensorFlow\n",
        "Menggunakan library seperti NumPy, pandas, atau scikit-learn, misalnya dengan:\n",
        "\n",
        "StandardScaler untuk standardisasi fitur numerik\n",
        "\n",
        "OneHotEncoder untuk fitur kategorikal\n",
        "\n",
        "Pendekatan ini cocok untuk eksplorasi awal, tetapi kurang ideal untuk production karena preprocessing terpisah dari model.\n",
        "\n",
        "Di dalam pipeline tf.data\n",
        "Preprocessing dilakukan melalui map(preprocess_fn) sehingga:\n",
        "\n",
        "berjalan paralel,\n",
        "\n",
        "terintegrasi langsung dengan pipeline training,\n",
        "\n",
        "mengurangi overhead pemrosesan terpisah.\n",
        "\n",
        "Sebagai preprocessing layer di dalam model Keras\n",
        "Pendekatan ini paling direkomendasikan untuk production, karena:\n",
        "\n",
        "preprocessing ikut tersimpan dan diekspor bersama model,\n",
        "\n",
        "konsistensi antara training dan inference terjamin,\n",
        "\n",
        "mengurangi risiko perbedaan logika preprocessing di lingkungan berbeda.\n",
        "\n",
        "Contoh Pendekatan\n",
        "\n",
        "Standardization layer\n",
        "Layer kustom atau bawaan Keras yang menggunakan metode adapt, dengan konsep serupa StandardScaler.\n",
        "\n",
        "Encoding fitur kategorikal, menggunakan:\n",
        "\n",
        "lookup table (StringLookup atau IntegerLookup),\n",
        "\n",
        "one-hot encoding,\n",
        "\n",
        "atau embedding layer untuk fitur kategorikal dengan kardinalitas besar.\n",
        "\n",
        "Pendekatan berbasis layer preprocessing memastikan bahwa alur data tetap konsisten dari tahap pelatihan hingga inferensi, sehingga lebih andal untuk sistem machine learning di lingkungan produksi."
      ],
      "metadata": {
        "id": "BkWCcKeZkgMc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from tensorflow import keras\n",
        "\n",
        "class Standardization(keras.layers.Layer):\n",
        "    def adapt(self, data_sample):\n",
        "        self.means_ = np.mean(data_sample, axis=0, keepdims=True)\n",
        "        self.stds_  = np.std(data_sample, axis=0, keepdims=True)\n",
        "\n",
        "    def call(self, inputs):\n",
        "        eps = keras.backend.epsilon()\n",
        "        return (inputs - self.means_) / (self.stds_ + eps)\n",
        "\n",
        "# Usage\n",
        "std_layer = Standardization()\n",
        "std_layer.adapt(X_train_sample)\n",
        "\n",
        "model = keras.models.Sequential([\n",
        "    std_layer,\n",
        "    keras.layers.Dense(30, activation=\"relu\"),\n",
        "    keras.layers.Dense(1)\n",
        "])"
      ],
      "metadata": {
        "id": "yJaLwkrCkp1J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "vocab = [\"<1H OCEAN\", \"INLAND\", \"NEAR OCEAN\", \"NEAR BAY\", \"ISLAND\"]\n",
        "indices = tf.range(len(vocab), dtype=tf.int64)\n",
        "table_init = tf.lookup.KeyValueTensorInitializer(vocab, indices)\n",
        "\n",
        "num_oov_buckets = 2\n",
        "table = tf.lookup.StaticVocabularyTable(table_init, num_oov_buckets)\n",
        "\n",
        "categories = tf.constant([\"NEAR BAY\", \"DESERT\", \"INLAND\", \"INLAND\"])\n",
        "cat_indices = table.lookup(categories)\n",
        "\n",
        "cat_one_hot = tf.one_hot(\n",
        "    cat_indices,\n",
        "    depth=len(vocab) + num_oov_buckets\n",
        ")"
      ],
      "metadata": {
        "id": "reAnvWdukt7v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. TF Transform & Keras Preprocessing Layers\n",
        "TensorFlow Transform (TF Transform / TFT) adalah bagian dari TFX yang memungkinkan preprocessing didefinisikan satu kali dan digunakan secara konsisten untuk training dan serving.\n",
        "\n",
        "TF Transform bekerja dengan cara:\n",
        "\n",
        "menjalankan preprocessing secara batch pada seluruh training set menggunakan Apache Beam,\n",
        "lalu mengompilasi preprocessing tersebut menjadi TF Function yang dapat ditanam langsung ke model saat serving.\n",
        "Keunggulan utama pendekatan ini adalah menghindari train/serving skew, karena preprocessing yang sama persis digunakan di kedua fase.\n",
        "\n",
        "Keras Preprocessing Layers\n",
        "Sebagai alternatif yang lebih ringan, Keras menyediakan preprocessing layers bawaan, antara lain:\n",
        "\n",
        "Normalization\n",
        "Discretization\n",
        "TextVectorization\n",
        "StringLookup / IntegerLookup\n",
        "Pola penggunaannya umumnya:\n",
        "\n",
        "Membuat layer preprocessing\n",
        "Memanggil adapt(data_sample)\n",
        "Menggunakannya sebagai layer pertama dalam model\n",
        "Pendekatan ini mudah diintegrasikan dan cocok untuk sebagian besar use case non-TFX."
      ],
      "metadata": {
        "id": "J0m0G6RNkyWv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow_transform as tft\n",
        "\n",
        "def preprocess(inputs):\n",
        "    median_age      = inputs[\"housing_median_age\"]\n",
        "    ocean_proximity = inputs[\"ocean_proximity\"]\n",
        "\n",
        "    standardized_age = tft.scale_to_z_score(median_age)\n",
        "    ocean_id = tft.compute_and_apply_vocabulary(ocean_proximity)\n",
        "\n",
        "    return {\n",
        "        \"standardized_median_age\": standardized_age,\n",
        "        \"ocean_proximity_id\": ocean_id,\n",
        "    }"
      ],
      "metadata": {
        "id": "FDio9TLgk0sJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "6. TensorFlow Datasets (TFDS)\n",
        "TensorFlow Datasets (TFDS) menyediakan akses mudah ke berbagai dataset populer, seperti:\n",
        "\n",
        "MNIST, Fashion-MNIST\n",
        "CIFAR\n",
        "ImageNet\n",
        "berbagai dataset NLP dan speech\n",
        "TFDS secara otomatis:\n",
        "\n",
        "mendownload dataset,\n",
        "memverifikasi checksum,\n",
        "dan menyajikannya sebagai tf.data.Dataset siap pakai.\n",
        "TFDS Usage Patterns\n",
        "tfds.load() dapat mengembalikan:\n",
        "\n",
        "dictionary dataset (misalnya {\"train\": ds_train, \"test\": ds_test}), atau\n",
        "pasangan (features, labels) jika as_supervised=True.\n",
        "Pipeline yang umum digunakan:\n",
        "\n",
        "tfds.load\n",
        "shuffle\n",
        "batch\n",
        "prefetch\n",
        "langsung digunakan pada model.fit()\n",
        "TFDS sangat membantu untuk eksperimen cepat, benchmarking, dan pembelajaran tanpa harus menulis pipeline input dari nol."
      ],
      "metadata": {
        "id": "BKt-NNDgk4b_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow_datasets as tfds\n",
        "from tensorflow import keras\n",
        "\n",
        "dataset = tfds.load(\n",
        "    name=\"mnist\",\n",
        "    batch_size=32,\n",
        "    as_supervised=True\n",
        ")\n",
        "mnist_train, mnist_test = dataset[\"train\"], dataset[\"test\"]\n",
        "\n",
        "mnist_train = mnist_train.prefetch(1)\n",
        "\n",
        "model = keras.models.Sequential([\n",
        "    keras.layers.Reshape([28, 28, 1], input_shape=[28, 28, 1]),\n",
        "    keras.layers.Flatten(),\n",
        "    keras.layers.Dense(300, activation=\"relu\"),\n",
        "    keras.layers.Dense(100, activation=\"relu\"),\n",
        "    keras.layers.Dense(10, activation=\"softmax\"),\n",
        "])\n",
        "\n",
        "model.compile(loss=\"sparse_categorical_crossentropy\",\n",
        "              optimizer=\"sgd\",\n",
        "              metrics=[\"accuracy\"])\n",
        "\n",
        "model.fit(mnist_train, epochs=5)\n"
      ],
      "metadata": {
        "id": "qbr8Ry_Sk6Ty"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}