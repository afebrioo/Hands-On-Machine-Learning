{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Rahmanda Afebrio Yuris Soesatyo - Chapter 19:  Training and Deploying TensorFlow Models at Scale"
      ],
      "metadata": {
        "id": "jWCisaE8qapY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Chapter 19 - Training and Deploying TensorFlow Models at Scale\n",
        "\n",
        "1. SavedModel dan TensorFlow Serving\n",
        "Setelah pelatihan, model tf.keras biasanya diekspor ke format SavedModel, yang menyimpan:\n",
        "graph komputasi,\n",
        "bobot model,\n",
        "aset tambahan (mis. vocabulary atau contoh data).\n",
        "SavedModel berbentuk folder dengan struktur utama:\n",
        "saved_model.pb\n",
        "variables/ (file bobot)\n",
        "assets/ (opsional).\n",
        "Model dapat dimuat kembali menggunakan:\n",
        "tf.saved_model.load() atau\n",
        "keras.models.load_model().\n",
        "TensorFlow Serving adalah server C++ berperforma tinggi untuk deployment model, dengan fitur:\n",
        "serving multi-versi model,\n",
        "auto-reload dan rollback versi,\n",
        "batching request otomatis.\n",
        "Model dapat diakses melalui REST (HTTP JSON) atau gRPC, dan mudah dijalankan menggunakan Docker."
      ],
      "metadata": {
        "id": "ks6trJ9Jqbw0"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Cfccz1IbqYIW"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import numpy as np\n",
        "\n",
        "# 1) Train simple Keras model (contoh MNIST)\n",
        "model = keras.models.Sequential([\n",
        "    keras.layers.Flatten(input_shape=[28, 28]),\n",
        "    keras.layers.Dense(300, activation=\"relu\"),\n",
        "    keras.layers.Dense(100, activation=\"relu\"),\n",
        "    keras.layers.Dense(10, activation=\"softmax\"),\n",
        "])\n",
        "model.compile(loss=\"sparse_categorical_crossentropy\",\n",
        "              optimizer=\"sgd\",\n",
        "              metrics=[\"accuracy\"])\n",
        "history = model.fit(X_train, y_train, epochs=5, validation_split=0.1)\n",
        "\n",
        "# 2) Export to SavedModel (versi 0001)\n",
        "model_version = \"0001\"\n",
        "model_name = \"my_mnist_model\"\n",
        "model_path = os.path.join(model_name, model_version)\n",
        "tf.saved_model.save(model, model_path)\n",
        "# atau: model.save(model_path)  # juga SavedModel jika tanpa ekstensi .h5\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 3) Query TF Serving via REST\n",
        "import json\n",
        "import requests\n",
        "import numpy as np\n",
        "\n",
        "X_new = X_test[:3]  # misal 3 gambar uji\n",
        "input_data_json = json.dumps({\n",
        "    \"signature_name\": \"serving_default\",\n",
        "    \"instances\": X_new.tolist(),\n",
        "})\n",
        "\n",
        "SERVER_URL = \"http://localhost:8501/v1/models/my_mnist_model:predict\"\n",
        "response = requests.post(SERVER_URL, data=input_data_json)\n",
        "response.raise_for_status()\n",
        "response = response.json()\n",
        "y_proba = np.array(response[\"predictions\"])\n",
        "print(y_proba.round(2))"
      ],
      "metadata": {
        "id": "683P9CsGqipY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#  4) Query TF Serving via gRPC\n",
        "import grpc\n",
        "import tensorflow as tf\n",
        "from tensorflow_serving.apis.predict_pb2 import PredictRequest\n",
        "from tensorflow_serving.apis import prediction_service_pb2_grpc\n",
        "\n",
        "channel = grpc.insecure_channel(\"localhost:8500\")\n",
        "predict_service = prediction_service_pb2_grpc.PredictionServiceStub(channel)\n",
        "\n",
        "request = PredictRequest()\n",
        "request.model_spec.name = model_name\n",
        "request.model_spec.signature_name = \"serving_default\"\n",
        "input_name = model.input_names[0]\n",
        "request.inputs[input_name].CopyFrom(tf.make_tensor_proto(X_new))\n",
        "\n",
        "response = predict_service.Predict(request, timeout=10.0)\n",
        "output_name = model.output_names[0]\n",
        "outputs_proto = response.outputs[output_name]\n",
        "y_proba = tf.make_ndarray(outputs_proto)\n",
        "print(y_proba.round(2))\n"
      ],
      "metadata": {
        "id": "UH_-UFJbqkN9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Google Cloud AI Platform: Prediction Service & Hyperparameter Tuning\n",
        "\n",
        "Google Cloud AI Platform (Vertex AI) menyediakan layanan hosting prediction berbasis TensorFlow Serving dengan:\n",
        "autoscaling,\n",
        "logging,\n",
        "integrasi langsung dengan Google Cloud Storage (GCS).\n",
        "Alur deployment umum:\n",
        "Upload SavedModel ke GCS.\n",
        "Buat Model di AI Platform.\n",
        "Buat Version yang menunjuk ke path SavedModel\n",
        "(mis. gs://bucket/model_name/0001/).\n",
        "Akses prediction dilakukan melalui REST API menggunakan Google API Client Library.\n",
        "Autentikasi menggunakan service account dengan key JSON yang diset melalui GOOGLE_APPLICATION_CREDENTIALS.\n",
        "AI Platform mendukung hyperparameter tuning berbasis Bayesian optimization (Google Vizier), dikonfigurasi lewat file YAML dan menggunakan metrik dari TensorBoard (misalnya accuracy atau loss)."
      ],
      "metadata": {
        "id": "4iA6F6_kqnlO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import googleapiclient.discovery\n",
        "\n",
        "# 1) Autentikasi service account\n",
        "os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] = \"my_service_account_key.json\"\n",
        "\n",
        "# 2) Bangun resource untuk AI Platform\n",
        "project_id = \"your-gcp-project-id\"\n",
        "model_id = \"my_mnist_model\"\n",
        "model_path = f\"projects/{project_id}/models/{model_id}\"\n",
        "\n",
        "ml_resource = googleapiclient.discovery.build(\"ml\", \"v1\").projects()\n",
        "\n",
        "output_name = \"dense_1\"  # sesuaikan dengan nama output layer SavedModel\n",
        "\n",
        "def predict(X):\n",
        "    body = {\n",
        "        \"signature_name\": \"serving_default\",\n",
        "        \"instances\": X.tolist(),\n",
        "    }\n",
        "    request = ml_resource.predict(name=model_path, body=body)\n",
        "    response = request.execute()\n",
        "    if \"error\" in response:\n",
        "        raise RuntimeError(response[\"error\"])\n",
        "    return np.array([pred[output_name] for pred in response[\"predictions\"]])\n",
        "\n",
        "Y_probas = predict(X_new)\n",
        "print(np.round(Y_probas, 2))\n"
      ],
      "metadata": {
        "id": "gXJ0kKpgqqJR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Di training script: gunakan argumen CLI & TensorBoard callback\n",
        "import argparse\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "\n",
        "parser = argparse.ArgumentParser()\n",
        "parser.add_argument(\"--n_layers\", type=int, default=20)\n",
        "parser.add_argument(\"--momentum\", type=float, default=0.9)\n",
        "parser.add_argument(\"--job-dir\", type=str, default=\"/tmp/jobdir\")\n",
        "args, _ = parser.parse_known_args()\n",
        "\n",
        "model = keras.models.Sequential(\n",
        "    [keras.layers.Flatten(input_shape=[28, 28])] +\n",
        "    [keras.layers.Dense(100, activation=\"relu\") for _ in range(args.n_layers)] +\n",
        "    [keras.layers.Dense(10, activation=\"softmax\")]\n",
        ")\n",
        "optimizer = keras.optimizers.SGD(momentum=args.momentum)\n",
        "model.compile(loss=\"sparse_categorical_crossentropy\",\n",
        "              optimizer=optimizer,\n",
        "              metrics=[\"accuracy\"])\n",
        "\n",
        "tb_cb = keras.callbacks.TensorBoard(\n",
        "    log_dir=args.job_dir,\n",
        "    update_freq=\"batch\"\n",
        ")\n",
        "\n",
        "model.fit(X_train, y_train, epochs=10, validation_split=0.1,\n",
        "          callbacks=[tb_cb])"
      ],
      "metadata": {
        "id": "YPqlo0ZDqsEC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. TFLite dan TF.js: Deployment ke Mobile, Embedded, dan Browser\n",
        "\n",
        "Bagian ini membahas TensorFlow Lite (TFLite) dan TensorFlow.js sebagai solusi deployment model ke platform dengan keterbatasan sumber daya seperti perangkat mobile, embedded system, dan browser. TFLite dirancang agar model bisa berjalan dengan ukuran file yang jauh lebih kecil, latensi rendah, serta konsumsi energi yang hemat, sehingga cocok untuk hardware dengan keterbatasan RAM dan CPU. Model yang awalnya berupa SavedModel atau tf.keras dikonversi menjadi format .tflite berbasis FlatBuffers menggunakan TFLite Converter, yang sekaligus menerapkan berbagai optimisasi graph seperti menghapus operasi yang hanya dibutuhkan saat training, menggabungkan Batch Normalization, dan menyederhanakan struktur komputasi.\n",
        "\n",
        "Untuk meningkatkan efisiensi lebih jauh, TFLite mendukung post-training quantization, misalnya mengubah representasi bobot dari float32 ke int8. Teknik ini secara signifikan mengecilkan ukuran model dan mempercepat inference. Pada skema kuantisasi simetris, nilai float dalam rentang tertentu dipetakan ke rentang integer terbatas, sehingga memungkinkan full integer inference dengan latensi yang sangat rendah. Jika penurunan akurasi menjadi perhatian, quantization-aware training (QAT) dapat digunakan dengan menyimulasikan efek kuantisasi selama training, sehingga model lebih siap saat dikonversi ke format TFLite.\n",
        "\n",
        "Sementara itu, TensorFlow.js memungkinkan model dijalankan langsung di browser menggunakan format model.json dan shard bobot biner. Pendekatan ini sangat cocok untuk aplikasi web yang membutuhkan latensi rendah, kemampuan berjalan secara offline, serta perlindungan privasi karena inference dilakukan sepenuhnya di sisi klien tanpa harus mengirim data ke server. Dengan dukungan TFLite dan TF.js, TensorFlow menyediakan jalur deployment end-to-end dari training hingga inference di berbagai platform nyata."
      ],
      "metadata": {
        "id": "nNwREmQHq1K1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "# Asumsikan model Keras sudah dilatih\n",
        "saved_model_path = \"my_mnist_model/0001\"\n",
        "\n",
        "# 1) Konversi SavedModel ke TFLite basic\n",
        "converter = tf.lite.TFLiteConverter.from_saved_model(saved_model_path)\n",
        "tflite_model = converter.convert()\n",
        "with open(\"model_basic.tflite\", \"wb\") as f:\n",
        "    f.write(tflite_model)\n",
        "\n",
        "# 2) Post-training quantization (OPTIMIZE_FOR_SIZE)\n",
        "converter = tf.lite.TFLiteConverter.from_saved_model(saved_model_path)\n",
        "converter.optimizations = [tf.lite.Optimize.OPTIMIZE_FOR_SIZE]\n",
        "tflite_quant_model = converter.convert()\n",
        "with open(\"model_quant.tflite\", \"wb\") as f:\n",
        "    f.write(tflite_quant_model)\n",
        "\n",
        "# 3) Jalankan TFLite model di Python dengan TFLite Interpreter\n",
        "import numpy as np\n",
        "\n",
        "interpreter = tf.lite.Interpreter(model_path=\"model_quant.tflite\")\n",
        "interpreter.allocate_tensors()\n",
        "\n",
        "input_details = interpreter.get_input_details()\n",
        "output_details = interpreter.get_output_details()\n",
        "\n",
        "X_sample = X_test[:1].astype(np.float32)\n",
        "interpreter.set_tensor(input_details[0][\"index\"], X_sample)\n",
        "interpreter.invoke()\n",
        "output_data = interpreter.get_tensor(output_details[0][\"index\"])\n",
        "print(output_data.round(2))"
      ],
      "metadata": {
        "id": "BR9k6nYzq5Xl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. GPU/TPU, Manajemen Memori, dan Multi-Device Placement\n",
        "\n",
        "Melatih deep model di CPU umumnya terasa lambat karena operasi utama seperti convolution dan perkalian matriks sangat intensif secara komputasi. GPU dan TPU dirancang khusus untuk jenis operasi ini, sehingga mampu mempercepat proses training dan inference secara signifikan. Karena itu, penggunaan akselerator menjadi standar dalam praktik deep learning modern.\n",
        "\n",
        "Untuk menjalankan TensorFlow dengan GPU lokal, diperlukan beberapa komponen pendukung, yaitu driver NVIDIA, CUDA, dan cuDNN yang versinya saling kompatibel. Setelah terpasang, ketersediaan GPU bisa dicek langsung dari TensorFlow, misalnya dengan fungsi tf.test.is_gpu_available() atau dengan melihat daftar device fisik melalui tf.config.experimental.list_physical_devices('GPU').\n",
        "\n",
        "Jika tidak ingin repot dengan instalasi lokal, alternatif yang jauh lebih praktis adalah menggunakan Google Colab yang menyediakan GPU dan TPU gratis, atau menyewa cloud VM berbasis GPU seperti Google Cloud Deep Learning VM. Opsi-opsi ini sering dipakai untuk eksperimen, tugas kuliah, maupun training model berskala besar.\n",
        "\n",
        "Secara default, TensorFlow akan langsung mengalokasikan seluruh memori dari GPU pertama yang terdeteksi, yang kadang menyulitkan jika GPU ingin dipakai bersama proses lain. Untuk mengatasinya, tersedia pengaturan seperti set_virtual_device_configuration dan set_memory_growth yang memungkinkan pembatasan penggunaan memori GPU, alokasi memori secara bertahap sesuai kebutuhan, serta pembagian satu GPU untuk beberapa proses.\n",
        "\n",
        "Dalam hal penempatan komputasi, TensorFlow biasanya secara otomatis menentukan apakah sebuah operasi atau variabel dijalankan di CPU atau GPU. Namun, jika dibutuhkan kontrol lebih spesifik, penempatan ini bisa dioverride secara manual menggunakan konteks tf.device, misalnya memaksa eksekusi ke /gpu:1 atau /cpu:0 sesuai kebutuhan eksperimen atau debugging."
      ],
      "metadata": {
        "id": "vGKDq27qq6aV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "# 1) Cek GPU\n",
        "print(tf.config.experimental.list_physical_devices(\"GPU\"))\n",
        "\n",
        "# 2) Set memory growth supaya TF tidak grab semua RAM GPU\n",
        "gpus = tf.config.experimental.list_physical_devices(\"GPU\")\n",
        "for gpu in gpus:\n",
        "    tf.config.experimental.set_memory_growth(gpu, True)\n",
        "\n",
        "# 3) Batasi RAM GPU (misal 2 GiB per virtual device)\n",
        "for gpu in gpus:\n",
        "    tf.config.experimental.set_virtual_device_configuration(\n",
        "        gpu,\n",
        "        [tf.config.experimental.VirtualDeviceConfiguration(memory_limit=2048)]\n",
        "    )\n",
        "\n",
        "# 4) Atau split 1 GPU menjadi 2 virtual GPU (2 GiB + 2 GiB)\n",
        "physical_gpus = tf.config.experimental.list_physical_devices(\"GPU\")\n",
        "if physical_gpus:\n",
        "    tf.config.experimental.set_virtual_device_configuration(\n",
        "        physical_gpus[0],\n",
        "        [\n",
        "            tf.config.experimental.VirtualDeviceConfiguration(memory_limit=2048),\n",
        "            tf.config.experimental.VirtualDeviceConfiguration(memory_limit=2048),\n",
        "        ],\n",
        "    )\n",
        "    logical_gpus = tf.config.experimental.list_logical_devices(\"GPU\")\n",
        "    print(\"Logical GPUs:\", logical_gpus)\n"
      ],
      "metadata": {
        "id": "eSXPNjZFrEK1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 5) Contoh penempatan variabel di CPU vs GPU\n",
        "a = tf.Variable(42.0)  # float → GPU default bila ada\n",
        "b = tf.Variable(42)    # int → CPU\n",
        "print(\"a on:\", a.device)\n",
        "print(\"b on:\", b.device)\n",
        "\n",
        "with tf.device(\"/cpu:0\"):\n",
        "    c = tf.Variable(3.14)\n",
        "print(\"c on:\", c.device)\n"
      ],
      "metadata": {
        "id": "TYgZng3RrEtn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. Distribution Strategies: Multi-GPU & Multi-Server Training\n",
        "\n",
        "Pada skala besar, proses training biasanya dipercepat dengan data parallelism, yaitu dengan mereplikasi model yang sama ke beberapa device. Setiap device memproses mini-batch data yang berbeda, lalu gradien yang dihasilkan digabungkan—umumnya menggunakan mekanisme seperti AllReduce—sebelum parameter model diperbarui. Dengan cara ini, beban komputasi terbagi rata tanpa mengubah arsitektur model secara fundamental.\n",
        "\n",
        "Untuk kasus multi-GPU dalam satu mesin, TensorFlow menyediakan MirroredStrategy. Strategi ini membuat replika model di setiap GPU dan secara otomatis melakukan agregasi gradien menggunakan AllReduce berbasis NCCL. Pendekatan ini relatif sederhana untuk digunakan dan sangat efisien pada single-node dengan banyak GPU, sehingga sering menjadi pilihan default untuk training paralel lokal.\n",
        "\n",
        "Alternatif lain adalah CentralStorageStrategy, di mana seluruh parameter model disimpan di satu device, biasanya CPU, sementara GPU hanya bertugas menghitung gradien. Strategi ini cocok untuk model yang relatif kecil atau pada kondisi di mana memori GPU terbatas, meskipun performanya umumnya tidak secepat MirroredStrategy karena adanya bottleneck pada device pusat.\n",
        "\n",
        "Untuk training terdistribusi lintas banyak mesin, TensorFlow mendukung MultiWorkerMirroredStrategy dan ParameterServerStrategy. Konfigurasi cluster didefinisikan melalui variabel environment TF_CONFIG, yang menetapkan peran seperti chief, worker, parameter server, dan evaluator. MultiWorkerMirroredStrategy pada dasarnya memperluas MirroredStrategy ke level multi-server dengan sinkronisasi gradien lintas mesin menggunakan AllReduce, sehingga cocok untuk training sinkron berskala besar.\n",
        "\n",
        "Sebaliknya, ParameterServerStrategy memisahkan peran komputasi dan penyimpanan parameter: worker bertugas menghitung gradien, sementara parameter server menyimpan dan memperbarui bobot model. Pendekatan ini lebih fleksibel dan sering dipakai pada cluster besar atau skenario asinkron, meskipun arsitekturnya lebih kompleks. Dalam praktiknya, job distributed seperti ini sering dijalankan di cloud, misalnya menggunakan layanan seperti Vertex AI.\n",
        "\n",
        "Terlepas dari strategi distribusi yang digunakan saat training, model hasil akhirnya tetap disimpan dalam format SavedModel standar. Model ini dapat diload dan dijalankan tanpa konteks distribusi sama sekali, atau kembali dijalankan dengan distribution strategy jika ingin melakukan inference paralel di lingkungan multi-GPU."
      ],
      "metadata": {
        "id": "kMeNJIrYrGw0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "\n",
        "# 1) Buat strategy di semua GPU lokal\n",
        "strategy = tf.distribute.MirroredStrategy()  # atau MirroredStrategy([\"/gpu:0\",\"/gpu:1\"])\n",
        "print(\"Num replicas in sync:\", strategy.num_replicas_in_sync)\n",
        "\n",
        "# 2) Bangun & compile model di dalam scope strategy\n",
        "with strategy.scope():\n",
        "    model = keras.models.Sequential([\n",
        "        keras.layers.Flatten(input_shape=[28, 28]),\n",
        "        keras.layers.Dense(300, activation=\"relu\"),\n",
        "        keras.layers.Dense(100, activation=\"relu\"),\n",
        "        keras.layers.Dense(10, activation=\"softmax\"),\n",
        "    ])\n",
        "    model.compile(loss=\"sparse_categorical_crossentropy\",\n",
        "                  optimizer=keras.optimizers.Adam(),\n",
        "                  metrics=[\"accuracy\"])\n",
        "\n",
        "# Batch size harus kelipatan jumlah replika\n",
        "global_batch_size = 256\n",
        "history = model.fit(X_train, y_train,\n",
        "                    epochs=10,\n",
        "                    batch_size=global_batch_size,\n",
        "                    validation_split=0.1)\n",
        "\n",
        "# 3) Simpan model seperti biasa\n",
        "model.save(\"mnist_multi_gpu.h5\")\n",
        "\n",
        "# 4) Load untuk inference saja (single device)\n",
        "single_model = keras.models.load_model(\"mnist_multi_gpu.h5\")\n",
        "y_proba_single = single_model.predict(X_test[:512])\n",
        "\n",
        "# 5) Atau load lagi dalam scope untuk inference multi-GPU\n",
        "with strategy.scope():\n",
        "    dist_model = keras.models.load_model(\"mnist_multi_gpu.h5\")\n",
        "y_proba_dist = dist_model.predict(X_test[:global_batch_size])"
      ],
      "metadata": {
        "id": "MJRbjwL3rQ-q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Example skeleton: CentralStorageStrategy\n",
        "strategy = tf.distribute.experimental.CentralStorageStrategy()\n",
        "\n",
        "with strategy.scope():\n",
        "    model = keras.models.Sequential([\n",
        "        keras.layers.Flatten(input_shape=[28, 28]),\n",
        "        keras.layers.Dense(256, activation=\"relu\"),\n",
        "        keras.layers.Dense(10, activation=\"softmax\"),\n",
        "    ])\n",
        "    model.compile(loss=\"sparse_categorical_crossentropy\",\n",
        "                  optimizer=\"adam\",\n",
        "                  metrics=[\"accuracy\"])\n",
        "\n",
        "model.fit(X_train, y_train,\n",
        "          epochs=5,\n",
        "          batch_size=global_batch_size,\n",
        "          validation_split=0.1)\n"
      ],
      "metadata": {
        "id": "RxMLO8SgrRgp"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}