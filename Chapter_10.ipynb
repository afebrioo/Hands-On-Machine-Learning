{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Rahmanda Afebrio Yuris Soesatyo - Chapter 10:,Pengantar Artificial Neural Networks dengan Keras"
      ],
      "metadata": {
        "id": "kJQ_hmW-D7Z7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Chapter 10 â€“ Pengantar Artificial Neural Networks dengan Keras\n",
        "\n",
        "Notebook ini mengadaptasi kode utama dari Chapter 10 buku Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow, sekaligus ngasih penjelasan teori singkat di tiap bagian. Pembahasannya mencakup konsep neuron buatan, Multilayer Perceptron (MLP), proses backpropagation, sampai implementasi MLP menggunakan Keras (tf.keras).\n",
        "\n",
        "1. Dari Neuron Biologis ke Neuron Buatan\n",
        "\n",
        "Neuron biologis bekerja dengan menerima sinyal lewat dendrit, menggabungkan sinyal tersebut, lalu mengirim impuls jika sinyal totalnya melewati ambang tertentu.\n",
        "\n",
        "McCulloch dan Pitts kemudian memformalkan konsep ini ke dalam neuron buatan biner, dengan input dan output bernilai 0 atau 1. Model sederhana ini sudah cukup untuk merepresentasikan operasi logika dasar seperti AND, OR, NOT, bahkan XOR jika dikombinasikan dalam jaringan.\n",
        "\n",
        "Perkembangan berikutnya adalah Perceptron yang diperkenalkan oleh Rosenblatt pada tahun 1957. Model ini menggunakan Threshold Logic Unit (TLU) yang menghitung jumlah berbobot dari input (ğ‘§ = x Â· w), lalu melewatkannya ke fungsi ambang untuk menghasilkan output biner.\n",
        "\n",
        "Namun, perceptron punya keterbatasan besar: ia hanya mampu memisahkan data yang linear separable. Akibatnya, masalah sederhana seperti XOR tidak bisa diselesaikan. Kelemahan ini akhirnya diatasi dengan Multilayer Perceptron (MLP), yang menambahkan hidden layer non-linear.\n",
        "\n",
        "2. Komputasi Logika Menggunakan Neuron\n",
        "\n",
        "McCulloch dan Pitts menunjukkan bahwa neuron buatan dengan beberapa input biner dan satu output biner dapat meniru operasi logika. Neuron akan aktif jika jumlah input aktif melewati ambang tertentu.\n",
        "\n",
        "Dengan neuron-neuron sederhana ini, berbagai operasi logika seperti identity, AND, OR, hingga ekspresi lebih kompleks (misalnya A aktif dan B tidak aktif atau A âˆ§ Â¬B) dapat dibentuk.\n",
        "\n",
        "Jaringan logika tersebut bisa digabungkan untuk merepresentasikan ekspresi logika apa pun. Artinya, secara teori Artificial Neural Network (ANN) mampu melakukan komputasi logika umum.\n",
        "\n",
        "3. Perceptron\n",
        "\n",
        "Perceptron merupakan bentuk paling dasar dari Artificial Neural Network, yang tersusun dari neuron jenis Threshold Logic Unit (TLU) atau Linear Threshold Unit (LTU).\n",
        "\n",
        "Setiap TLU menghitung jumlah berbobot dari input, lalu hasilnya diproses menggunakan fungsi langkah (step function) seperti fungsi Heaviside atau sign untuk menghasilkan output biner.\n",
        "\n",
        "Struktur perceptron terdiri dari:\n",
        "\n",
        "Satu lapisan TLU\n",
        "\n",
        "Koneksi penuh (fully connected) ke neuron input\n",
        "\n",
        "Satu neuron bias yang selalu bernilai 1\n",
        "\n",
        "Karena sifatnya linear, perceptron cocok digunakan sebagai model klasifikasi linear, baik untuk klasifikasi biner maupun multi-output.\n",
        "\n",
        "Learning Rule (Aturan Pembelajaran)\n",
        "\n",
        "Proses pembelajaran perceptron dilakukan dengan memperbarui bobot menggunakan versi modifikasi dari aturan Hebbian.\n",
        "\n",
        "Intinya, bobot yang membantu mengurangi kesalahan prediksi akan diperkuat. Perubahan bobot ditentukan oleh:\n",
        "\n",
        "Learning rate\n",
        "\n",
        "Selisih antara nilai target dan prediksi\n",
        "\n",
        "Nilai input yang bersangkutan\n",
        "\n",
        "Dengan mekanisme ini, perceptron secara bertahap belajar menyesuaikan bobot agar prediksi semakin mendekati target."
      ],
      "metadata": {
        "id": "WC-G7HYE7cm_"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dXh_RV4M7YA5",
        "outputId": "3e2a4ca2-0d27-4395-f9ff-170bbc0604ad"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0]\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.linear_model import Perceptron\n",
        "\n",
        "iris = load_iris()\n",
        "X = iris.data[:, (2, 3)]  # petal length, petal width\n",
        "y = (iris.target == 0).astype(np.int32)  # Iris setosa?\n",
        "\n",
        "per_clf = Perceptron()\n",
        "per_clf.fit(X, y)\n",
        "\n",
        "y_pred = per_clf.predict([[2, 0.5]])\n",
        "print(y_pred)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Perceptron hanya bisa memisahkan data yang linearly separable, dan tidak bisa menyelesaikan kasus XOR, sehingga digantikan oleh arsitektur multilayer yang lebih kuat"
      ],
      "metadata": {
        "id": "u2vI8PxWEe2q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. Multilayer Perceptron (MLP) dan Backpropagation\n",
        "\n",
        "Multilayer Perceptron (MLP) merupakan pengembangan dari perceptron tunggal yang memiliki struktur sebagai berikut:\n",
        "\n",
        "Input layer yang hanya meneruskan data (passthrough)\n",
        "\n",
        "Satu atau lebih hidden layer\n",
        "\n",
        "Output layer\n",
        "\n",
        "Setiap neuron antar lapisan terhubung secara fully connected, dan masing-masing lapisan (kecuali output layer) memiliki neuron bias. Aliran data dalam MLP hanya bergerak satu arah, dari input ke output, sehingga termasuk dalam kategori feedforward neural network.\n",
        "\n",
        "Apabila jumlah hidden layer cukup banyak, maka jaringan ini disebut sebagai Deep Neural Network (DNN).\n",
        "\n",
        "Backpropagation Algorithm\n",
        "\n",
        "Metode pelatihan utama pada MLP adalah backpropagation, yaitu penerapan Gradient Descent yang dipadukan dengan perhitungan gradien otomatis menggunakan reverse-mode automatic differentiation.\n",
        "\n",
        "Proses training terdiri dari dua tahap utama:\n",
        "\n",
        "Forward Pass\n",
        "\n",
        "Data input dilewatkan ke seluruh jaringan hingga menghasilkan output. Selanjutnya, nilai loss dihitung berdasarkan selisih antara hasil prediksi dan target sebenarnya.\n",
        "\n",
        "Backward Pass\n",
        "\n",
        "Kesalahan (error) ditelusuri kembali dari output ke input menggunakan aturan rantai (chain rule). Gradien terhadap setiap bobot dan bias dihitung, lalu digunakan untuk memperbarui parameter jaringan melalui langkah Gradient Descent.\n",
        "\n",
        "Activation Functions\n",
        "\n",
        "Berbeda dengan perceptron tunggal yang menggunakan fungsi langkah (step function), MLP memanfaatkan fungsi aktivasi yang terdiferensiasi. Hal ini penting agar gradien dapat dihitung dengan baik dan proses pembelajaran berjalan stabil.\n",
        "\n",
        "Beberapa fungsi aktivasi yang umum digunakan antara lain:\n",
        "\n",
        "Sigmoid\n",
        "\n",
        "Hyperbolic Tangent (tanh)\n",
        "\n",
        "Rectified Linear Unit (ReLU)\n",
        "\n",
        "Penggunaan fungsi aktivasi ini memungkinkan jaringan mempelajari hubungan non-linear yang kompleks dalam data.\n",
        "\n",
        "5. Activation Functions\n",
        "\n",
        "Fungsi aktivasi berfungsi menambahkan non-linearitas ke dalam jaringan saraf. Tanpa non-linearitas, gabungan beberapa layer hanya akan setara dengan satu transformasi linear, sehingga jaringan sedalam apa pun tetap tidak mampu menangkap pola kompleks.\n",
        "\n",
        "Dengan adanya fungsi aktivasi non-linear, Deep Neural Network (DNN) secara teoretis mampu mendekati fungsi kontinu apa pun (universal approximation).\n",
        "\n",
        "Sigmoid\n",
        "\n",
        "Fungsi sigmoid menghasilkan output dalam rentang (0, 1).\n",
        "Fungsi ini memiliki peran penting dalam sejarah awal neural network, namun sering mengalami saturasi gradien pada nilai ekstrem, sehingga kurang efektif untuk jaringan yang dalam.\n",
        "\n",
        "Hyperbolic Tangent (tanh)\n",
        "\n",
        "Fungsi tanh menghasilkan output pada rentang [-1, 1], sehingga nilai aktivasi lebih terpusat di sekitar nol.\n",
        "Karakteristik ini umumnya membantu proses pelatihan menjadi lebih cepat dibandingkan sigmoid.\n",
        "\n",
        "Rectified Linear Unit (ReLU)\n",
        "\n",
        "Fungsi ReLU sangat sederhana dan efisien secara komputasi, sehingga menjadi pilihan default untuk hidden layer pada model modern.\n",
        "Namun, ReLU memiliki kelemahan berupa gradien nol untuk nilai input negatif, yang dapat menyebabkan masalah dead neurons.\n",
        "\n",
        "6. Regression MLPs\n",
        "\n",
        "Untuk permasalahan regresi, arsitektur Multilayer Perceptron (MLP) umumnya disusun dengan pola berikut.\n",
        "\n",
        "Architecture\n",
        "Input Layer\n",
        "\n",
        "Satu neuron untuk setiap fitur input\n",
        "\n",
        "Hidden Layers\n",
        "\n",
        "Biasanya terdiri dari 1â€“5 lapisan\n",
        "\n",
        "Setiap lapisan memiliki sekitar 10â€“100 neuron\n",
        "\n",
        "Menggunakan fungsi aktivasi ReLU atau SELU\n",
        "\n",
        "Output Layer\n",
        "\n",
        "Satu neuron untuk setiap dimensi target\n",
        "\n",
        "Pilihan fungsi aktivasi:\n",
        "\n",
        "Tanpa aktivasi (linear) untuk regresi umum\n",
        "\n",
        "ReLU atau softplus jika output harus bernilai positif\n",
        "\n",
        "Sigmoid atau tanh jika output dibatasi, dengan target diskalakan ke rentang yang sesuai\n",
        "\n",
        "Loss Functions\n",
        "\n",
        "Fungsi loss yang sering digunakan pada regresi meliputi:\n",
        "\n",
        "Mean Squared Error (MSE) sebagai pilihan standar\n",
        "\n",
        "Mean Absolute Error (MAE) atau Huber Loss jika data mengandung banyak outlier\n",
        "\n",
        "Pemilihan loss function sangat berpengaruh terhadap sensitivitas model terhadap kesalahan ekstrem serta kestabilan proses pelatihan."
      ],
      "metadata": {
        "id": "naf3hJYb8OlU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from sklearn.datasets import fetch_california_housing\n",
        "\n",
        "housing = fetch_california_housing()\n",
        "\n",
        "X_train_full, X_test, y_train_full, y_test = train_test_split(\n",
        "    housing.data, housing.target, random_state=42\n",
        ")\n",
        "X_train, X_valid, y_train, y_valid = train_test_split(\n",
        "    X_train_full, y_train_full, random_state=42\n",
        ")\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_valid = scaler.transform(X_valid)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "model = keras.models.Sequential([\n",
        "    keras.layers.Dense(30, activation=\"relu\", input_shape=X_train.shape[1:]),\n",
        "    keras.layers.Dense(1)\n",
        "])\n",
        "\n",
        "model.compile(loss=\"mean_squared_error\", optimizer=\"sgd\")\n",
        "history = model.fit(X_train, y_train, epochs=20,\n",
        "                    validation_data=(X_valid, y_valid))\n",
        "mse_test = model.evaluate(X_test, y_test)\n",
        "X_new = X_test[:3]\n",
        "y_pred = model.predict(X_new)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SHI1dyB4DaC_",
        "outputId": "fabdda20-06b7-4170-e202-fa86a1ec7b16"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/keras/src/layers/core/dense.py:93: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "\u001b[1m363/363\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - loss: 1.5046 - val_loss: 9.0786\n",
            "Epoch 2/20\n",
            "\u001b[1m363/363\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 0.6676 - val_loss: 0.7783\n",
            "Epoch 3/20\n",
            "\u001b[1m363/363\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.4538 - val_loss: 0.4165\n",
            "Epoch 4/20\n",
            "\u001b[1m363/363\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 0.4027 - val_loss: 0.4460\n",
            "Epoch 5/20\n",
            "\u001b[1m363/363\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.4160 - val_loss: 0.4601\n",
            "Epoch 6/20\n",
            "\u001b[1m363/363\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.4288 - val_loss: 0.4393\n",
            "Epoch 7/20\n",
            "\u001b[1m363/363\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.4004 - val_loss: 0.4497\n",
            "Epoch 8/20\n",
            "\u001b[1m363/363\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.4044 - val_loss: 0.3963\n",
            "Epoch 9/20\n",
            "\u001b[1m363/363\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.3780 - val_loss: 0.4321\n",
            "Epoch 10/20\n",
            "\u001b[1m363/363\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.4012 - val_loss: 0.4086\n",
            "Epoch 11/20\n",
            "\u001b[1m363/363\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.3744 - val_loss: 0.4209\n",
            "Epoch 12/20\n",
            "\u001b[1m363/363\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.3822 - val_loss: 0.3622\n",
            "Epoch 13/20\n",
            "\u001b[1m363/363\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.3715 - val_loss: 0.4137\n",
            "Epoch 14/20\n",
            "\u001b[1m363/363\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.3861 - val_loss: 0.3693\n",
            "Epoch 15/20\n",
            "\u001b[1m363/363\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.3773 - val_loss: 0.3920\n",
            "Epoch 16/20\n",
            "\u001b[1m363/363\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.3778 - val_loss: 0.4005\n",
            "Epoch 17/20\n",
            "\u001b[1m363/363\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.3673 - val_loss: 0.4039\n",
            "Epoch 18/20\n",
            "\u001b[1m363/363\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.3810 - val_loss: 0.4368\n",
            "Epoch 19/20\n",
            "\u001b[1m363/363\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.3673 - val_loss: 0.3725\n",
            "Epoch 20/20\n",
            "\u001b[1m363/363\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.3768 - val_loss: 0.3686\n",
            "\u001b[1m162/162\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.3614\n",
            "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "7. Classification MLPs\n",
        "\n",
        "Pada klasifikasi biner, MLP umumnya menggunakan satu neuron pada output layer dengan fungsi aktivasi sigmoid. Nilai output dapat langsung diartikan sebagai probabilitas bahwa sebuah sampel termasuk ke kelas positif.\n",
        "\n",
        "Untuk klasifikasi multilabel biner (di mana setiap label bersifat independen satu sama lain), arsitektur yang digunakan adalah:\n",
        "\n",
        "Satu neuron output dengan aktivasi sigmoid untuk setiap label\n",
        "\n",
        "Pada skenario ini, probabilitas antar label tidak harus berjumlah 1, karena masing-masing label bisa aktif secara bersamaan.\n",
        "\n",
        "Sementara itu, untuk klasifikasi multiclass eksklusif (contohnya pengenalan 10 kelas pada dataset MNIST atau Fashion MNIST), pendekatan yang digunakan adalah:\n",
        "\n",
        "Satu neuron output untuk setiap kelas\n",
        "\n",
        "Fungsi aktivasi softmax, sehingga:\n",
        "\n",
        "Setiap probabilitas berada pada rentang (0, 1)\n",
        "\n",
        "Total probabilitas seluruh kelas bernilai 1\n",
        "\n",
        "Fungsi loss yang paling umum dipakai pada kasus ini adalah cross-entropy (log loss), karena model memprediksi distribusi probabilitas kelas.\n",
        "\n",
        "8. Implementasi MLP dengan Keras\n",
        "(Fashion MNIST â€“ Sequential API)\n",
        "\n",
        "Bagian ini membahas implementasi Multilayer Perceptron (MLP) menggunakan Keras (tf.keras) untuk tugas klasifikasi pada dataset Fashion MNIST.\n",
        "\n",
        "Dataset Fashion MNIST berisi 70.000 gambar grayscale berukuran 28Ã—28 piksel, yang terbagi ke dalam 10 kelas item fashion. Dataset ini umumnya dibagi menjadi:\n",
        "\n",
        "Training set\n",
        "\n",
        "Validation set (dibentuk secara manual dari data training penuh)\n",
        "\n",
        "Test set\n",
        "\n",
        "Sebelum proses pelatihan, nilai piksel biasanya diskalakan agar berada pada rentang yang sesuai, sehingga proses optimisasi dapat berjalan lebih stabil.\n",
        "\n",
        "Model Architecture\n",
        "\n",
        "Model dibangun menggunakan Sequential API dengan susunan layer sebagai berikut:\n",
        "\n",
        "Flatten layer\n",
        "Mengubah input gambar 28Ã—28 menjadi vektor satu dimensi berukuran 784\n",
        "\n",
        "Dua Dense hidden layer\n",
        "Menggunakan fungsi aktivasi ReLU\n",
        "\n",
        "Output layer\n",
        "Dense layer dengan 10 neuron dan fungsi aktivasi softmax\n",
        "\n",
        "Training and Evaluation\n",
        "\n",
        "Model dikompilasi dengan konfigurasi:\n",
        "\n",
        "Loss function: sparse_categorical_crossentropy\n",
        "\n",
        "Optimizer: Stochastic Gradient Descent (SGD)\n",
        "\n",
        "Metric: accuracy\n",
        "\n",
        "Proses pelatihan dilakukan menggunakan metode fit(), sedangkan evaluasi performa model dilakukan dengan evaluate().\n",
        "Untuk memperoleh prediksi kelas, model dapat menggunakan metode predict()."
      ],
      "metadata": {
        "id": "YSZUN2UmDq11"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import numpy as np\n",
        "\n",
        "# Load dataset\n",
        "fashion_mnist = keras.datasets.fashion_mnist\n",
        "(X_train_full, y_train_full), (X_test, y_test) = fashion_mnist.load_data()\n",
        "\n",
        "# Scale to [0, 1] and create validation set\n",
        "X_train_full = X_train_full / 255.0\n",
        "X_test = X_test / 255.0\n",
        "\n",
        "X_valid, X_train = X_train_full[:5000], X_train_full[5000:]\n",
        "y_valid, y_train = y_train_full[:5000], y_train_full[5000:]\n",
        "\n",
        "class_names = [\"T-shirt/top\", \"Trouser\", \"Pullover\", \"Dress\", \"Coat\",\n",
        "               \"Sandal\", \"Shirt\", \"Sneaker\", \"Bag\", \"Ankle boot\"]\n",
        "\n",
        "# Build model\n",
        "model = keras.models.Sequential([\n",
        "    keras.layers.Flatten(input_shape=[28, 28]),\n",
        "    keras.layers.Dense(300, activation=\"relu\"),\n",
        "    keras.layers.Dense(100, activation=\"relu\"),\n",
        "    keras.layers.Dense(10, activation=\"softmax\")\n",
        "])\n",
        "\n",
        "# Compile model\n",
        "model.compile(loss=\"sparse_categorical_crossentropy\",\n",
        "              optimizer=\"sgd\",\n",
        "              metrics=[\"accuracy\"])\n",
        "\n",
        "# Train model\n",
        "history = model.fit(X_train, y_train, epochs=30,\n",
        "                    validation_data=(X_valid, y_valid))\n",
        "\n",
        "# Evaluate on test set\n",
        "test_loss, test_acc = model.evaluate(X_test, y_test)\n",
        "\n",
        "# Predict some examples\n",
        "X_new = X_test[:3]\n",
        "y_proba = model.predict(X_new)\n",
        "y_pred = np.argmax(y_proba, axis=1)\n",
        "\n",
        "print(\"Predicted classes:\", y_pred)\n",
        "print(\"True classes     :\", y_test[:3])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dnGVuLsPDr2s",
        "outputId": "48026644-b445-4a16-f869-6dea8faf7080"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-labels-idx1-ubyte.gz\n",
            "\u001b[1m29515/29515\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-images-idx3-ubyte.gz\n",
            "\u001b[1m26421880/26421880\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-labels-idx1-ubyte.gz\n",
            "\u001b[1m5148/5148\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-images-idx3-ubyte.gz\n",
            "\u001b[1m4422102/4422102\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/keras/src/layers/reshaping/flatten.py:37: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(**kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/30\n",
            "\u001b[1m1719/1719\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 9ms/step - accuracy: 0.6715 - loss: 1.0192 - val_accuracy: 0.8324 - val_loss: 0.5016\n",
            "Epoch 2/30\n",
            "\u001b[1m1719/1719\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 4ms/step - accuracy: 0.8251 - loss: 0.5020 - val_accuracy: 0.8464 - val_loss: 0.4501\n",
            "Epoch 3/30\n",
            "\u001b[1m1719/1719\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - accuracy: 0.8426 - loss: 0.4500 - val_accuracy: 0.8466 - val_loss: 0.4406\n",
            "Epoch 4/30\n",
            "\u001b[1m1719/1719\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 4ms/step - accuracy: 0.8543 - loss: 0.4168 - val_accuracy: 0.8652 - val_loss: 0.3900\n",
            "Epoch 5/30\n",
            "\u001b[1m1719/1719\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 3ms/step - accuracy: 0.8601 - loss: 0.3955 - val_accuracy: 0.8678 - val_loss: 0.3819\n",
            "Epoch 6/30\n",
            "\u001b[1m1719/1719\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 4ms/step - accuracy: 0.8667 - loss: 0.3790 - val_accuracy: 0.8654 - val_loss: 0.3904\n",
            "Epoch 7/30\n",
            "\u001b[1m1719/1719\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - accuracy: 0.8710 - loss: 0.3682 - val_accuracy: 0.8720 - val_loss: 0.3719\n",
            "Epoch 8/30\n",
            "\u001b[1m1719/1719\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 4ms/step - accuracy: 0.8717 - loss: 0.3594 - val_accuracy: 0.8734 - val_loss: 0.3551\n",
            "Epoch 9/30\n",
            "\u001b[1m1719/1719\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - accuracy: 0.8797 - loss: 0.3404 - val_accuracy: 0.8704 - val_loss: 0.3638\n",
            "Epoch 10/30\n",
            "\u001b[1m1719/1719\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - accuracy: 0.8807 - loss: 0.3359 - val_accuracy: 0.8760 - val_loss: 0.3518\n",
            "Epoch 11/30\n",
            "\u001b[1m1719/1719\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 4ms/step - accuracy: 0.8841 - loss: 0.3247 - val_accuracy: 0.8762 - val_loss: 0.3462\n",
            "Epoch 12/30\n",
            "\u001b[1m1719/1719\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 3ms/step - accuracy: 0.8875 - loss: 0.3172 - val_accuracy: 0.8758 - val_loss: 0.3426\n",
            "Epoch 13/30\n",
            "\u001b[1m1719/1719\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 4ms/step - accuracy: 0.8887 - loss: 0.3092 - val_accuracy: 0.8818 - val_loss: 0.3335\n",
            "Epoch 14/30\n",
            "\u001b[1m1719/1719\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 3ms/step - accuracy: 0.8893 - loss: 0.3063 - val_accuracy: 0.8766 - val_loss: 0.3408\n",
            "Epoch 15/30\n",
            "\u001b[1m1719/1719\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 4ms/step - accuracy: 0.8933 - loss: 0.3006 - val_accuracy: 0.8806 - val_loss: 0.3314\n",
            "Epoch 16/30\n",
            "\u001b[1m1719/1719\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 4ms/step - accuracy: 0.8956 - loss: 0.2907 - val_accuracy: 0.8866 - val_loss: 0.3231\n",
            "Epoch 17/30\n",
            "\u001b[1m1719/1719\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 4ms/step - accuracy: 0.8959 - loss: 0.2853 - val_accuracy: 0.8886 - val_loss: 0.3189\n",
            "Epoch 18/30\n",
            "\u001b[1m1719/1719\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 4ms/step - accuracy: 0.9002 - loss: 0.2766 - val_accuracy: 0.8838 - val_loss: 0.3135\n",
            "Epoch 19/30\n",
            "\u001b[1m1719/1719\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 7ms/step - accuracy: 0.9004 - loss: 0.2768 - val_accuracy: 0.8890 - val_loss: 0.3146\n",
            "Epoch 20/30\n",
            "\u001b[1m1719/1719\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 8ms/step - accuracy: 0.9008 - loss: 0.2746 - val_accuracy: 0.8872 - val_loss: 0.3161\n",
            "Epoch 21/30\n",
            "\u001b[1m1719/1719\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 4ms/step - accuracy: 0.9027 - loss: 0.2657 - val_accuracy: 0.8820 - val_loss: 0.3188\n",
            "Epoch 22/30\n",
            "\u001b[1m1719/1719\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 4ms/step - accuracy: 0.9040 - loss: 0.2625 - val_accuracy: 0.8924 - val_loss: 0.3032\n",
            "Epoch 23/30\n",
            "\u001b[1m1719/1719\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 3ms/step - accuracy: 0.9077 - loss: 0.2548 - val_accuracy: 0.8924 - val_loss: 0.3022\n",
            "Epoch 24/30\n",
            "\u001b[1m1719/1719\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 5ms/step - accuracy: 0.9089 - loss: 0.2513 - val_accuracy: 0.8876 - val_loss: 0.3124\n",
            "Epoch 25/30\n",
            "\u001b[1m1719/1719\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 5ms/step - accuracy: 0.9137 - loss: 0.2430 - val_accuracy: 0.8932 - val_loss: 0.2933\n",
            "Epoch 26/30\n",
            "\u001b[1m1719/1719\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 4ms/step - accuracy: 0.9143 - loss: 0.2413 - val_accuracy: 0.8818 - val_loss: 0.3222\n",
            "Epoch 27/30\n",
            "\u001b[1m1719/1719\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 4ms/step - accuracy: 0.9144 - loss: 0.2367 - val_accuracy: 0.8902 - val_loss: 0.3059\n",
            "Epoch 28/30\n",
            "\u001b[1m1719/1719\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 4ms/step - accuracy: 0.9137 - loss: 0.2345 - val_accuracy: 0.8922 - val_loss: 0.3074\n",
            "Epoch 29/30\n",
            "\u001b[1m1719/1719\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 3ms/step - accuracy: 0.9175 - loss: 0.2297 - val_accuracy: 0.8896 - val_loss: 0.3023\n",
            "Epoch 30/30\n",
            "\u001b[1m1719/1719\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 5ms/step - accuracy: 0.9177 - loss: 0.2271 - val_accuracy: 0.8834 - val_loss: 0.3224\n",
            "\u001b[1m313/313\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.8745 - loss: 0.3498\n",
            "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 76ms/step\n",
            "Predicted classes: [9 2 1]\n",
            "True classes     : [9 2 1]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "9. Building Complex Models Using the Functional API\n",
        "\n",
        "\n",
        "Functional API digunakan saat arsitektur lebih kompleks dari sekadar stack berurutan, misalnya Wide & Deep network, multiple inputs, atau multiple outputs. Wide & Deep menggabungkan jalur â€œdeepâ€ (beberapa hidden layer) dengan shortcut dari input langsung ke output, sehingga jaringan bisa belajar pola kompleks dan aturan sederhana sekaligus.â€‹\n",
        "\n",
        "Dengan banyak input, sebagian fitur bisa dikirim ke jalur â€œwideâ€ dan subset lain ke jalur â€œdeepâ€ lalu dikonkatenasi, sedangkan dengan banyak output, satu model bisa menyelesaikan beberapa tugas sekaligus (misalnya regresi dan klasifikasi), atau menambah auxiliary output sebagai regularizer.â€‹"
      ],
      "metadata": {
        "id": "kv8-YJyNDyfz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow import keras\n",
        "\n",
        "input_A = keras.layers.Input(shape=[5], name=\"wide_input\")\n",
        "input_B = keras.layers.Input(shape=[6], name=\"deep_input\")\n",
        "\n",
        "hidden1 = keras.layers.Dense(30, activation=\"relu\")(input_B)\n",
        "hidden2 = keras.layers.Dense(30, activation=\"relu\")(hidden1)\n",
        "\n",
        "concat = keras.layers.concatenate([input_A, hidden2])\n",
        "output = keras.layers.Dense(1, name=\"output\")(concat)\n",
        "\n",
        "model = keras.Model(inputs=[input_A, input_B], outputs=[output])\n",
        "\n",
        "model.compile(loss=\"mse\", optimizer=keras.optimizers.SGD(learning_rate=1e-3))\n",
        "\n",
        "# Example split of features:\n",
        "# X_train_A = X_train[:, :5]\n",
        "# X_train_B = X_train[:, 2:]\n",
        "# history = model.fit((X_train_A, X_train_B), y_train, epochs=20,\n",
        "#                     validation_data=((X_valid_A, X_valid_B), y_valid))"
      ],
      "metadata": {
        "id": "yOuH7eTsHOBT"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "10. Saving Models, Callbacks, TensorBoard, dan Hyperparameter Tuning\n",
        "\n",
        "Keras menyediakan cara yang praktis untuk menyimpan dan memuat kembali model, baik dalam bentuk model utuh maupun hanya parameter bobotnya saja. Fitur ini sangat berguna untuk deployment, eksperimen ulang, dan melanjutkan training tanpa harus memulai dari nol.\n",
        "\n",
        "Saving and Loading Models\n",
        "\n",
        "Model dapat disimpan dan dipulihkan dengan dua pendekatan utama:\n",
        "\n",
        "model.save() dan load_model()\n",
        "Digunakan untuk menyimpan dan memuat seluruh model, termasuk arsitektur, bobot, serta konfigurasi training.\n",
        "\n",
        "save_weights() dan load_weights()\n",
        "Digunakan jika hanya ingin menyimpan dan memuat bobot model saja.\n",
        "\n",
        "Pendekatan ini memudahkan proses pengembangan model secara berulang dan menjaga konsistensi hasil eksperimen.\n",
        "\n",
        "Callbacks\n",
        "\n",
        "Untuk proses pelatihan yang memakan waktu lama, Keras menyediakan callbacks yang berfungsi mengontrol dan memantau training secara otomatis.\n",
        "\n",
        "Beberapa callback yang paling sering digunakan antara lain:\n",
        "\n",
        "ModelCheckpoint\n",
        "Menyimpan checkpoint model secara otomatis selama proses training.\n",
        "\n",
        "EarlyStopping\n",
        "Menghentikan training lebih awal ketika performa pada data validasi tidak lagi meningkat.\n",
        "\n",
        "Penggunaan callbacks membantu mencegah overfitting sekaligus menghemat waktu dan sumber daya komputasi.\n",
        "\n",
        "TensorBoard\n",
        "\n",
        "TensorBoard adalah alat visualisasi yang digunakan untuk memantau dan menganalisis proses pelatihan model secara lebih mendalam, seperti:\n",
        "\n",
        "Kurva pembelajaran (learning curves) untuk loss dan accuracy\n",
        "\n",
        "Visualisasi arsitektur model\n",
        "\n",
        "Histogram bobot dan nilai aktivasi\n",
        "\n",
        "Distribusi gradien dan proses optimisasi\n",
        "\n",
        "TensorBoard dapat diintegrasikan melalui TensorBoard callback atau menggunakan API tf.summary.\n",
        "\n",
        "Hyperparameter Tuning\n",
        "\n",
        "Untuk mendapatkan performa model yang optimal, sering kali diperlukan penyesuaian hiperparameter, seperti:\n",
        "\n",
        "Jumlah layer dan jumlah neuron\n",
        "\n",
        "Learning rate\n",
        "\n",
        "Batch size\n",
        "\n",
        "Optimizer\n",
        "\n",
        "Fungsi aktivasi\n",
        "\n",
        "Proses tuning ini dapat dilakukan dengan beberapa pendekatan, antara lain:\n",
        "\n",
        "RandomizedSearchCV dengan wrapper KerasRegressor atau KerasClassifier\n",
        "\n",
        "Library khusus seperti Hyperopt, Keras Tuner, dan tools serupa\n",
        "\n",
        "Pendekatan ini memungkinkan eksplorasi ruang hiperparameter secara terstruktur dan efisien, sehingga peluang mendapatkan konfigurasi model terbaik menjadi lebih besar."
      ],
      "metadata": {
        "id": "Flq_bjN5HQzn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow import keras\n",
        "\n",
        "# Save full model\n",
        "model.save(\"my_keras_model.h5\")\n",
        "\n",
        "# Load model\n",
        "model = keras.models.load_model(\"my_keras_model.h5\")\n",
        "\n",
        "# Checkpoint callback (save best model on validation)\n",
        "checkpoint_cb = keras.callbacks.ModelCheckpoint(\n",
        "    \"best_model.h5\", save_best_only=True\n",
        ")\n",
        "\n",
        "# Early stopping callback\n",
        "early_stopping_cb = keras.callbacks.EarlyStopping(\n",
        "    patience=10, restore_best_weights=True\n",
        ")\n",
        "\n",
        "history = model.fit(\n",
        "    X_train, y_train,\n",
        "    epochs=100,\n",
        "    validation_data=(X_valid, y_valid),\n",
        "    callbacks=[checkpoint_cb, early_stopping_cb]\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 428
        },
        "id": "T6H5jE4xHgD-",
        "outputId": "6d2ba3c2-c8f9-4f7e-a2e3-15c25d480762"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n",
            "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "NotImplementedError",
          "evalue": "numpy() is only available when eager execution is enabled.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-4094965928.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     17\u001b[0m )\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m history = model.fit(\n\u001b[0m\u001b[1;32m     20\u001b[0m     \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    120\u001b[0m             \u001b[0;31m# To get the full stack trace, call:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m             \u001b[0;31m# `keras.config.disable_traceback_filtering()`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 122\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    123\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/keras/src/backend/tensorflow/core.py\u001b[0m in \u001b[0;36mconvert_to_numpy\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m    169\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRaggedTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    170\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 171\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    172\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    173\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNotImplementedError\u001b[0m: numpy() is only available when eager execution is enabled."
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "lL2uhIILHhDw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import os\n",
        "import time\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "\n",
        "def get_run_logdir(root_logdir=\"my_logs\"):\n",
        "    run_id = time.strftime(\"run_%Y_%m_%d-%H_%M_%S\")\n",
        "    return os.path.join(root_logdir, run_id)\n",
        "\n",
        "log_dir = get_run_logdir()\n",
        "tensorboard_cb = keras.callbacks.TensorBoard(log_dir)\n",
        "\n",
        "history = model.fit(\n",
        "    X_train, y_train,\n",
        "    epochs=20,\n",
        "    validation_data=(X_valid, y_valid),\n",
        "    callbacks=[tensorboard_cb]\n",
        ")\n",
        "\n",
        "# Lower-level summary example\n",
        "writer = tf.summary.create_file_writer(log_dir)\n",
        "with writer.as_default():\n",
        "    for step in range(1, 1001):\n",
        "        tf.summary.scalar(\"my_scalar\", np.sin(step / 10), step=step)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 373
        },
        "id": "NxfkZ4adH2PU",
        "outputId": "63b2663b-67bd-419f-9163-9958c9752c7a"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "NotImplementedError",
          "evalue": "numpy() is only available when eager execution is enabled.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-125451582.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0mtensorboard_cb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensorBoard\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlog_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m history = model.fit(\n\u001b[0m\u001b[1;32m     15\u001b[0m     \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    120\u001b[0m             \u001b[0;31m# To get the full stack trace, call:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m             \u001b[0;31m# `keras.config.disable_traceback_filtering()`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 122\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    123\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/keras/src/backend/tensorflow/core.py\u001b[0m in \u001b[0;36mconvert_to_numpy\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m    169\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRaggedTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    170\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 171\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    172\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    173\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNotImplementedError\u001b[0m: numpy() is only available when eager execution is enabled."
          ]
        }
      ]
    }
  ]
}