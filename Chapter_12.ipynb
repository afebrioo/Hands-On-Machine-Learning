{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Rahmanda Afebrio Yuris Soesatyo - Chapter 12:,Custom Models and Training with TensorFlow"
      ],
      "metadata": {
        "id": "kJQ_hmW-D7Z7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. A Quick Tour of TensorFlow\n",
        "\n",
        "TensorFlow merupakan library komputasi numerik yang memiliki kemiripan dengan NumPy, namun dirancang khusus untuk kebutuhan pembelajaran mesin dan deep learning berskala besar. TensorFlow dilengkapi berbagai fitur lanjutan, seperti dukungan komputasi pada GPU dan TPU, eksekusi terdistribusi, automatic differentiation (autodiff), serta Just-In-Time (JIT) compiler berbasis computation graph yang memungkinkan eksekusi program menjadi lebih efisien.\n",
        "\n",
        "Di atas core engine tersebut, TensorFlow menyediakan beragam modul tingkat tinggi yang mendukung seluruh siklus pengembangan model, antara lain:\n",
        "\n",
        "tf.keras untuk pengembangan model deep learning tingkat tinggi,\n",
        "\n",
        "tf.data untuk membangun pipeline input data yang efisien dan skalabel,\n",
        "\n",
        "tf.image dan tf.signal untuk pemrosesan data khusus,\n",
        "\n",
        "tf.summary untuk visualisasi proses pelatihan menggunakan TensorBoard,\n",
        "\n",
        "TFX (TensorFlow Extended) untuk membangun production ML pipelines,\n",
        "\n",
        "TensorFlow Hub sebagai repositori model pra-latih,\n",
        "\n",
        "serta TensorFlow Lite dan TensorFlow.js untuk deployment pada perangkat mobile dan aplikasi berbasis web.\n",
        "\n",
        "TensorFlow Architecture\n",
        "\n",
        "Arsitektur TensorFlow memisahkan antara:\n",
        "\n",
        "kode Python sebagai high-level API (seperti Keras dan Data API), dan\n",
        "\n",
        "execution engine berbasis C++ yang bertugas menjalankan komputasi pada CPU, GPU, atau TPU.\n",
        "\n",
        "Pemisahan ini memungkinkan model untuk dikembangkan dan dilatih di satu lingkungan, kemudian dieksekusi secara efisien di lingkungan lain tanpa memerlukan perubahan besar pada kode program.\n",
        "\n",
        "2. Custom Losses, Metrics, Initializers, Regularizers, dan Constraints\n",
        "\n",
        "Bab ini membahas cara mendefinisikan berbagai komponen kustom dalam Keras untuk menyesuaikan kebutuhan model, termasuk loss functions, metrics, activation functions, initializers, regularizers, serta constraints.\n",
        "\n",
        "Komponen-komponen tersebut umumnya ditulis menggunakan operasi TensorFlow, sehingga dapat dikonversi menjadi computation graph. Dengan demikian, komponen kustom tetap memperoleh manfaat optimisasi TensorFlow dan kompatibilitas penuh dengan ekosistem yang tersedia.\n",
        "\n",
        "Custom Losses and Metrics\n",
        "\n",
        "Salah satu contoh penting adalah implementasi Huber loss secara kustom. Fungsi loss ini dapat dibuat:\n",
        "\n",
        "sebagai fungsi sederhana, atau\n",
        "\n",
        "sebagai subclass dari keras.losses.Loss, sehingga parameter seperti nilai threshold dapat disimpan secara otomatis saat model disimpan.\n",
        "\n",
        "Pendekatan yang serupa juga dapat diterapkan pada metrics dengan membuat subclass dari keras.metrics.Metric. Cara ini memungkinkan:\n",
        "\n",
        "akumulasi state antar-batch selama proses evaluasi,\n",
        "\n",
        "penyimpanan informasi statistik seperti jumlah true positives dan false positives secara bertahap.\n",
        "\n",
        "Pendekatan ini sangat berguna untuk metrik evaluasi yang kompleks, terutama ketika perhitungannya tidak dapat dilakukan hanya dalam satu batch data."
      ],
      "metadata": {
        "id": "WC-G7HYE7cm_"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "dXh_RV4M7YA5"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "\n",
        "# Functional form (simple)\n",
        "def huber_fn(y_true, y_pred, threshold=1.0):\n",
        "    error = y_true - y_pred\n",
        "    is_small_error = tf.abs(error) < threshold\n",
        "    squared_loss = tf.square(error) / 2.0\n",
        "    linear_loss  = threshold * tf.abs(error) - threshold**2 / 2.0\n",
        "    return tf.where(is_small_error, squared_loss, linear_loss)\n",
        "\n",
        "model.compile(loss=lambda y_true, y_pred: huber_fn(y_true, y_pred, 1.0),\n",
        "              optimizer=\"nadam\")\n",
        "\n",
        "# Class form (threshold tersimpan di config)\n",
        "class HuberLoss(keras.losses.Loss):\n",
        "    def __init__(self, threshold=1.0, **kwargs):\n",
        "        self.threshold = threshold\n",
        "        super().__init__(**kwargs)\n",
        "\n",
        "    def call(self, y_true, y_pred):\n",
        "        error = y_true - y_pred\n",
        "        is_small_error = tf.abs(error) < self.threshold\n",
        "        squared_loss = tf.square(error) / 2.0\n",
        "        linear_loss  = (self.threshold * tf.abs(error)\n",
        "                        - self.threshold**2 / 2.0)\n",
        "        return tf.where(is_small_error, squared_loss, linear_loss)\n",
        "\n",
        "    def get_config(self):\n",
        "        base = super().get_config()\n",
        "        return {**base, \"threshold\": self.threshold}\n",
        "\n",
        "model.compile(loss=HuberLoss(2.0), optimizer=\"nadam\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "\n",
        "# Functional form (simple)\n",
        "def huber_fn(y_true, y_pred, threshold=1.0):\n",
        "    error = y_true - y_pred\n",
        "    is_small_error = tf.abs(error) < threshold\n",
        "    squared_loss = tf.square(error) / 2.0\n",
        "    linear_loss  = threshold * tf.abs(error) - threshold**2 / 2.0\n",
        "    return tf.where(is_small_error, squared_loss, linear_loss)\n",
        "\n",
        "model.compile(loss=lambda y_true, y_pred: huber_fn(y_true, y_pred, 1.0),\n",
        "              optimizer=\"nadam\")\n",
        "\n",
        "# Class form (threshold tersimpan di config)\n",
        "class HuberLoss(keras.losses.Loss):\n",
        "    def __init__(self, threshold=1.0, **kwargs):\n",
        "        self.threshold = threshold\n",
        "        super().__init__(**kwargs)\n",
        "\n",
        "    def call(self, y_true, y_pred):\n",
        "        error = y_true - y_pred\n",
        "        is_small_error = tf.abs(error) < self.threshold\n",
        "        squared_loss = tf.square(error) / 2.0\n",
        "        linear_loss  = (self.threshold * tf.abs(error)\n",
        "                        - self.threshold**2 / 2.0)\n",
        "        return tf.where(is_small_error, squared_loss, linear_loss)\n",
        "\n",
        "    def get_config(self):\n",
        "        base = super().get_config()\n",
        "        return {**base, \"threshold\": self.threshold}\n",
        "\n",
        "model.compile(loss=HuberLoss(2.0), optimizer=\"nadam\")\n"
      ],
      "metadata": {
        "id": "pqasU_bTQfqi"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Custom Layers\n",
        "Jika dibutuhkan perilaku layer yang tidak tersedia secara langsung di Keras, terdapat beberapa pendekatan yang dapat digunakan:\n",
        "\n",
        "Lambda layer\n",
        "Digunakan untuk operasi sederhana tanpa bobot, misalnya fungsi exp atau transformasi element-wise lainnya.\n",
        "\n",
        "Subclass keras.layers.Layer\n",
        "Digunakan untuk membuat layer dengan bobot atau perilaku yang lebih kompleks, mirip dengan implementasi layer Dense kustom.\n",
        "\n",
        "Implementing a Custom Layer\n",
        "Subclass dari keras.layers.Layer umumnya mengimplementasikan metode berikut:\n",
        "\n",
        "__init__() untuk menyimpan hyperparameter\n",
        "build() untuk membuat bobot layer menggunakan add_weight()\n",
        "call() untuk mendefinisikan proses forward pass\n",
        "get_config() agar layer dapat di-save dan di-load dengan benar\n",
        "Untuk layer yang berperilaku berbeda antara training dan inference (seperti Dropout, Batch Normalization, atau Noise layers), metode call() menerima argumen training dan menyesuaikan perilakunya berdasarkan mode tersebut."
      ],
      "metadata": {
        "id": "naf3hJYb8OlU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow import keras\n",
        "import tensorflow as tf\n",
        "\n",
        "class MyDense(keras.layers.Layer):\n",
        "    def __init__(self, units, activation=None, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.units = units\n",
        "        self.activation = keras.activations.get(activation)\n",
        "\n",
        "    def build(self, batch_input_shape):\n",
        "        self.kernel = self.add_weight(\n",
        "            name=\"kernel\",\n",
        "            shape=[batch_input_shape[-1], self.units],\n",
        "            initializer=\"glorot_normal\"\n",
        "        )\n",
        "        self.bias = self.add_weight(\n",
        "            name=\"bias\",\n",
        "            shape=[self.units],\n",
        "            initializer=\"zeros\"\n",
        "        )\n",
        "        super().build(batch_input_shape)\n",
        "\n",
        "    def call(self, X):\n",
        "        z = X @ self.kernel + self.bias\n",
        "        return self.activation(z) if self.activation is not None else z\n",
        "\n",
        "    def get_config(self):\n",
        "        base = super().get_config()\n",
        "        return {\n",
        "            **base,\n",
        "            \"units\": self.units,\n",
        "            \"activation\": keras.activations.serialize(self.activation),\n",
        "        }\n",
        "\n",
        "# Use it like a normal layer\n",
        "model = keras.models.Sequential([\n",
        "    keras.layers.InputLayer(input_shape=(10,)),\n",
        "    MyDense(30, activation=\"relu\"),\n",
        "    MyDense(1)\n",
        "])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SHI1dyB4DaC_",
        "outputId": "36a970dd-6145-48ac-eb9f-a07bbb61d620"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/keras/src/layers/core/input_layer.py:27: UserWarning: Argument `input_shape` is deprecated. Use `shape` instead.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class MyGaussianNoise(keras.layers.Layer):\n",
        "    def __init__(self, stddev, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.stddev = stddev\n",
        "\n",
        "    def call(self, X, training=None):\n",
        "        if training:\n",
        "            noise = tf.random.normal(tf.shape(X), stddev=self.stddev)\n",
        "            return X + noise\n",
        "        return X\n",
        "\n",
        "    def get_config(self):\n",
        "        base = super().get_config()\n",
        "        return {**base, \"stddev\": self.stddev}\n"
      ],
      "metadata": {
        "id": "hVESnBCIYepH"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. Custom Models & Losses Based on Internals\n",
        "\n",
        "Untuk arsitektur jaringan yang lebih kompleks—misalnya model dengan residual blocks, auxiliary heads, atau alur komputasi yang tidak bersifat linear—Keras menyediakan fleksibilitas dengan cara melakukan subclassing terhadap keras.Model.\n",
        "\n",
        "Dalam pendekatan ini:\n",
        "\n",
        "Method __init__() digunakan untuk mendefinisikan seluruh layer atau blok penyusun model.\n",
        "\n",
        "Method call() bertugas mengatur proses forward pass, termasuk logika seperti perulangan, skip connections, percabangan alur, serta output ganda (multi-output).\n",
        "\n",
        "Pendekatan subclassing memberikan kontrol penuh terhadap struktur dan aliran data di dalam model, sehingga sangat cocok untuk arsitektur non-standar yang sulit direpresentasikan menggunakan Sequential atau Functional API.\n",
        "\n",
        "Internal-State-Based Losses\n",
        "\n",
        "Selain loss yang berasal langsung dari perbandingan prediksi dan label, Keras juga mendukung penambahan loss tambahan yang bergantung pada kondisi internal model, seperti nilai aktivasi atau bobot tertentu. Loss semacam ini dapat dimasukkan ke total fungsi objektif menggunakan metode add_loss().\n",
        "\n",
        "Pendekatan ini bermanfaat untuk:\n",
        "\n",
        "regularisasi berbasis aktivasi,\n",
        "\n",
        "penerapan structural constraints pada model,\n",
        "\n",
        "atau tujuan optimisasi tambahan yang tidak secara eksplisit bergantung pada label target.\n",
        "\n",
        "Dengan mekanisme ini, proses pelatihan dapat diarahkan tidak hanya untuk meminimalkan kesalahan prediksi, tetapi juga untuk memenuhi karakteristik internal tertentu yang diinginkan dari model."
      ],
      "metadata": {
        "id": "YSZUN2UmDq11"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ResidualBlock(keras.layers.Layer):\n",
        "    def __init__(self, n_layers, n_neurons, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.hidden = [\n",
        "            keras.layers.Dense(\n",
        "                n_neurons, activation=\"elu\", kernel_initializer=\"he_normal\"\n",
        "            )\n",
        "            for _ in range(n_layers)\n",
        "        ]\n",
        "\n",
        "    def call(self, inputs):\n",
        "        Z = inputs\n",
        "        for layer in self.hidden:\n",
        "            Z = layer(Z)\n",
        "        return inputs + Z  # skip connection\n",
        "\n",
        "\n",
        "class ResidualRegressor(keras.Model):\n",
        "    def __init__(self, output_dim, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.hidden1 = keras.layers.Dense(\n",
        "            30, activation=\"elu\", kernel_initializer=\"he_normal\"\n",
        "        )\n",
        "        self.block1 = ResidualBlock(2, 30)\n",
        "        self.block2 = ResidualBlock(2, 30)\n",
        "        self.out = keras.layers.Dense(output_dim)\n",
        "\n",
        "    def call(self, inputs):\n",
        "        Z = self.hidden1(inputs)\n",
        "        for _ in range(1 + 3):\n",
        "            Z = self.block1(Z)\n",
        "        Z = self.block2(Z)\n",
        "        return self.out(Z)\n"
      ],
      "metadata": {
        "id": "dnGVuLsPDr2s"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class ReconstructingRegressor(keras.Model):\n",
        "    def __init__(self, output_dim, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.hidden = [\n",
        "            keras.layers.Dense(\n",
        "                30, activation=\"selu\", kernel_initializer=\"lecun_normal\"\n",
        "            )\n",
        "            for _ in range(5)\n",
        "        ]\n",
        "        self.out = keras.layers.Dense(output_dim)\n",
        "\n",
        "    def build(self, batch_input_shape):\n",
        "        n_inputs = batch_input_shape[-1]\n",
        "        self.reconstruct = keras.layers.Dense(n_inputs)\n",
        "        super().build(batch_input_shape)\n",
        "\n",
        "    def call(self, inputs):\n",
        "        Z = inputs\n",
        "        for layer in self.hidden:\n",
        "            Z = layer(Z)\n",
        "        reconstruction = self.reconstruct(Z)\n",
        "        recon_loss = tf.reduce_mean(tf.square(reconstruction - inputs))\n",
        "        self.add_loss(0.05 * recon_loss)  # auxiliary regularization loss\n",
        "        return self.out(Z)\n"
      ],
      "metadata": {
        "id": "lsbW8D-hYu-Z"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. Automatic Differentiation dengan tf.GradientTape & Custom Gradients\n",
        "\n",
        "TensorFlow mendukung reverse-mode automatic differentiation melalui mekanisme tf.GradientTape, yang memungkinkan perhitungan gradien secara efisien untuk fungsi yang kompleks terhadap banyak parameter sekaligus.\n",
        "\n",
        "Di dalam blok with tf.GradientTape():, TensorFlow secara otomatis merekam seluruh operasi yang melibatkan tf.Variable. Setelah proses forward computation selesai, gradien dari suatu nilai skalar—biasanya loss function—dapat dihitung terhadap satu atau lebih variabel menggunakan metode tape.gradient().\n",
        "\n",
        "Pendekatan ini menjadi fondasi utama dalam proses pelatihan model berbasis gradient-based optimization di TensorFlow.\n",
        "\n",
        "Penggunaan Lanjutan GradientTape\n",
        "\n",
        "tf.GradientTape menyediakan sejumlah fitur lanjutan yang meningkatkan fleksibilitas eksperimen, antara lain:\n",
        "\n",
        "Persistent Tape\n",
        "Dengan mengaktifkan opsi persistent=True, satu tape yang sama dapat digunakan untuk menghitung gradien lebih dari satu kali. Hal ini berguna pada skenario optimisasi kompleks yang membutuhkan beberapa perhitungan gradien dari ekspresi yang sama.\n",
        "\n",
        "Watching Tensors\n",
        "Selain tf.Variable, tape juga dapat secara eksplisit memantau (watch) tensor biasa. Fitur ini diperlukan ketika gradien terhadap input dibutuhkan, misalnya pada regularisasi berbasis sensitivitas atau analisis pengaruh input terhadap output model.\n",
        "\n",
        "Custom Gradients\n",
        "Untuk fungsi yang sulit didiferensiasikan secara numerik atau berpotensi tidak stabil, TensorFlow menyediakan dekorator @tf.custom_gradient. Dengan mekanisme ini, rumus gradien dapat didefinisikan secara manual, sehingga proses pelatihan menjadi lebih stabil dan terkontrol.\n",
        "\n",
        "Kemampuan ini memberikan keleluasaan tinggi dalam merancang metode optimisasi dan eksperimen pelatihan tingkat lanjut yang melampaui konfigurasi standar Keras."
      ],
      "metadata": {
        "id": "kv8-YJyNDyfz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "def f(w1, w2):\n",
        "    return 3 * w1**2 + 2 * w1 * w2\n",
        "\n",
        "w1 = tf.Variable(5.0)\n",
        "w2 = tf.Variable(3.0)\n",
        "\n",
        "with tf.GradientTape() as tape:\n",
        "    z = f(w1, w2)\n",
        "\n",
        "grads = tape.gradient(z, [w1, w2])\n",
        "print(grads[0].numpy(), grads[1].numpy())  # 36.0, 10.0\n",
        "\n",
        ""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "laD95mxuY4_E",
        "outputId": "4e9102f4-6586-4a83-92ca-34ba2e9a4ffd"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "36.0 10.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "@tf.custom_gradient\n",
        "def my_better_softplus(z):\n",
        "    exp = tf.exp(z)\n",
        "    y = tf.math.log(exp + 1.0)\n",
        "\n",
        "    def grad(dy):\n",
        "        return dy / (1.0 + 1.0 / exp)  # stable derivative\n",
        "\n",
        "    return y, grad"
      ],
      "metadata": {
        "id": "aO-RGi_gZcH5"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "6. Custom Training Loops & TensorFlow Functions\n",
        "\n",
        "Walaupun model.fit() di Keras sudah mencakup sebagian besar kebutuhan pelatihan model (sekitar 95% skenario umum), ada kondisi tertentu di mana custom training loop menjadi pilihan yang lebih tepat. Biasanya ini dibutuhkan ketika:\n",
        "\n",
        "menggunakan lebih dari satu optimizer untuk bagian jaringan yang berbeda,\n",
        "\n",
        "menerapkan aturan update gradien yang tidak standar,\n",
        "\n",
        "atau membutuhkan kontrol penuh terhadap logging, monitoring, dan alur training.\n",
        "\n",
        "Menulis Custom Training Loop\n",
        "\n",
        "Dengan memanfaatkan tf.GradientTape, proses pelatihan manual umumnya dilakukan melalui tahapan berikut:\n",
        "\n",
        "Mengambil satu batch data dari dataset\n",
        "\n",
        "Melakukan forward pass untuk menghasilkan prediksi\n",
        "\n",
        "Menghitung total loss, termasuk komponen regularisasi dari model.losses\n",
        "\n",
        "Menghitung gradien loss terhadap parameter model\n",
        "\n",
        "Menerapkan gradien menggunakan optimizer yang dipilih\n",
        "\n",
        "Memperbarui nilai metrik secara eksplisit\n",
        "\n",
        "Pendekatan ini memberikan fleksibilitas maksimal, karena setiap langkah pelatihan dapat dimodifikasi sesuai kebutuhan eksperimen atau desain algoritma tertentu.\n",
        "\n",
        "TensorFlow Functions (tf.function)\n",
        "\n",
        "Dekorator tf.function berfungsi mengonversi fungsi Python biasa menjadi TensorFlow Function yang dieksekusi sebagai optimized computation graph. Proses ini dilakukan melalui mekanisme AutoGraph dan graph tracing, sehingga eksekusi menjadi lebih cepat dan efisien dibandingkan eksekusi Python murni.\n",
        "\n",
        "Agar tf.function bekerja optimal, terdapat beberapa praktik penting yang perlu diperhatikan:\n",
        "\n",
        "Gunakan operasi TensorFlow (tf.*), bukan NumPy murni\n",
        "\n",
        "Hindari side-effect Python yang memengaruhi alur logika program\n",
        "\n",
        "Definisikan variabel di luar fungsi yang didekorasi tf.function\n",
        "\n",
        "Gunakan tf.range untuk perulangan yang ingin dimasukkan ke dalam computation graph\n",
        "\n",
        "Dengan mengikuti aturan-aturan tersebut, TensorFlow dapat membangun graph yang stabil, efisien, dan cocok untuk eksekusi skala besar maupun deployment."
      ],
      "metadata": {
        "id": "hC-UElHvO5hr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from tensorflow import keras\n",
        "import tensorflow as tf\n",
        "import keras\n",
        "\n",
        "# Model\n",
        "l2_reg = keras.regularizers.l2(0.05)\n",
        "model = keras.models.Sequential([\n",
        "    keras.layers.Dense(30, activation=\"elu\",\n",
        "                       kernel_initializer=\"he_normal\",\n",
        "                       kernel_regularizer=l2_reg),\n",
        "    keras.layers.Dense(1, kernel_regularizer=l2_reg)\n",
        "])\n",
        "\n",
        "# Random mini-batch sampler\n",
        "def random_batch(X, y, batch_size=32):\n",
        "    idx = np.random.randint(len(X), size=batch_size)\n",
        "    return X[idx], y[idx]\n",
        "\n",
        "# Status bar\n",
        "def print_status_bar(iteration, total, loss, metrics=None):\n",
        "    metrics = \" - \".join(\n",
        "        \"{}: {:.4f}\".format(m.name, m.result())\n",
        "        for m in [loss] + (metrics or [])\n",
        "    )\n",
        "    end = \"\" if iteration < total else \"\\n\"\n",
        "    print(\"\\r{}/{} - {}\".format(iteration, total, metrics), end=end)\n",
        "\n",
        "n_epochs = 5\n",
        "batch_size = 32\n",
        "n_steps = len(X_train) // batch_size\n",
        "optimizer = keras.optimizers.Nadam(learning_rate=0.01)\n",
        "loss_fn = keras.losses.mean_squared_error\n",
        "\n",
        "mean_loss = keras.metrics.Mean(name=\"loss\")\n",
        "metrics = [keras.metrics.MeanAbsoluteError(name=\"mae\")]\n",
        "\n",
        "for epoch in range(1, n_epochs + 1):\n",
        "    print(\"Epoch {}/{}\".format(epoch, n_epochs))\n",
        "    for step in range(1, n_steps + 1):\n",
        "        X_batch, y_batch = random_batch(X_train_scaled, y_train, batch_size)\n",
        "        with tf.GradientTape() as tape:\n",
        "            y_pred = model(X_batch, training=True)\n",
        "            main_loss = tf.reduce_mean(loss_fn(y_batch, y_pred))\n",
        "            loss = tf.add_n([main_loss] + model.losses)  # include reg losses\n",
        "        grads = tape.gradient(loss, model.trainable_variables)\n",
        "        optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
        "\n",
        "        mean_loss.update_state(loss)\n",
        "        for metric in metrics:\n",
        "            metric.update_state(y_batch, y_pred)\n",
        "\n",
        "        print_status_bar(step * batch_size, len(X_train), mean_loss, metrics)\n",
        "\n",
        "    print_status_bar(len(X_train), len(X_train), mean_loss, metrics)\n",
        "    for m in [mean_loss] + metrics:\n",
        "        m.reset_states()\n"
      ],
      "metadata": {
        "id": "Qxq0lFjZO8ci"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@tf.function\n",
        "def train_step(X_batch, y_batch):\n",
        "    with tf.GradientTape() as tape:\n",
        "        y_pred = model(X_batch, training=True)\n",
        "        main_loss = tf.reduce_mean(loss_fn(y_batch, y_pred))\n",
        "        loss = tf.add_n([main_loss] + model.losses)\n",
        "    grads = tape.gradient(loss, model.trainable_variables)\n",
        "    optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
        "    mean_loss.update_state(loss)\n",
        "    for metric in metrics:\n",
        "        metric.update_state(y_batch, y_pred)\n"
      ],
      "metadata": {
        "id": "yYvtGSOEPcDs"
      },
      "execution_count": 31,
      "outputs": []
    }
  ]
}