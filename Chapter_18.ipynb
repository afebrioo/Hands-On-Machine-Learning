{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Rahmanda Afebrio Yuris Soesatyo - Chapter 18:Reinforcement Learning"
      ],
      "metadata": {
        "id": "WjUuCIL5pbZ-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Reinforcement Learning Basics, Policy Search, and OpenAI Gym\n",
        "\n",
        "Reinforcement Learning (RL) memandang pengambilan keputusan sebagai proses interaksi berulang antara agen dan lingkungan. Di setiap langkah waktu, agen mengamati state, memilih sebuah action, lalu menerima reward sekaligus berpindah ke state berikutnya. Target akhirnya bukan sekadar reward sesaat, tapi memaksimalkan akumulasi reward jangka panjang yang sudah didiskon, sehingga agen belajar membuat keputusan yang bagus secara berkelanjutan.\n",
        "\n",
        "Kerangka ini kepake di banyak banget skenario dunia nyata, mulai dari kontrol robot, game Atari kayak Ms. Pac-Man, game strategi kompleks seperti Go, sampai sistem otomatisasi suhu dan trading algoritmik. Yang beda-beda di tiap kasus biasanya definisi state, ruang aksi, aturan transisi environment, dan bentuk reward-nya. Tapi pola dasarnya selalu sama: agen coba-coba, dapat feedback, lalu memperbaiki perilakunya.\n",
        "\n",
        "Perilaku agen sendiri diatur oleh policy, yaitu aturan yang memetakan state ke action. Policy bisa deterministik (state tertentu → aksi tertentu) atau stokastik (menghasilkan distribusi probabilitas aksi). Di pendekatan modern, policy hampir selalu dimodelkan sebagai neural network. Proses mencari policy terbaik ini disebut policy search, yang spektrumnya luas: dari cara simpel seperti brute-force, metode evolusioner (genetic algorithm, NEAT), sampai pendekatan berbasis gradien yang langsung mengoptimalkan parameter policy agar reward makin besar.\n",
        "\n",
        "Buat praktik dan eksperimen, OpenAI Gym jadi standar de facto karena nyediain banyak environment siap pakai dengan antarmuka yang konsisten. Contoh klasiknya CartPole-v1, simulasi kereta dengan tongkat yang harus dijaga tetap seimbang. Environment ini punya metode reset() dan step(action), plus informasi observation_space dan action_space yang menjelaskan bentuk state dan aksi, jadi enak banget buat mulai belajar dan ngetes algoritma RL."
      ],
      "metadata": {
        "id": "U9hameF_pmRQ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wlZydvYypZzs"
      },
      "outputs": [],
      "source": [
        "import gym\n",
        "import numpy as np\n",
        "\n",
        "env = gym.make(\"CartPole-v1\")\n",
        "obs = env.reset()\n",
        "\n",
        "def basic_policy(obs):\n",
        "    angle = obs[2]\n",
        "    return 0 if angle < 0 else 1  # 0: left, 1: right\n",
        "\n",
        "totals = []\n",
        "for episode in range(500):\n",
        "    episode_rewards = 0\n",
        "    obs = env.reset()\n",
        "    for step in range(200):\n",
        "        action = basic_policy(obs)\n",
        "        obs, reward, done, info = env.step(action)\n",
        "        episode_rewards += reward\n",
        "        if done:\n",
        "            break\n",
        "    totals.append(episode_rewards)\n",
        "\n",
        "print(np.mean(totals), np.std(totals), np.min(totals), np.max(totals))\n",
        "env.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Policy Gradients and CartPole with TensorFlow/Keras\n",
        "\n",
        "Metode Policy Gradient (PG) mengoptimasi policy secara langsung dengan memaksimalkan ekspektasi return, tanpa membangun model eksplisit dari environment atau fungsi nilai. Ide dasarnya adalah memperkuat aksi yang menghasilkan return tinggi dan melemahkan aksi yang menghasilkan return rendah. Proses training umumnya melibatkan pengumpulan beberapa episode, perhitungan return terdiskonto untuk setiap aksi, dan penggunaan nilai tersebut sebagai sinyal pembelajaran.\n",
        "\n",
        "Algoritma klasik REINFORCE mengimplementasikan pendekatan ini dengan mendefinisikan loss sebagai negatif dari log-probabilitas aksi yang diambil, dikalikan dengan return yang diperoleh. Gradien dari loss ini menjadi estimasi gradien reward terhadap parameter policy, sehingga update parameter mendorong peningkatan probabilitas aksi yang mengarah pada return tinggi.\n",
        "\n",
        "Pada environment CartPole, policy dapat dimodelkan sebagai neural network sederhana yang menerima vektor observasi berdimensi empat dan menghasilkan probabilitas untuk salah satu aksi (misalnya mendorong ke kiri), biasanya melalui fungsi aktivasi sigmoid. Aksi kemudian di-sample secara stokastik dari distribusi ini, yang secara alami mendorong eksplorasi.\n",
        "\n",
        "Return dihitung sebagai jumlah reward terdiskonto dengan faktor diskonto ( \\gamma ) (umumnya sekitar 0.95), lalu dinormalisasi di seluruh aksi dan episode untuk mengurangi varians estimasi gradien. Normalisasi ini menghasilkan advantage yang lebih stabil, sehingga proses pembelajaran menjadi lebih konsisten dan konvergen."
      ],
      "metadata": {
        "id": "4gUvsQ3qpqRb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import gym\n",
        "\n",
        "env = gym.make(\"CartPole-v1\")\n",
        "n_inputs = env.observation_space.shape[0]\n",
        "\n",
        "model = keras.models.Sequential([\n",
        "    keras.layers.Dense(5, activation=\"elu\", input_shape=[n_inputs]),\n",
        "    keras.layers.Dense(1, activation=\"sigmoid\"),\n",
        "])\n",
        "\n",
        "optimizer = keras.optimizers.Adam(learning_rate=0.01)\n",
        "loss_fn = keras.losses.binary_crossentropy\n",
        "\n",
        "def play_one_step(env, obs, model, loss_fn):\n",
        "    with tf.GradientTape() as tape:\n",
        "        left_proba = model(obs[np.newaxis])\n",
        "        action = tf.cast(tf.random.uniform([1, 1]) > left_proba, tf.int32)\n",
        "        y_target = 1.0 - tf.cast(action, tf.float32)\n",
        "        loss = tf.reduce_mean(loss_fn(y_target, left_proba))\n",
        "    grads = tape.gradient(loss, model.trainable_variables)\n",
        "    obs, reward, done, info = env.step(int(action[0, 0]))\n",
        "    return obs, reward, done, grads\n",
        "\n",
        "def play_multiple_episodes(env, n_episodes, n_max_steps, model, loss_fn):\n",
        "    all_rewards, all_grads = [], []\n",
        "    for episode in range(n_episodes):\n",
        "        current_rewards, current_grads = [], []\n",
        "        obs = env.reset()\n",
        "        for step in range(n_max_steps):\n",
        "            obs, reward, done, grads = play_one_step(env, obs, model, loss_fn)\n",
        "            current_rewards.append(reward)\n",
        "            current_grads.append(grads)\n",
        "            if done:\n",
        "                break\n",
        "        all_rewards.append(current_rewards)\n",
        "        all_grads.append(current_grads)\n",
        "    return all_rewards, all_grads\n",
        "\n",
        "def discount_rewards(rewards, discount_factor):\n",
        "    discounted = np.array(rewards, dtype=np.float32)\n",
        "    for step in range(len(rewards) - 2, -1, -1):\n",
        "        discounted[step] += discount_factor * discounted[step + 1]\n",
        "    return discounted\n",
        "\n",
        "def discount_and_normalize_rewards(all_rewards, discount_factor):\n",
        "    all_discounted = [discount_rewards(r, discount_factor) for r in all_rewards]\n",
        "    flat = np.concatenate(all_discounted)\n",
        "    mean, std = flat.mean(), flat.std()\n",
        "    return [(d - mean) / (std + 1e-8) for d in all_discounted]\n",
        "\n",
        "n_iterations = 150\n",
        "n_episodes_per_update = 10\n",
        "n_max_steps = 200\n",
        "discount_factor = 0.95\n",
        "\n",
        "for iteration in range(n_iterations):\n",
        "    all_rewards, all_grads = play_multiple_episodes(\n",
        "        env, n_episodes_per_update, n_max_steps, model, loss_fn\n",
        "    )\n",
        "    all_final_rewards = discount_and_normalize_rewards(\n",
        "        all_rewards, discount_factor\n",
        "    )\n",
        "    all_mean_grads = []\n",
        "    for var_index in range(len(model.trainable_variables)):\n",
        "        mean_grads = tf.reduce_mean([\n",
        "            final_reward * all_grads[episode_idx][step][var_index]\n",
        "            for episode_idx, final_rewards in enumerate(all_final_rewards)\n",
        "            for step, final_reward in enumerate(final_rewards)\n",
        "        ], axis=0)\n",
        "        all_mean_grads.append(mean_grads)\n",
        "    optimizer.apply_gradients(zip(all_mean_grads, model.trainable_variables))\n",
        "\n",
        "env.close()"
      ],
      "metadata": {
        "id": "8700sLOxptv8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Markov Decision Processes, Temporal-Difference Learning, Q-Learning, and Deep Q-Networks\n",
        "\n",
        "Sebagian besar masalah dalam Reinforcement Learning bisa diformalkan sebagai Markov Decision Process (MDP). Di sini, lingkungan didefinisikan lewat sekumpulan state, sekumpulan aksi, probabilitas transisi antar-state, fungsi reward, dan faktor diskonto. Asumsi Markov intinya bilang kalau state berikutnya dan reward yang diterima cuma bergantung pada state dan aksi saat ini, bukan seluruh riwayat sebelumnya. Asumsi ini yang bikin banyak algoritma RL jadi masuk akal dan bisa dianalisis secara matematis.\n",
        "\n",
        "Dalam kerangka MDP, konsep kunci yang sering muncul adalah Bellman Optimality Equation, yang mendeskripsikan nilai optimal sebuah state maupun pasangan state–aksi. Kalau fungsi transisi dan reward diketahui secara lengkap, nilai optimal ini bisa dihitung secara iteratif menggunakan algoritma seperti Value Iteration atau Q-Value Iteration. Algoritma-algoritma ini bekerja dengan cara menerapkan persamaan Bellman berulang kali sampai nilainya konvergen ke solusi optimal.\n",
        "\n",
        "Masalahnya, di dunia nyata agen hampir nggak pernah tahu fungsi transisi dan reward secara eksplisit. Karena itu, pembelajaran biasanya dilakukan secara model-free, murni dari pengalaman interaksi dengan environment. Salah satu pendekatan penting di sini adalah Temporal-Difference (TD) Learning, yang menggabungkan ide Monte Carlo dan dynamic programming. Intinya, nilai diperbarui berdasarkan selisih antara prediksi saat ini dan target bootstrap yang berasal dari estimasi nilai di langkah berikutnya.\n",
        "\n",
        "Algoritma TD yang paling terkenal adalah Q-Learning. Di metode ini, agen memperbarui estimasi nilai state–aksi (Q-value) menggunakan aturan pembaruan berbasis TD error dan learning rate. Q-Learning bersifat off-policy, artinya proses belajarnya tetap mengarah ke policy optimal meskipun agen saat itu sedang menjalankan policy eksplorasi yang berbeda, misalnya ε-greedy.\n",
        "\n",
        "Ketika ruang state menjadi sangat besar atau bahkan kontinu, menyimpan Q-value dalam bentuk tabel sudah nggak realistis. Solusinya adalah Approximate Q-Learning, di mana fungsi Q didekati oleh fungsi parametrik. Kalau fungsi aproksimasi ini berupa neural network dalam, pendekatannya dikenal sebagai Deep Q-Network (DQN). DQN dilatih dengan meminimalkan error antara prediksi Q dan target TD, biasanya menggunakan Mean Squared Error. Pendekatan ini memungkinkan agen menangani environment berdimensi tinggi, seperti input visual mentah pada game Atari, dan jadi salah satu tonggak penting dalam perkembangan Reinforcement Learning modern."
      ],
      "metadata": {
        "id": "ekDMq-9BpuPE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Tabular example on tiny MDP (3 states, handcrafted transitions)\n",
        "transition_probabilities = [\n",
        "    [[0.7, 0.3, 0.0], [1.0, 0.0, 0.0], [0.8, 0.2, 0.0]],\n",
        "    [[0.0, 1.0, 0.0], None,           [0.0, 0.0, 1.0]],\n",
        "    [None,           [0.8, 0.1, 0.1], None          ],\n",
        "]\n",
        "rewards = [\n",
        "    [[+10, 0, 0], [0, 0, 0], [0, 0, 0]],\n",
        "    [[0, 0, 0],   [0, 0, 0], [0, 0, -50]],\n",
        "    [[0, 0, 0],   [+40, 0, 0], [0, 0, 0]],\n",
        "]\n",
        "possible_actions = [[0, 1, 2], [0, 2], [1]]\n",
        "\n",
        "Q_values = np.full((3, 3), -np.inf)\n",
        "for s, actions in enumerate(possible_actions):\n",
        "    Q_values[s, actions] = 0.0\n",
        "\n",
        "def step(state, action):\n",
        "    probas = transition_probabilities[state][action]\n",
        "    next_state = np.random.choice([0, 1, 2], p=probas)\n",
        "    reward = rewards[state][action][next_state]\n",
        "    return next_state, reward\n",
        "\n",
        "def exploration_policy(state):\n",
        "    return np.random.choice(possible_actions[state])\n",
        "\n",
        "alpha0, decay, gamma = 0.05, 0.005, 0.90\n",
        "state = 0\n",
        "for iteration in range(10000):\n",
        "    action = exploration_policy(state)\n",
        "    next_state, reward = step(state, action)\n",
        "    next_value = np.max(Q_values[next_state])\n",
        "    alpha = alpha0 / (1 + iteration * decay)\n",
        "    Q_values[state, action] *= (1 - alpha)\n",
        "    Q_values[state, action] += alpha * (reward + gamma * next_value)\n",
        "    state = next_state\n",
        "\n",
        "optimal_actions = np.argmax(Q_values, axis=1)\n",
        "print(\"Optimal actions per state:\", optimal_actions)\n",
        "\n",
        ""
      ],
      "metadata": {
        "id": "7YTAc192p4WB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# DQN for CartPole\n",
        "import gym\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from collections import deque\n",
        "\n",
        "env = gym.make(\"CartPole-v0\")\n",
        "input_shape = [env.observation_space.shape[0]]\n",
        "n_outputs = env.action_space.n\n",
        "\n",
        "model = keras.models.Sequential([\n",
        "    keras.layers.Dense(32, activation=\"elu\", input_shape=input_shape),\n",
        "    keras.layers.Dense(32, activation=\"elu\"),\n",
        "    keras.layers.Dense(n_outputs),\n",
        "])\n",
        "\n",
        "def epsilon_greedy_policy(state, epsilon=0):\n",
        "    if np.random.rand() < epsilon:\n",
        "        return np.random.randint(n_outputs)\n",
        "    Q_values = model.predict(state[np.newaxis], verbose=0)\n",
        "    return int(np.argmax(Q_values[0]))\n",
        "\n",
        "replay_buffer = deque(maxlen=2000)\n",
        "\n",
        "def sample_experiences(batch_size):\n",
        "    indices = np.random.randint(len(replay_buffer), size=batch_size)\n",
        "    batch = [replay_buffer[i] for i in indices]\n",
        "    states, actions, rewards, next_states, dones = [\n",
        "        np.array([exp[field] for exp in batch]) for field in range(5)\n",
        "    ]\n",
        "    return states, actions, rewards, next_states, dones\n",
        "\n",
        "def play_one_step(env, state, epsilon):\n",
        "    action = epsilon_greedy_policy(state, epsilon)\n",
        "    next_state, reward, done, info = env.step(action)\n",
        "    replay_buffer.append((state, action, reward, next_state, done))\n",
        "    return next_state, reward, done, info\n",
        "\n",
        "batch_size = 32\n",
        "discount_factor = 0.95\n",
        "optimizer = keras.optimizers.Adam(learning_rate=1e-3)\n",
        "loss_fn = keras.losses.mean_squared_error\n",
        "\n",
        "def training_step(batch_size):\n",
        "    states, actions, rewards, next_states, dones = sample_experiences(batch_size)\n",
        "    next_Q_values = model.predict(next_states, verbose=0)\n",
        "    max_next_Q_values = np.max(next_Q_values, axis=1)\n",
        "    target_Q_values = rewards + (1 - dones) * discount_factor * max_next_Q_values\n",
        "    mask = tf.one_hot(actions, n_outputs)\n",
        "    with tf.GradientTape() as tape:\n",
        "        all_Q = model(states)\n",
        "        Q_values = tf.reduce_sum(all_Q * mask, axis=1, keepdims=True)\n",
        "        loss = tf.reduce_mean(loss_fn(target_Q_values, Q_values))\n",
        "    grads = tape.gradient(loss, model.trainable_variables)\n",
        "    optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
        "\n",
        "for episode in range(600):\n",
        "    obs = env.reset()\n",
        "    for step in range(200):\n",
        "        epsilon = max(1 - episode / 500, 0.01)\n",
        "        obs, reward, done, info = play_one_step(env, obs, epsilon)\n",
        "        if done:\n",
        "            break\n",
        "    if episode > 50:\n",
        "        training_step(batch_size)\n",
        "\n",
        "env.close()\n"
      ],
      "metadata": {
        "id": "FJzAKx_Rp4vr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. Deep Q-Learning Variants and Improvements\n",
        "\n",
        "Walaupun Deep Q-Network (DQN) berhasil memperluas Q-Learning ke ruang state berdimensi tinggi, versi dasarnya terkenal cukup “rewel” saat dilatih. Training sering tidak stabil dan rentan terhadap catastrophic forgetting. Akar masalahnya ada pada fakta bahwa jaringan saraf dipakai sekaligus sebagai learner dan sebagai pembuat target. Parameter model terus berubah, policy ikut berubah, dan distribusi data dari replay buffer pun ikut bergeser, sehingga target pembelajaran jadi tidak stasioner dan mudah memicu osilasi.\n",
        "\n",
        "Salah satu perbaikan paling awal dan paling penting adalah Fixed Q-Value Targets. Ide sederhananya adalah memisahkan jaringan menjadi dua: online network dan target network. Online network dipakai untuk update parameter, sedangkan target network hanya bertugas menghasilkan target Q-value. Bobot target network tidak ikut berubah setiap step, melainkan disalin secara periodik dari online network. Dengan target yang relatif “diam” ini, proses training jadi jauh lebih stabil dan tidak terlalu liar.\n",
        "\n",
        "Masalah lain pada DQN klasik adalah over-estimation bias, yang muncul karena operator maksimum cenderung melebih-lebihkan nilai Q. Untuk mengatasi ini, diperkenalkan Double DQN. Triknya adalah memisahkan proses memilih aksi dan mengevaluasi nilainya. Online network digunakan untuk menentukan aksi terbaik pada state berikutnya, sementara target network dipakai untuk menghitung nilai Q dari aksi tersebut. Pemisahan ini kelihatannya sepele, tapi efeknya besar dalam menurunkan bias estimasi dan meningkatkan performa secara konsisten.\n",
        "\n",
        "Selain arsitektur dan target, cara mengambil data dari replay buffer juga berpengaruh besar. Prioritized Experience Replay (PER) mengubah strategi sampling dengan memberi prioritas lebih tinggi pada transisi yang punya TD error besar. Intinya, pengalaman yang “mengejutkan” dianggap lebih informatif dan lebih layak dipelajari ulang. Supaya pembelajaran tetap tidak bias, setiap sampel diberi importance-sampling weight yang mengoreksi kontribusinya terhadap loss. Hasilnya, agen belajar lebih cepat dan lebih efisien dari pengalaman penting.\n",
        "\n",
        "Varian lain yang cukup elegan adalah Dueling DQN, yang mengubah struktur jaringan itu sendiri. Alih-alih langsung memprediksi Q-value untuk setiap aksi, jaringan dipisah menjadi dua alur: satu untuk mempelajari state value dan satu lagi untuk advantage tiap aksi. Keduanya kemudian digabung untuk menghasilkan Q-value akhir. Pendekatan ini sangat berguna pada state di mana pilihan aksi tidak terlalu berpengaruh, karena jaringan bisa fokus belajar seberapa “bagus” state tersebut tanpa harus bergantung penuh pada perbedaan aksi.\n",
        "\n",
        "Gabungan dari teknik-teknik ini—fixed targets, Double DQN, PER, dan dueling architecture—membuat DQN jauh lebih stabil, akurat, dan praktis digunakan. Bahkan, banyak implementasi modern langsung mengombinasikan semuanya, karena secara kolektif mereka mengatasi hampir semua kelemahan utama DQN versi awal."
      ],
      "metadata": {
        "id": "_qF3P8Wgp7wF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Double DQN training_step (modifikasi target)\n",
        "target = keras.models.clone_model(model)\n",
        "target.set_weights(model.get_weights())\n",
        "\n",
        "def training_step_double(batch_size):\n",
        "    states, actions, rewards, next_states, dones = sample_experiences(batch_size)\n",
        "    next_Q_online = model.predict(next_states, verbose=0)\n",
        "    best_next_actions = np.argmax(next_Q_online, axis=1)\n",
        "    next_Q_target = target.predict(next_states, verbose=0)\n",
        "    next_mask = tf.one_hot(best_next_actions, n_outputs).numpy()\n",
        "    next_best_Q = np.sum(next_Q_target * next_mask, axis=1)\n",
        "    target_Q_values = rewards + (1 - dones) * discount_factor * next_best_Q\n",
        "    mask = tf.one_hot(actions, n_outputs)\n",
        "    with tf.GradientTape() as tape:\n",
        "        all_Q = model(states)\n",
        "        Q_values = tf.reduce_sum(all_Q * mask, axis=1, keepdims=True)\n",
        "        loss = tf.reduce_mean(loss_fn(target_Q_values, Q_values))\n",
        "    grads = tape.gradient(loss, model.trainable_variables)\n",
        "    optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
        "\n",
        "# Periodically sync target network\n",
        "if episode % 50 == 0:\n",
        "    target.set_weights(model.get_weights())\n",
        "\n",
        "# Dueling DQN architecture for CartPole-like inputs\n",
        "from tensorflow import keras\n",
        "import tensorflow.keras.backend as K\n",
        "\n",
        "n_outputs = 2\n",
        "input_states = keras.layers.Input(shape=[4])\n",
        "hidden1 = keras.layers.Dense(32, activation=\"elu\")(input_states)\n",
        "hidden2 = keras.layers.Dense(32, activation=\"elu\")(hidden1)\n",
        "state_values = keras.layers.Dense(1)(hidden2)\n",
        "raw_advantages = keras.layers.Dense(n_outputs)(hidden2)\n",
        "advantages = raw_advantages - K.max(raw_advantages, axis=1, keepdims=True)\n",
        "Q_values = state_values + advantages\n",
        "dueling_model = keras.Model(inputs=[input_states], outputs=[Q_values])"
      ],
      "metadata": {
        "id": "CcNN7b9bqHq5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. TF-Agents, Atari Breakout, and Modern RL Algorithms\n",
        "\n",
        "Di bagian penutup bab, diperkenalkan TF-Agents, yaitu library Reinforcement Learning berbasis TensorFlow yang dirancang supaya eksperimen RL jadi lebih rapi dan terstruktur. TF-Agents menyediakan banyak komponen siap pakai, mulai dari environment wrapper, replay buffer yang efisien, sampai implementasi berbagai algoritma RL modern seperti DQN, Double DQN, REINFORCE, PPO, dan Soft Actor-Critic. Selain itu, ada juga driver utilities yang membantu mengotomatisasi proses interaksi agen dengan environment untuk mengumpulkan experience.\n",
        "\n",
        "Sebagai studi kasus utama, bab ini membahas pelatihan agen DQN pada game Atari Breakout-v4 dari OpenAI Gym. Proses training mengikuti praktik standar yang diperkenalkan oleh DeepMind, termasuk preprocessing frame dengan mengubah gambar menjadi grayscale, menurunkan resolusi, dan menggabungkan beberapa frame sekaligus agar agen bisa menangkap dinamika gerak. Training juga memanfaatkan replay buffer berukuran besar, target network terpisah, serta pembaruan parameter secara berkala untuk menjaga stabilitas pembelajaran.\n",
        "\n",
        "Dalam TF-Agents, interaksi dengan environment direpresentasikan menggunakan objek TimeStep, yang berisi informasi seperti tipe langkah (awal, tengah, atau akhir episode), reward, discount, dan observasi. Library ini juga mendefinisikan specification formal seperti observation_spec, action_spec, dan time_step_spec, sehingga bentuk data yang masuk dan keluar bisa divalidasi dengan jelas dan meminimalkan error saat menghubungkan environment dengan algoritma.\n",
        "\n",
        "Pengumpulan data dilakukan melalui driver seperti DynamicStepDriver atau DynamicEpisodeDriver. Driver ini menjalankan policy di environment, mengirimkan aksi, menyimpan trajectory ke replay buffer, sekaligus memperbarui metrik training secara otomatis. Dengan pola ini, logika interaksi dengan environment dipisahkan dari logika pembelajaran, membuat kode lebih modular, mudah dibaca, dan gampang dikembangkan.\n",
        "\n",
        "Selain DQN, bab ini juga menyinggung beberapa algoritma Reinforcement Learning modern lainnya. Pendekatan Actor–Critic memisahkan peran policy dan estimasi nilai untuk menurunkan varians gradien. Soft Actor-Critic menambahkan konsep entropi ke dalam objektif sehingga policy menjadi lebih stabil dan eksploratif. Proximal Policy Optimization membatasi perubahan policy lewat mekanisme clipped loss agar training tidak terlalu agresif. Terakhir, dibahas juga curiosity-driven exploration, yang menambahkan reward intrinsik berbasis ketidakpastian atau error prediksi untuk mendorong agen mengeksplorasi environment dengan reward yang jarang muncul. Semua pendekatan ini mencerminkan arah utama RL modern, yang fokus pada stabilitas training, efisiensi data, dan kemampuan diskalakan ke masalah yang semakin kompleks."
      ],
      "metadata": {
        "id": "6uYBlSpkqIKK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tf_agents.environments import suite_gym\n",
        "from tf_agents.environments.wrappers import ActionRepeat\n",
        "from tf_agents.replay_buffers import tf_uniform_replay_buffer\n",
        "from tf_agents.agents.dqn import dqn_agent\n",
        "from tf_agents.networks import q_network\n",
        "from tf_agents.drivers.dynamic_step_driver import DynamicStepDriver\n",
        "from tf_agents.policies.random_tf_policy import RandomTFPolicy\n",
        "from tf_agents.utils.common import function\n",
        "\n",
        "# Load Breakout environment via TF-Agents (wrapper around Gym)\n",
        "tf_env = suite_gym.load(\"Breakout-v4\")\n",
        "\n",
        "# Q-network (CNN) on preprocessed 84x84x4 observations (assume preprocessing wrapper applied)\n",
        "preproc_env = tf_env  # placeholder if you add preprocessing wrappers\n",
        "q_net = q_network.QNetwork(\n",
        "    preproc_env.observation_spec(),\n",
        "    preproc_env.action_spec(),\n",
        "    fc_layer_params=(512,)\n",
        ")\n",
        "\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=1e-4)\n",
        "train_step = tf.Variable(0)\n",
        "\n",
        "agent = dqn_agent.DqnAgent(\n",
        "    preproc_env.time_step_spec(),\n",
        "    preproc_env.action_spec(),\n",
        "    q_network=q_net,\n",
        "    optimizer=optimizer,\n",
        "    td_errors_loss_fn=tf.keras.losses.Huber(reduction=\"none\"),\n",
        "    train_step_counter=train_step,\n",
        ")\n",
        "agent.initialize()\n",
        "\n",
        "replay_buffer = tf_uniform_replay_buffer.TFUniformReplayBuffer(\n",
        "    data_spec=agent.collect_data_spec,\n",
        "    batch_size=preproc_env.batch_size,\n",
        "    max_length=1000000\n",
        ")\n",
        "replay_buffer_observer = replay_buffer.add_batch\n",
        "\n",
        "update_period = 4\n",
        "collect_driver = DynamicStepDriver(\n",
        "    preproc_env,\n",
        "    agent.collect_policy,\n",
        "    observers=[replay_buffer_observer],\n",
        "    num_steps=update_period\n",
        ")\n",
        "\n",
        "# Warm up replay buffer with random policy\n",
        "initial_collect_policy = RandomTFPolicy(preproc_env.time_step_spec(),\n",
        "                                        preproc_env.action_spec())\n",
        "init_driver = DynamicStepDriver(\n",
        "    preproc_env,\n",
        "    initial_collect_policy,\n",
        "    observers=[replay_buffer.add_batch],\n",
        "    num_steps=20000\n",
        ")\n",
        "init_driver.run()\n",
        "\n",
        "dataset = replay_buffer.as_dataset(\n",
        "    sample_batch_size=64,\n",
        "    num_steps=2,\n",
        "    num_parallel_calls=3\n",
        ").prefetch(3)\n",
        "iterator = iter(dataset)\n",
        "\n",
        "collect_driver.run = function(collect_driver.run)\n",
        "agent.train = function(agent.train)\n",
        "\n",
        "def train_agent(n_iterations):\n",
        "    time_step = None\n",
        "    policy_state = agent.collect_policy.get_initial_state(preproc_env.batch_size)\n",
        "    for iteration in range(n_iterations):\n",
        "        time_step, policy_state = collect_driver.run(time_step, policy_state)\n",
        "        trajectories, buffer_info = next(iterator)\n",
        "        train_loss = agent.train(trajectories)\n",
        "\n",
        "# Example (would take long in practice):\n",
        "# train_agent(1000000)\n"
      ],
      "metadata": {
        "id": "bd1XAQOwqSMb"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}