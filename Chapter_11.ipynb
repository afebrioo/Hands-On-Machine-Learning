{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Rahmanda Afebrio Yuris Soesatyo - Chapter 11:,Training Deep Neural Networks"
      ],
      "metadata": {
        "id": "kJQ_hmW-D7Z7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Vanishing dan Exploding Gradients serta Weight Initialization\n",
        "\n",
        "Pada deep neural networks, proses pelatihan sering mengalami kendala karena nilai gradien dapat:\n",
        "\n",
        "Menghilang (vanishing gradients), yaitu gradien menjadi sangat kecil\n",
        "\n",
        "Membesar berlebihan (exploding gradients), yaitu gradien tumbuh tidak terkendali\n",
        "\n",
        "Masalah ini muncul saat gradien dipropagasikan mundur melalui banyak lapisan selama proses backpropagation.\n",
        "\n",
        "Kondisi tersebut menjadi semakin parah ketika menggunakan fungsi aktivasi klasik seperti sigmoid atau tanh, terutama jika dikombinasikan dengan inisialisasi bobot yang kurang tepat. Akibatnya, lapisan-lapisan awal hampir tidak menerima sinyal pembelajaran, sehingga gagal belajar secara efektif.\n",
        "\n",
        "Weight Initialization\n",
        "\n",
        "Untuk mengatasi permasalahan ini, Glorot dan Bengio memperkenalkan Xavier (Glorot) Initialization.\n",
        "Pada pendekatan ini, bobot diinisialisasi secara acak dengan varians tertentu sehingga:\n",
        "\n",
        "Varians output setiap layer relatif sama dengan varians input\n",
        "\n",
        "Sinyal aktivasi dan gradien tetap stabil di seluruh jaringan\n",
        "\n",
        "Skema ini terbukti membantu mengurangi risiko vanishing maupun exploding gradients, khususnya pada tahap awal pelatihan jaringan.\n",
        "\n",
        "Skema Inisialisasi Berdasarkan Fungsi Aktivasi\n",
        "\n",
        "Setiap fungsi aktivasi memiliki karakteristik yang berbeda, sehingga membutuhkan metode inisialisasi bobot yang sesuai agar performa optimal. Beberapa skema yang umum digunakan antara lain:\n",
        "\n",
        "Glorot (Xavier) Initialization\n",
        "Cocok untuk fungsi aktivasi tanh, sigmoid, dan softmax\n",
        "\n",
        "He Initialization\n",
        "Direkomendasikan untuk ReLU dan variannya\n",
        "\n",
        "LeCun Initialization\n",
        "Digunakan untuk fungsi aktivasi SELU\n",
        "\n",
        "Parameter serta perbandingan antar skema inisialisasi ini biasanya dirangkum dalam literatur pembelajaran mendalam, seperti yang ditunjukkan pada Tabel 11-1."
      ],
      "metadata": {
        "id": "WC-G7HYE7cm_"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "dXh_RV4M7YA5"
      },
      "outputs": [],
      "source": [
        "from tensorflow import keras\n",
        "\n",
        "# He initialization (untuk ReLU dan variannya)\n",
        "he_layer = keras.layers.Dense(\n",
        "    10, activation=\"relu\", kernel_initializer=\"he_normal\"\n",
        ")\n",
        "\n",
        "# Glorot (default Keras) dengan VarianceScaling\n",
        "he_avg_init = keras.initializers.VarianceScaling(\n",
        "    scale=2., mode=\"fan_avg\", distribution=\"uniform\"\n",
        ")\n",
        "glorot_like_layer = keras.layers.Dense(\n",
        "    10, activation=\"sigmoid\", kernel_initializer=he_avg_init\n",
        ")\n",
        "\n",
        "# LeCun initialization (untuk SELU)\n",
        "lecun_layer = keras.layers.Dense(\n",
        "    10, activation=\"selu\", kernel_initializer=\"lecun_normal\"\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Advanced Activation Functions (ReLU, ELU, SELU)\n",
        "\n",
        "Fungsi aktivasi klasik seperti sigmoid cenderung mengalami saturasi, yaitu kondisi di mana gradien mendekati nol untuk nilai input yang besar. Masalah ini memperparah fenomena vanishing gradients, terutama pada jaringan saraf yang dalam.\n",
        "\n",
        "Penelitian lanjutan menunjukkan bahwa fungsi aktivasi non-saturating, seperti ReLU, jauh lebih stabil dan efisien ketika digunakan pada deep neural networks, sehingga mempercepat konvergensi dan meningkatkan performa pelatihan.\n",
        "\n",
        "Variants of ReLU\n",
        "\n",
        "Untuk mengatasi kelemahan ReLU standar—khususnya masalah dead neurons—berbagai varian ReLU dikembangkan, antara lain:\n",
        "\n",
        "Leaky ReLU\n",
        "Mempertahankan gradien kecil namun tidak nol pada sisi input negatif, sehingga neuron tetap dapat belajar meskipun menerima nilai negatif.\n",
        "\n",
        "Randomized ReLU (RReLU)\n",
        "Menggunakan kemiringan acak pada sisi negatif selama proses training. Pendekatan ini juga berperan sebagai bentuk regularisasi.\n",
        "\n",
        "Parametric ReLU (PReLU)\n",
        "Menjadikan kemiringan pada sisi negatif sebagai parameter yang dapat dipelajari langsung oleh model, sehingga memberikan fleksibilitas lebih dalam proses pembelajaran."
      ],
      "metadata": {
        "id": "naf3hJYb8OlU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow import keras\n",
        "\n",
        "# Leaky ReLU sesudah Dense\n",
        "model = keras.models.Sequential([\n",
        "    keras.layers.Dense(10, kernel_initializer=\"he_normal\"),\n",
        "    keras.layers.LeakyReLU(alpha=0.2),\n",
        "])\n",
        "\n",
        "# PReLU (alpha dipelajari)\n",
        "model_prelu = keras.models.Sequential([\n",
        "    keras.layers.Dense(10, kernel_initializer=\"he_normal\"),\n",
        "    keras.layers.PReLU(),\n",
        "])\n",
        "\n",
        "# SELU + LeCun normal untuk self-normalizing dense net\n",
        "selu_layer = keras.layers.Dense(\n",
        "    10, activation=\"selu\", kernel_initializer=\"lecun_normal\"\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SHI1dyB4DaC_",
        "outputId": "dc0448f7-c460-4762-fa85-f88bd990ae25"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/keras/src/layers/activations/leaky_relu.py:41: UserWarning: Argument `alpha` is deprecated. Use `negative_slope` instead.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Batch Normalization & Gradient Clipping\n",
        "Batch Normalization (BN)\n",
        "\n",
        "Batch Normalization adalah teknik yang menormalkan input pada suatu layer dengan cara:\n",
        "\n",
        "melakukan zero-centering dan scaling berdasarkan statistik mini-batch,\n",
        "\n",
        "kemudian menerapkan transformasi ulang menggunakan parameter terlatih γ (gamma) dan β (beta).\n",
        "\n",
        "Manfaat Batch Normalization antara lain:\n",
        "\n",
        "mempercepat proses konvergensi training,\n",
        "\n",
        "mengurangi sensitivitas terhadap inisialisasi bobot,\n",
        "\n",
        "berperan sebagai regularisasi implisit yang membantu mencegah overfitting.\n",
        "\n",
        "Penempatan Batch Normalization\n",
        "\n",
        "Batch Normalization umumnya ditempatkan:\n",
        "\n",
        "sebelum fungsi aktivasi pada hidden layer, atau\n",
        "\n",
        "setelah fungsi aktivasi, tergantung arsitektur dan implementasi.\n",
        "\n",
        "Jika digunakan pada layer pertama, Batch Normalization bahkan dapat menggantikan proses standardization pada data input.\n",
        "\n",
        "Gradient Clipping\n",
        "\n",
        "Gradient clipping digunakan untuk mengatasi masalah exploding gradients, terutama pada model yang sangat dalam seperti Recurrent Neural Networks (RNN).\n",
        "\n",
        "Teknik ini membatasi besar gradien dengan dua pendekatan:\n",
        "\n",
        "Value clipping, yaitu membatasi setiap elemen gradien secara individual.\n",
        "\n",
        "Norm clipping, yaitu membatasi norma total gradien agar tidak melebihi threshold tertentu.\n",
        "\n",
        "Gradient clipping membantu menjaga stabilitas training dan mencegah pembaruan bobot yang terlalu ekstrem."
      ],
      "metadata": {
        "id": "YSZUN2UmDq11"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow import keras\n",
        "\n",
        "# Model dengan Batch Normalization setelah setiap hidden layer\n",
        "model = keras.models.Sequential([\n",
        "    keras.layers.Flatten(input_shape=[28, 28]),\n",
        "    keras.layers.BatchNormalization(),\n",
        "    keras.layers.Dense(300, activation=\"elu\", kernel_initializer=\"he_normal\"),\n",
        "    keras.layers.BatchNormalization(),\n",
        "    keras.layers.Dense(100, activation=\"elu\", kernel_initializer=\"he_normal\"),\n",
        "    keras.layers.BatchNormalization(),\n",
        "    keras.layers.Dense(10, activation=\"softmax\")\n",
        "])\n",
        "\n",
        "# Optimizer dengan gradient clipping by value\n",
        "optimizer = keras.optimizers.SGD(learning_rate=0.01, clipvalue=1.0)\n",
        "model.compile(loss=\"sparse_categorical_crossentropy\",\n",
        "              optimizer=optimizer,\n",
        "              metrics=[\"accuracy\"])"
      ],
      "metadata": {
        "id": "dnGVuLsPDr2s"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. Transfer Learning & Unsupervised Pretraining\n",
        "Untuk deep neural network berukuran besar, melatih model dari awal (training from scratch) sering kali tidak efisien, terutama ketika data berlabel terbatas. Transfer learning menawarkan solusi dengan memanfaatkan model yang telah dilatih sebelumnya pada tugas atau domain yang serupa.\n",
        "\n",
        "Pada pendekatan ini, layer bawah jaringan (yang berfungsi sebagai feature extractor) digunakan kembali, sementara output layer diganti dan dilatih ulang agar sesuai dengan tugas baru.\n",
        "\n",
        "Practical Workflow\n",
        "Dalam praktik, proses transfer learning biasanya dilakukan sebagai berikut:\n",
        "\n",
        "Reuse layer bawah dari model pra-latih.\n",
        "Bekukan (freeze) layer yang digunakan kembali agar bobot awal tidak langsung rusak oleh gradien besar dari layer baru.\n",
        "Latih output layer baru selama beberapa epoch.\n",
        "Unfreeze sebagian upper layers dan lakukan fine-tuning dengan learning rate yang lebih kecil.\n",
        "Strategi ini menjaga stabilitas pembelajaran sekaligus memungkinkan adaptasi bertahap terhadap tugas baru.\n",
        "\n",
        "Unsupervised and Self-Supervised Pretraining\n",
        "Jika tidak tersedia model pra-latih yang sesuai, dapat digunakan unsupervised pretraining atau self-supervised learning untuk mengekstraksi representasi fitur yang berguna, misalnya melalui:\n",
        "\n",
        "Autoencoders\n",
        "Generative Adversarial Networks (GANs)\n",
        "Tugas bantu (auxiliary tasks) berbasis self-supervision\n",
        "Layer bawah hasil pretraining ini kemudian digunakan kembali untuk tugas utama yang berlabel."
      ],
      "metadata": {
        "id": "kv8-YJyNDyfz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow import keras\n",
        "\n",
        "# Load model A (sudah dilatih)\n",
        "model_A = keras.models.load_model(\"my_model_A.h5\")\n",
        "\n",
        "# Reuse semua layer kecuali output, lalu tambah output baru biner\n",
        "model_B_on_A = keras.models.Sequential(model_A.layers[:-1])\n",
        "model_B_on_A.add(keras.layers.Dense(1, activation=\"sigmoid\"))\n",
        "\n",
        "# Bekukan layer lama\n",
        "for layer in model_B_on_A.layers[:-1]:\n",
        "    layer.trainable = False\n",
        "\n",
        "model_B_on_A.compile(loss=\"binary_crossentropy\",\n",
        "                     optimizer=\"sgd\",\n",
        "                     metrics=[\"accuracy\"])\n",
        "\n",
        "# Warm-up training\n",
        "history = model_B_on_A.fit(X_train_B, y_train_B, epochs=4,\n",
        "                           validation_data=(X_valid_B, y_valid_B))\n",
        "\n",
        "# Unfreeze dan fine-tune dengan learning rate kecil\n",
        "for layer in model_B_on_A.layers[:-1]:\n",
        "    layer.trainable = True\n",
        "\n",
        "optimizer = keras.optimizers.SGD(learning_rate=1e-4)\n",
        "model_B_on_A.compile(loss=\"binary_crossentropy\",\n",
        "                     optimizer=optimizer,\n",
        "                     metrics=[\"accuracy\"])\n",
        "\n",
        "history_fine = model_B_on_A.fit(X_train_B, y_train_B, epochs=16,\n",
        "                                validation_data=(X_valid_B, y_valid_B))\n"
      ],
      "metadata": {
        "id": "hvqZ24MRPAFL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. Faster Optimizers & Learning Rate Scheduling\n",
        "\n",
        "Selain desain arsitektur jaringan, efektivitas proses pelatihan model deep learning sangat dipengaruhi oleh pemilihan optimizer dan strategi learning rate scheduling. Optimizer yang tepat mampu mempercepat proses konvergensi, sementara pengaturan learning rate yang adaptif dapat meningkatkan stabilitas pelatihan serta kemampuan generalisasi model.\n",
        "\n",
        "Optimizers\n",
        "\n",
        "Beberapa optimizer yang umum digunakan dalam pelatihan deep neural networks antara lain:\n",
        "\n",
        "Momentum & Nesterov Accelerated Gradient (NAG)\n",
        "\n",
        "Metode Momentum menambahkan komponen kecepatan pada proses Gradient Descent, sehingga pembaruan parameter dapat bergerak lebih cepat dan stabil, terutama saat melewati area dengan gradien kecil atau lembah yang sempit.\n",
        "Nesterov Accelerated Gradient (NAG) merupakan pengembangan dari Momentum yang menghitung gradien pada posisi parameter yang telah diprediksi sebelumnya, sehingga sering kali menghasilkan konvergensi yang lebih cepat dan akurat.\n",
        "\n",
        "AdaGrad\n",
        "\n",
        "AdaGrad menerapkan learning rate adaptif untuk setiap parameter dengan cara memperkecil learning rate pada dimensi yang memiliki gradien besar. Pendekatan ini efektif untuk menangani fitur yang jarang muncul (sparse features). Namun, pada jaringan saraf yang dalam, learning rate cenderung mengecil terlalu cepat sehingga proses pelatihan dapat terhenti sebelum mencapai solusi optimal.\n",
        "\n",
        "RMSProp\n",
        "\n",
        "RMSProp merupakan perbaikan dari AdaGrad dengan hanya mempertimbangkan gradien terbaru melalui mekanisme exponential decay. Pendekatan ini membuat optimizer lebih stabil dan cocok digunakan pada pelatihan deep neural networks yang kompleks.\n",
        "\n",
        "Adam & Nadam\n",
        "\n",
        "Adam mengombinasikan konsep Momentum dan RMSProp dengan memperkirakan rata-rata (first moment) dan varians (second moment) gradien secara eksponensial. Dengan parameter default seperti learning rate 0.001, β₁ = 0.9, dan β₂ = 0.999, Adam sering memberikan performa yang baik tanpa memerlukan banyak penyesuaian parameter.\n",
        "Nadam merupakan varian Adam yang mengintegrasikan Nesterov momentum, sehingga mampu mempercepat proses konvergensi pada beberapa kasus.\n",
        "\n",
        "Learning Rate Scheduling\n",
        "\n",
        "Penggunaan learning rate yang berubah selama proses pelatihan umumnya memberikan hasil yang lebih baik dibandingkan learning rate konstan. Strategi ini memungkinkan model untuk belajar secara agresif pada tahap awal dan melakukan penyesuaian yang lebih halus pada tahap akhir pelatihan. Beberapa pendekatan learning rate scheduling yang umum digunakan antara lain:\n",
        "\n",
        "Exponential Decay, yaitu menurunkan learning rate secara bertahap mengikuti fungsi eksponensial.\n",
        "\n",
        "Performance-based Scheduling, seperti ReduceLROnPlateau, yang menurunkan learning rate ketika performa model berhenti meningkat.\n",
        "\n",
        "1cycle Policy, yang mengatur learning rate naik terlebih dahulu lalu turun secara bertahap, sehingga pelatihan menjadi lebih cepat dan generalisasi model meningkat."
      ],
      "metadata": {
        "id": "hC-UElHvO5hr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow import keras\n",
        "\n",
        "# Adam optimizer\n",
        "adam_opt = keras.optimizers.Adam(learning_rate=0.001, beta_1=0.9, beta_2=0.999)\n",
        "\n",
        "# RMSProp optimizer\n",
        "rms_opt = keras.optimizers.RMSprop(learning_rate=0.001, rho=0.9)\n",
        "\n",
        "# SGD with Momentum + Nesterov\n",
        "nag_opt = keras.optimizers.SGD(learning_rate=0.001, momentum=0.9, nesterov=True)\n",
        "\n",
        "model.compile(loss=\"sparse_categorical_crossentropy\",\n",
        "              optimizer=adam_opt,\n",
        "              metrics=[\"accuracy\"])\n",
        "\n",
        "# Exponential learning rate decay (callback)\n",
        "def exponential_decay_fn(epoch):\n",
        "    return 0.01 * 0.1**(epoch / 20)\n",
        "\n",
        "lr_scheduler = keras.callbacks.LearningRateScheduler(exponential_decay_fn)\n",
        "\n",
        "history = model.fit(X_train_scaled, y_train,\n",
        "                    epochs=50,\n",
        "                    validation_data=(X_valid_scaled, y_valid),\n",
        "                    callbacks=[lr_scheduler])\n"
      ],
      "metadata": {
        "id": "Qxq0lFjZO8ci"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "6. Regularization: L1/L2, Dropout, MC Dropout, dan Max-Norm\n",
        "\n",
        "Deep neural networks umumnya memiliki jumlah parameter yang sangat besar, sehingga rentan mengalami overfitting. Oleh karena itu, teknik regularisasi menjadi bagian penting dalam proses pelatihan model. Selain early stopping dan efek regularisasi implisit dari Batch Normalization, beberapa metode regularisasi utama dibahas pada bagian ini.\n",
        "\n",
        "L1 dan L2 Regularization\n",
        "\n",
        "L1 dan L2 regularization bekerja dengan menambahkan komponen penalti terhadap bobot ke dalam fungsi loss, sehingga kompleksitas model dapat dikendalikan.\n",
        "\n",
        "L2 regularization (weight decay) berfungsi mengecilkan nilai bobot secara bertahap dan kontinu. Teknik ini membantu mencegah bobot menjadi terlalu besar, sehingga model menjadi lebih stabil dan memiliki kemampuan generalisasi yang lebih baik.\n",
        "\n",
        "L1 regularization mendorong terbentuknya bobot nol pada banyak parameter, sehingga menghasilkan model yang lebih sparse. Selain mengurangi kompleksitas, pendekatan ini juga dapat meningkatkan interpretabilitas dan efisiensi komputasi.\n",
        "\n",
        "Dropout\n",
        "\n",
        "Dropout merupakan teknik regularisasi yang bekerja dengan cara menonaktifkan neuron secara acak selama proses pelatihan. Setiap neuron (kecuali neuron output) memiliki probabilitas tertentu p untuk diabaikan, sehingga output neuron tersebut menjadi nol.\n",
        "\n",
        "Pendekatan ini memberikan beberapa keuntungan, antara lain:\n",
        "\n",
        "memaksa jaringan untuk menyebarkan representasi informasi ke banyak neuron,\n",
        "\n",
        "mengurangi ketergantungan berlebihan pada neuron tertentu,\n",
        "\n",
        "serta bertindak seperti ensemble dari banyak subnet yang berbeda, sehingga mengurangi risiko overfitting.\n",
        "\n",
        "Monte Carlo (MC) Dropout\n",
        "\n",
        "MC Dropout merupakan pengembangan dari dropout standar dengan menerapkannya juga pada tahap inferensi. Dropout tetap diaktifkan, dan model dijalankan beberapa kali untuk input yang sama. Hasil prediksi kemudian dirata-ratakan untuk menghasilkan output akhir.\n",
        "\n",
        "Teknik ini memiliki beberapa keunggulan, yaitu:\n",
        "\n",
        "dapat meningkatkan akurasi prediksi,\n",
        "\n",
        "memungkinkan estimasi ketidakpastian prediksi (predictive uncertainty),\n",
        "\n",
        "sangat berguna pada aplikasi dengan tingkat risiko tinggi, seperti sistem medis atau keuangan.\n",
        "\n",
        "Max-Norm Regularization\n",
        "\n",
        "Max-Norm regularization membatasi norma vektor bobot agar tidak melebihi nilai ambang tertentu r. Jika norma bobot melebihi batas ini, bobot akan diskalakan kembali ke dalam rentang yang diizinkan.\n",
        "\n",
        "Metode ini:\n",
        "\n",
        "membantu mengontrol kompleksitas model,\n",
        "\n",
        "menstabilkan proses pelatihan dengan mencegah bobot tumbuh terlalu besar,\n",
        "\n",
        "dan sering dikombinasikan dengan dropout untuk menghasilkan model yang lebih robust."
      ],
      "metadata": {
        "id": "UnL6qgOkPMXo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow import keras\n",
        "from functools import partial\n",
        "import numpy as np\n",
        "\n",
        "# L2-regularized dense layer helper\n",
        "RegularizedDense = partial(\n",
        "    keras.layers.Dense,\n",
        "    activation=\"elu\",\n",
        "    kernel_initializer=\"he_normal\",\n",
        "    kernel_regularizer=keras.regularizers.l2(0.01)\n",
        ")\n",
        "\n",
        "model = keras.models.Sequential([\n",
        "    keras.layers.Flatten(input_shape=[28, 28]),\n",
        "    keras.layers.Dropout(rate=0.2),\n",
        "    RegularizedDense(300),\n",
        "    keras.layers.Dropout(rate=0.2),\n",
        "    RegularizedDense(100),\n",
        "    keras.layers.Dropout(rate=0.2),\n",
        "    keras.layers.Dense(10, activation=\"softmax\",\n",
        "                       kernel_initializer=\"glorot_uniform\",\n",
        "                       kernel_constraint=keras.constraints.max_norm(1.0))\n",
        "])\n",
        "\n",
        "model.compile(loss=\"sparse_categorical_crossentropy\",\n",
        "              optimizer=\"adam\",\n",
        "              metrics=[\"accuracy\"])\n",
        "\n",
        "history = model.fit(X_train_scaled, y_train,\n",
        "                    epochs=30,\n",
        "                    validation_data=(X_valid_scaled, y_valid))\n",
        "\n",
        "# MC Dropout: rata-rata 100 prediksi dengan dropout aktif\n",
        "y_probas = np.stack([\n",
        "    model(X_test_scaled, training=True)  # penting: training=True\n",
        "    for _ in range(100)\n",
        "])\n",
        "y_proba = y_probas.mean(axis=0)\n",
        "y_pred = np.argmax(y_proba, axis=1)"
      ],
      "metadata": {
        "id": "yYvtGSOEPcDs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "7. Practical Default Configurations\n",
        "Bab ini menutup pembahasan dengan beberapa konfigurasi default praktis yang dapat dijadikan titik awal dalam melatih deep neural networks.\n",
        "\n",
        "General Deep Neural Networks\n",
        "Untuk jaringan saraf dalam secara umum, konfigurasi yang sering bekerja dengan baik adalah:\n",
        "\n",
        "He initialization dengan fungsi aktivasi ELU\n",
        "Batch Normalization jika jaringan cukup dalam\n",
        "Early stopping, dengan tambahan L2 regularization bila diperlukan\n",
        "Optimizer berbasis momentum, seperti Momentum, RMSProp, atau Nadam\n",
        "Learning rate schedule 1cycle untuk mempercepat konvergensi dan meningkatkan generalisasi\n",
        "Self-Normalizing Dense Networks\n",
        "Untuk arsitektur dense feedforward yang dirancang agar bersifat self-normalizing, konfigurasi yang direkomendasikan meliputi:\n",
        "\n",
        "LeCun initialization dengan fungsi aktivasi SELU\n",
        "Tanpa Batch Normalization\n",
        "Alpha Dropout jika regularisasi tambahan diperlukan\n",
        "Optimizer dan learning rate scheduling yang serupa dengan konfigurasi umum\n",
        "Pendekatan ini menjaga mean dan standar deviasi aktivasi tetap stabil di seluruh jaringan.\n",
        "\n",
        "Data-Efficient Training Strategies\n",
        "Ketika data berlabel terbatas, sangat dianjurkan untuk memanfaatkan:\n",
        "\n",
        "Transfer learning\n",
        "Unsupervised atau self-supervised pretraining\n",
        "Auxiliary tasks untuk membantu pembelajaran representasi\n",
        "Strategi-strategi ini secara signifikan meningkatkan efisiensi pelatihan dan performa model."
      ],
      "metadata": {
        "id": "lcszrnLVPg0v"
      }
    }
  ]
}