{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Rahmanda Afebrio Yuris Soesatyo - Chapter 15:,Processing Sequences Using RNNs and CNNs"
      ],
      "metadata": {
        "id": "FKeQPvxrmU01"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Recurrent Neurons and Sequence Types\n",
        "\n",
        "Recurrent Neural Network (RNN) memperkenalkan koneksi rekuren yang memungkinkan sebuah neuron memanfaatkan tidak hanya input saat ini\n",
        "ð‘¥\n",
        "(\n",
        "ð‘¡\n",
        ")\n",
        "x(t), tetapi juga state atau output dari langkah waktu sebelumnya\n",
        "ð‘¦\n",
        "(\n",
        "ð‘¡\n",
        "âˆ’\n",
        "1\n",
        ")\n",
        "y(tâˆ’1). Dengan mekanisme ini, keluaran pada waktu\n",
        "ð‘¡\n",
        "t dipengaruhi oleh seluruh riwayat input sebelumnya.\n",
        "\n",
        "Ketika RNN di-unroll sepanjang dimensi waktu, neuron yang sama akan muncul kembali di setiap time step, dengan bobot yang dibagi (shared weights). Hal ini memungkinkan model mempelajari pola temporal tanpa menambah jumlah parameter secara linear terhadap panjang urutan.\n",
        "\n",
        "Secara matematis, satu layer RNN sederhana melibatkan:\n",
        "\n",
        "matriks bobot\n",
        "ð‘Š\n",
        "ð‘¥\n",
        "W\n",
        "x\n",
        "\tâ€‹\n",
        "\n",
        " untuk koneksi dari input,\n",
        "\n",
        "matriks bobot\n",
        "ð‘Š\n",
        "â„Ž\n",
        "W\n",
        "h\n",
        "\tâ€‹\n",
        "\n",
        " untuk koneksi dari hidden state sebelumnya,\n",
        "\n",
        "vektor bias\n",
        "ð‘\n",
        "b,\n",
        "\n",
        "serta fungsi aktivasi\n",
        "ðœ™\n",
        "Ï•, yang umumnya berupa tanh.\n",
        "\n",
        "Jenis Hubungan Inputâ€“Output pada RNN\n",
        "\n",
        "Berdasarkan bentuk input dan output yang dipetakan, RNN dapat digunakan dalam beberapa konfigurasi utama:\n",
        "\n",
        "Sequence-to-Sequence\n",
        "Baik input maupun output berbentuk urutan, misalnya pada pemodelan time series atau machine translation.\n",
        "\n",
        "Sequence-to-Vector\n",
        "Sebuah urutan dipetakan menjadi satu vektor representasi, contohnya pada klasifikasi sentimen.\n",
        "\n",
        "Vector-to-Sequence\n",
        "Satu vektor digunakan untuk menghasilkan urutan, seperti pada tugas image captioning.\n",
        "\n",
        "Encoderâ€“Decoder\n",
        "Urutan sumber dienkode menjadi representasi vektor oleh encoder, lalu decoder menghasilkan urutan target. Arsitektur ini umum digunakan pada penerjemahan mesin dan sequence transduction lainnya."
      ],
      "metadata": {
        "id": "cYncUqCgmZRu"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1Rz5b9VsmKZm"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from tensorflow import keras\n",
        "\n",
        "def generate_time_series(batch_size, n_steps):\n",
        "    freq1, freq2, offs1, offs2 = np.random.rand(4, batch_size, 1)\n",
        "    time = np.linspace(0, 1, n_steps)\n",
        "    series = 0.5 * np.sin((time - offs1) * (freq1 * 10 + 10))\n",
        "    series += 0.2 * np.sin((time - offs2) * (freq2 * 20 + 20))\n",
        "    series += 0.1 * (np.random.rand(batch_size, n_steps) - 0.5)\n",
        "    return series[..., np.newaxis].astype(np.float32)\n",
        "\n",
        "n_steps = 50\n",
        "series = generate_time_series(10000, n_steps + 1)\n",
        "X_train, y_train = series[:7000, :n_steps], series[:7000, -1]\n",
        "X_valid, y_valid = series[7000:9000, :n_steps], series[7000:9000, -1]\n",
        "\n",
        "model = keras.models.Sequential([\n",
        "    keras.layers.SimpleRNN(1, input_shape=[None, 1])\n",
        "])\n",
        "model.compile(loss=\"mse\", optimizer=\"adam\")\n",
        "history = model.fit(X_train, y_train, epochs=20,\n",
        "                    validation_data=(X_valid, y_valid))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Training RNNs and Time Series Forecasting\n",
        "\n",
        "RNN dilatih menggunakan Backpropagation Through Time (BPTT), yaitu dengan melakukan unrolling jaringan sepanjang beberapa langkah waktu. Pada tahap forward pass, model menghitung output dan nilai loss di setiap time step. Selanjutnya, pada backward pass, gradien disebarkan mundur melalui dimensi waktu.\n",
        "\n",
        "Karena bobot yang sama digunakan berulang pada setiap langkah waktu, gradien untuk satu parameter akan dijumlahkan sepanjang urutan. Kondisi ini membuat RNN sangat rentan terhadap masalah vanishing gradients dan exploding gradients, terutama ketika menangani urutan yang panjang."
      ],
      "metadata": {
        "id": "xyyLxqcpmjJd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow import keras\n",
        "import numpy as np\n",
        "\n",
        "# One-step-ahead deep RNN with Dense output\n",
        "model = keras.models.Sequential([\n",
        "    keras.layers.SimpleRNN(20, return_sequences=True, input_shape=[None, 1]),\n",
        "    keras.layers.SimpleRNN(20),\n",
        "    keras.layers.Dense(1)\n",
        "])\n",
        "model.compile(loss=\"mse\", optimizer=\"adam\")\n",
        "model.fit(X_train, y_train, epochs=20,\n",
        "          validation_data=(X_valid, y_valid))\n",
        "\n",
        "# Forecast 10 steps ahead at once (sequence-to-vector)\n",
        "series = generate_time_series(10000, n_steps + 10)\n",
        "X_train, Y_train = series[:7000, :n_steps], series[:7000, -10:, 0]\n",
        "X_valid, Y_valid = series[7000:9000, :n_steps], series[7000:9000, -10:, 0]\n",
        "\n",
        "model = keras.models.Sequential([\n",
        "    keras.layers.SimpleRNN(20, return_sequences=True, input_shape=[None, 1]),\n",
        "    keras.layers.SimpleRNN(20),\n",
        "    keras.layers.Dense(10)\n",
        "])\n",
        "model.compile(loss=\"mse\", optimizer=\"adam\")\n",
        "model.fit(X_train, Y_train, epochs=20,\n",
        "          validation_data=(X_valid, Y_valid))\n"
      ],
      "metadata": {
        "id": "zZ_Gv6wrmr4u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Fighting Unstable Gradients and Using LSTM/GRU\n",
        "\n",
        "RNN memiliki dua keterbatasan utama, yaitu gradien yang tidak stabil serta kemampuan memori jangka pendek. Untuk mengatasi masalah gradien, beberapa teknik yang umum digunakan pada deep feedforward networks tetap relevan, seperti:\n",
        "\n",
        "optimizer berbasis momentum,\n",
        "\n",
        "gradient clipping,\n",
        "\n",
        "dropout,\n",
        "\n",
        "serta inisialisasi bobot yang tepat.\n",
        "\n",
        "Namun, Batch Normalization cenderung kurang efektif ketika diterapkan langsung di dalam sel RNN karena adanya ketergantungan antar time step.\n",
        "\n",
        "Layer Normalization\n",
        "\n",
        "Sebagai alternatif, Layer Normalization lebih cocok untuk arsitektur rekuren. Teknik ini menormalkan aktivasi di sepanjang dimensi fitur untuk setiap instance dan setiap langkah waktu, sehingga:\n",
        "\n",
        "distribusi aktivasi menjadi lebih stabil,\n",
        "\n",
        "proses pelatihan menjadi lebih konsisten,\n",
        "\n",
        "dan sensitivitas terhadap inisialisasi bobot berkurang.\n",
        "\n",
        "LSTM dan GRU\n",
        "\n",
        "Untuk mengatasi keterbatasan memori jangka pendek pada RNN klasik, diperkenalkan dua arsitektur utama:\n",
        "\n",
        "Long Short-Term Memory (LSTM)\n",
        "\n",
        "Gated Recurrent Unit (GRU)\n",
        "\n",
        "Kedua model ini menggunakan mekanisme gatingâ€”seperti forget, input, output, serta update/reset gateâ€”dan mempertahankan state jangka panjang. Mekanisme ini memungkinkan jaringan:\n",
        "\n",
        "menyimpan informasi penting,\n",
        "\n",
        "melupakan informasi yang tidak relevan,\n",
        "\n",
        "dan mempertahankan dependensi hingga puluhan langkah waktu.\n",
        "\n",
        "Dalam praktiknya, LSTM dan GRU jauh lebih stabil dan mudah dilatih dibandingkan SimpleRNN, terutama pada urutan yang panjang. Oleh karena itu, keduanya sering menjadi pilihan default untuk berbagai tugas pemodelan sekuens."
      ],
      "metadata": {
        "id": "jU750bZ0msZ9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow import keras\n",
        "\n",
        "# Stacked LSTM seq2seq\n",
        "model_lstm = keras.models.Sequential([\n",
        "    keras.layers.LSTM(20, return_sequences=True, input_shape=[None, 1]),\n",
        "    keras.layers.LSTM(20, return_sequences=True),\n",
        "    keras.layers.TimeDistributed(keras.layers.Dense(10))\n",
        "])\n",
        "\n",
        "# GRU with Conv1D front-end (see next section for context)\n",
        "model_gru = keras.models.Sequential([\n",
        "    keras.layers.GRU(20, return_sequences=True, input_shape=[None, 1]),\n",
        "    keras.layers.GRU(20, return_sequences=True),\n",
        "    keras.layers.TimeDistributed(keras.layers.Dense(10))\n",
        "])\n",
        "\n",
        "# Custom SimpleRNNCell with Layer Normalization\n",
        "class LNSimpleRNNCell(keras.layers.Layer):\n",
        "    def __init__(self, units, activation=\"tanh\", **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.state_size = units\n",
        "        self.output_size = units\n",
        "        self.simple_rnn_cell = keras.layers.SimpleRNNCell(units, activation=None)\n",
        "        self.layer_norm = keras.layers.LayerNormalization()\n",
        "        self.activation = keras.activations.get(activation)\n",
        "\n",
        "    def call(self, inputs, states):\n",
        "        outputs, _ = self.simple_rnn_cell(inputs, states)\n",
        "        norm_outputs = self.activation(self.layer_norm(outputs))\n",
        "        return norm_outputs, [norm_outputs]\n",
        "\n",
        "model_ln = keras.models.Sequential([\n",
        "    keras.layers.RNN(LNSimpleRNNCell(20), return_sequences=True,\n",
        "                     input_shape=[None, 1]),\n",
        "    keras.layers.RNN(LNSimpleRNNCell(20), return_sequences=True),\n",
        "    keras.layers.TimeDistributed(keras.layers.Dense(10))\n",
        "])"
      ],
      "metadata": {
        "id": "PuNNEGmtm298"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. Mixing RNNs with 1D Convolutions and WaveNet\n",
        "\n",
        "Untuk urutan yang sangat panjang, bab ini mengeksplorasi kombinasi Conv1D dan RNN, atau bahkan mengganti RNN sepenuhnya dengan CNN satu dimensi. Conv1D dengan stride > 1 atau padding valid dapat melakukan downsampling temporal, sehingga memperpendek urutan sambil tetap menangkap pola lokal.\n",
        "\n",
        "Dengan memilih kernel size dan stride yang tepat, model dapat melatih GRU atau LSTM di atas representasi urutan yang lebih ringkas. Output multi-step kemudian dihasilkan menggunakan TimeDistributed(Dense), yang terbukti meningkatkan performa forecasting.\n",
        "\n",
        "Selain itu, diperkenalkan WaveNet, arsitektur berbasis CNN murni dengan dilated convolutions yang dilation-nya meningkat secara eksponensial (1, 2, 4, 8, â€¦). Pendekatan ini memungkinkan model menangkap ketergantungan jangka sangat panjang secara efisien dan mencapai performa tinggi pada tugas audio dan sekuens panjang."
      ],
      "metadata": {
        "id": "XUvmhSt5m3hj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from tensorflow import keras\n",
        "\n",
        "# Conv1D front-end + GRU stack\n",
        "model = keras.models.Sequential([\n",
        "    keras.layers.Conv1D(\n",
        "        filters=20, kernel_size=4, strides=2, padding=\"valid\",\n",
        "        input_shape=[None, 1]\n",
        "    ),\n",
        "    keras.layers.GRU(20, return_sequences=True),\n",
        "    keras.layers.GRU(20, return_sequences=True),\n",
        "    keras.layers.TimeDistributed(keras.layers.Dense(10))\n",
        "])\n",
        "\n",
        "model.compile(loss=\"mse\", optimizer=\"adam\", metrics=[last_time_step_mse])\n",
        "history = model.fit(\n",
        "    X_train, Y_train[:, 3::2], epochs=20,\n",
        "    validation_data=(X_valid, Y_valid[:, 3::2])\n",
        ")\n",
        "\n",
        "# Simplified WaveNet-style dilated Conv1D stack\n",
        "wavenet = keras.models.Sequential()\n",
        "wavenet.add(keras.layers.InputLayer(input_shape=[None, 1]))\n",
        "for rate in (1, 2, 4, 8) * 2:\n",
        "    wavenet.add(keras.layers.Conv1D(\n",
        "        filters=20, kernel_size=2, padding=\"causal\",\n",
        "        activation=\"relu\", dilation_rate=rate\n",
        "    ))\n",
        "wavenet.add(keras.layers.Conv1D(filters=10, kernel_size=1))\n",
        "wavenet.compile(loss=\"mse\", optimizer=\"adam\", metrics=[last_time_step_mse])\n",
        "history_w = wavenet.fit(\n",
        "    X_train, Y_train, epochs=20,\n",
        "    validation_data=(X_valid, Y_valid)\n",
        ")"
      ],
      "metadata": {
        "id": "SqxMN4b1m7Hh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. Practical Forecasting Patterns and Multi-Step Strategies\n",
        "\n",
        "Bab ini menutup dengan beberapa strategi praktis untuk multi-step forecasting:\n",
        "\n",
        "Iterative strategy\n",
        "Model dilatih untuk memprediksi satu langkah ke depan, lalu prediksi tersebut digunakan kembali sebagai input untuk langkah berikutnya. Metode ini sederhana namun rentan terhadap akumulasi error.\n",
        "\n",
        "Direct strategy\n",
        "Model dilatih untuk langsung memprediksi beberapa langkah ke depan sekaligus sebagai output vektor. Pendekatan ini lebih stabil, meskipun bisa lebih sulit untuk horizon yang sangat jauh.\n",
        "\n",
        "Sequence-to-sequence multi-horizon\n",
        "Target diubah menjadi urutan vektor multi-step, sehingga setiap time step memprediksi beberapa langkah ke depan. Dengan RNN seq-to-seq dan TimeDistributed(Dense), sinyal loss menjadi lebih kaya, yang sering mempercepat dan menstabilkan training.\n",
        "\n",
        "Strategi-strategi ini dikombinasikan dengan baseline yang kuat, pilihan arsitektur (SimpleRNN, LSTM, GRU, Conv1D, WaveNet), serta teknik stabilisasi seperti clipping, normalisasi, dan dropout untuk membangun model forecasting yang benar-benar kompetitif."
      ],
      "metadata": {
        "id": "ImqySN7Nm7o1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from tensorflow import keras\n",
        "\n",
        "n_steps = 50\n",
        "series = generate_time_series(10000, n_steps + 10)\n",
        "\n",
        "Y = np.empty((10000, n_steps, 10))\n",
        "for step_ahead in range(1, 11):\n",
        "    Y[:, :, step_ahead - 1] = series[:, step_ahead: step_ahead + n_steps, 0]\n",
        "\n",
        "Y_train, Y_valid, Y_test = Y[:7000], Y[7000:9000], Y[9000:]\n",
        "\n",
        "model = keras.models.Sequential([\n",
        "    keras.layers.SimpleRNN(20, return_sequences=True, input_shape=[None, 1]),\n",
        "    keras.layers.SimpleRNN(20, return_sequences=True),\n",
        "    keras.layers.TimeDistributed(keras.layers.Dense(10))\n",
        "])\n",
        "\n",
        "model.compile(loss=\"mse\", optimizer=\"adam\")\n",
        "history = model.fit(X_train, Y_train, epochs=20,\n",
        "                    validation_data=(X_valid, Y_valid))\n",
        "\n",
        ""
      ],
      "metadata": {
        "id": "eh_3-bvfm_rN"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}